{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EduardoMoraesRitter/NLP-Chatbot/blob/main/analyticsWatson.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Update/installs Watson SDK, scikit-learn and termcolor \n",
        "#!pip install -U scikit-learn\n",
        "!pip install termcolor\n",
        "!pip install \"tensorflow-tensorboard<0.2.0,>=0.1.0\"\n",
        "#!pip install \"watson-developer-cloud\"\n",
        "!pip install \"ibm-watson\"\n",
        "#!pip install --upgrade watson-developer-cloud\n",
        "!pip install nltk\n",
        "\n",
        "# Supporting Libs\n",
        "import re\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import time\n",
        "import nltk\n",
        "import sklearn\n",
        "import itertools\n",
        "import matplotlib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import ibm_watson\n",
        "\n",
        "# Watson APIs Libs\n",
        "#from watson_developer_cloud import AssistantV1\n",
        "\n",
        "# Metrics & ML\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix, precision_score, accuracy_score, recall_score, f1_score\n",
        "\n",
        "# Visualization configs\n",
        "from termcolor import colored, cprint\n",
        "from IPython.display import display, HTML\n",
        "%matplotlib inline\n",
        "matplotlib.style.use('ggplot')\n",
        "pd.options.display.max_colwidth = 150"
      ],
      "metadata": {
        "id": "fqTFwIxLrRg-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fill it out with your WAS credentials\n",
        "WAS_WORKSPACE = \"\"\n",
        "WAS_API_KEY = \"\"\n",
        "WAS_URL = \"https://api.us-east.assistant.watson.cloud.ibm.com\""
      ],
      "metadata": {
        "id": "zX-dO9AbYN7M"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ibm_watson import AssistantV1\n",
        "from ibm_cloud_sdk_core.authenticators import IAMAuthenticator\n",
        "\n",
        "authenticator = IAMAuthenticator(WAS_API_KEY)\n",
        "assistant = AssistantV1(\n",
        "    version='2019-09-20',\n",
        "    authenticator=authenticator\n",
        ")\n",
        "\n",
        "assistant.set_service_url(WAS_URL)\n",
        "original_workspace_id = WAS_WORKSPACE"
      ],
      "metadata": {
        "id": "3DDXqEIeYQcw"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if the skill is ready to receive calls, and if it is not waiting 30 seconds and try again. This function blocks the rest of the code!\n",
        "def check_wksp_status(check_workspace_id):\n",
        "    wksp_notready = True\n",
        "    \n",
        "    while(wksp_notready == True):\n",
        "        print('Testing workspace...' + check_workspace_id)\n",
        "        workspace = assistant.get_workspace(workspace_id=check_workspace_id).get_result()\n",
        "\n",
        "        print('Workspace status: {0}'.format(workspace['status']))\n",
        "        if workspace['status'] == 'Available':\n",
        "            wksp_notready = False\n",
        "            print('Ready to go!')\n",
        "        else:\n",
        "            print('In training...wait 30s and try again.')\n",
        "            time.sleep(30)\n",
        "\n",
        "# Prints the logs in red and bold\n",
        "def printred(str_temp,isbold):\n",
        "    if isbold:\n",
        "        print(colored(str_temp, 'red', attrs=['bold']))\n",
        "    else:\n",
        "        print(colored(str_temp, 'red'))"
      ],
      "metadata": {
        "id": "Sgd1nXREYXq8"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get list of intentions from the previous json export\n",
        "list_original_intents = assistant.list_intents(workspace_id = original_workspace_id).get_result()\n",
        "\n",
        "list_original_examples = []\n",
        "list_original_intent_names = []\n",
        "\n",
        "# Variable declaration\n",
        "intent_distribution = pd.DataFrame(columns=['classes', 'size'])\n",
        "avg_size = 0;\n",
        "\n",
        "# Assemble distribution view\n",
        "for idx, intent in enumerate(list_original_intents['intents']):\n",
        "    examples = assistant.list_examples(\n",
        "    workspace_id = original_workspace_id,\n",
        "    intent = list_original_intents['intents'][idx]['intent']\n",
        "    ).get_result()\n",
        "    avg_size = avg_size + len(examples['examples'])\n",
        "    for example in examples['examples']:\n",
        "        list_original_examples.append(example['text'])\n",
        "        list_original_intent_names.append(list_original_intents['intents'][idx]['intent'])\n",
        "        intent_distribution.loc[idx] = pd.Series({'classes':list_original_intents['intents'][idx]['intent'], 'size': len(examples['examples'])})\n",
        "        #avg_size = avg_size + len(examples['examples'])\n",
        "\n",
        "# Print the chart on the screen\n",
        "intent_distribution.plot(kind='bar',x='classes', y='size',figsize=(30,7))\n",
        "\n",
        "# Mount the data frame\n",
        "intent_distribution = pd.DataFrame({\n",
        "    'Example': list_original_examples,\n",
        "    'Intent': list_original_intent_names\n",
        "}, columns=['Example','Intent'])"
      ],
      "metadata": {
        "id": "o5HwoXmWYbDQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mean size of intention\n",
        "final_avg_size = avg_size/len(list_original_intents['intents'])\n",
        "\n",
        "print(\"Average of instances by intention: \" + str(final_avg_size))"
      ],
      "metadata": {
        "id": "lIr2UAGqYgEV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Coefficient of discrepancy\n",
        "cte_coef_disc = 0.5\n",
        "\n",
        "print(colored(\"\\nIntentions with a discrepancy of examples (intent # examples):\\n\", attrs=['bold']))\n",
        "\n",
        "if final_avg_size < 5:\n",
        "    print(colored(\">>> The sample presented does not meet the minimum required for training (5 examples) and, therefore, the deviations will not be calculated.\", 'red', attrs=['bold']))\n",
        "else:\n",
        "    # Validation of which classes are \"offending\" the distribution\n",
        "    for intent in list_original_intents['intents']:\n",
        "        examples = assistant.list_examples(\n",
        "        workspace_id = original_workspace_id,\n",
        "        intent = intent['intent']\n",
        "        ).get_result()\n",
        "\n",
        "        diff = float(len(examples['examples'])) - final_avg_size\n",
        "        if(abs(diff) > (final_avg_size * cte_coef_disc)):\n",
        "            if diff > 0:\n",
        "                printred(\"[+] >>> \" + intent['intent'] + ' # ' + str(len(examples['examples'])) + ' / has ' + str(round(diff-(final_avg_size * cte_coef_disc),2)) + ' more examples than expected.',True)\n",
        "            else:\n",
        "                printred(\"[-] >>> \" + intent['intent'] + ' # ' + str(len(examples['examples'])) + ' / has ' + str(round(abs(diff)-(final_avg_size * cte_coef_disc),2)) + ' fewer examples than expected.',True)"
      ],
      "metadata": {
        "id": "nDrh7LvgYjNs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(colored(\"\\nIntent without minimal amount of examples (5):\\n\", attrs=['bold']))\n",
        "\n",
        "# Verifica se o m√≠nimo exigido foi cumprido\n",
        "for intent in list_original_intents['intents']:\n",
        "    examples = assistant.list_examples(\n",
        "    workspace_id = original_workspace_id,\n",
        "    intent = intent['intent']\n",
        "    ).get_result()\n",
        "\n",
        "    if len(examples['examples']) < 5:\n",
        "        printred(\">>> \" + intent['intent'] + ' # ' + str(len(examples['examples'])),True)\n",
        "        \n",
        "print(colored(\"\\n\\nIntention without minimum amount of examples SUGGESTED (10):\\n\", attrs=['bold']))\n",
        "\n",
        "# Checks if the suggested minimum has been met\n",
        "for intent in list_original_intents['intents']:\n",
        "    examples = assistant.list_examples(\n",
        "    workspace_id = original_workspace_id,\n",
        "    intent = intent['intent']\n",
        "    ).get_result()\n",
        "\n",
        "    if len(examples['examples']) < 10:\n",
        "        printred(\">>> \" + intent['intent'] + ' # ' + str(len(examples['examples'])),True)"
      ],
      "metadata": {
        "id": "6VwAjiw5Yl2k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(colored(\"\\nSelect repeated examples from our training set:\\n\", attrs=['bold']))\n",
        "\n",
        "# Mounts example frequency\n",
        "fdist = nltk.FreqDist(intent_distribution['Example'])        \n",
        "\n",
        "# Select those with more than one occurrence\n",
        "repeated = [x for idx,x in intent_distribution.sort_values(\"Example\").iterrows() if x['Example'] in [k for k,v in fdist.items() if v > 1]]\n",
        "for y in repeated:\n",
        "    print(y['Example'] + ' # ' + y['Intent'])\n",
        "    \n",
        "if len(repeated) <= 2:\n",
        "    print(colored(\"There are no repeated examples in our set. Congratulations!\", 'green'))"
      ],
      "metadata": {
        "id": "XXnDCWtcYoqs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Auxiliary function that calls WAS to log output\n",
        "# By default it will bring 100 last interactions, so use the page_limit parameter\n",
        "def get_logs(wid, pl, to=200):\n",
        "    response = assistant.list_logs( workspace_id = wid, page_limit = pl ).get_result()\n",
        "    cursor_regex = r\".*?cursor=(.*?)&\"\n",
        "    logs = response['logs']\n",
        "    page = response['pagination']\n",
        "\n",
        "    total = 0\n",
        "    while response:\n",
        "        total += 1\n",
        "        cont = 0\n",
        "        if not page: #'pagination' not in response or 'next_url' not in response['pagination']:\n",
        "            break\n",
        "    \n",
        "        cursor_res = re.search(cursor_regex, page['next_url'], re.IGNORECASE)\n",
        "        #cursor_res = re.search(cursor_regex, response['pagination']['next_url'], re.IGNORECASE)\n",
        "        cursor = None\n",
        "    \n",
        "        if cursor_res and total<to:\n",
        "            cursor = cursor_res.group(1)\n",
        "        if not cursor:\n",
        "            break\n",
        "     \n",
        "        response = assistant.list_logs(workspace_id=wid, page_limit=pl, cursor=cursor).get_result()\n",
        "        logs += response['logs']\n",
        "        page = response['pagination']\n",
        "\n",
        "    return logs"
      ],
      "metadata": {
        "id": "l7BHgaJNWkg5"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gather logs of all conversations made in the workspace\n",
        "logs = get_logs(original_workspace_id, 1, 10)\n",
        "len(logs)"
      ],
      "metadata": {
        "id": "fTyYJov0W5SZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Auxiliary function that calls WAS to log output\n",
        "# By default it will bring 100 last interactions, so use the page_limit parameter\n",
        "def mount_logs_dump(logs):\n",
        "    list_mount_examples = []\n",
        "    list_mount_intents = []\n",
        "    list_mount_intents2 = []\n",
        "    list_mount_confidence = []\n",
        "    list_mount_confidence2 = []\n",
        "    list_mount_entities = []\n",
        "    list_mount_erro = []\n",
        "\n",
        "    for log in logs:\n",
        "        \n",
        "      if log[\"response\"]:\n",
        "        lresponse = log['response']\n",
        "        if 'input' in lresponse and 'text' in lresponse['input']:\n",
        "\n",
        "          if 'output' in lresponse and 'log_messages' in lresponse[\"output\"] and len(lresponse[\"output\"][\"log_messages\"]) > 0:\n",
        "            list_mount_erro.append(lresponse[\"output\"][\"log_messages\"][0][\"msg\"])\n",
        "          else:\n",
        "            list_mount_erro.append('')\n",
        "\n",
        "          if 'entities' in lresponse and lresponse['entities']:\n",
        "            list_mount_entities.append(lresponse['entities'][0])\n",
        "          else:\n",
        "            list_mount_entities.append('')\n",
        "\n",
        "          if 'intents' in lresponse and lresponse['intents']:\n",
        "            list_mount_examples.append(lresponse['input']['text'].strip())\n",
        "            list_mount_intents.append(lresponse['intents'][0]['intent'])\n",
        "            list_mount_confidence.append(lresponse['intents'][0]['confidence'])\n",
        "\n",
        "            if 'alternate_intents' in log['request'] and log['request']['alternate_intents'] == True:\n",
        "              list_mount_intents2.append(lresponse['intents'][1]['intent'])\n",
        "              list_mount_confidence2.append(lresponse['intents'][1]['confidence'])\n",
        "            else:\n",
        "              list_mount_intents2.append('N/A')\n",
        "              list_mount_confidence2.append('0')\n",
        "          else:\n",
        "            list_mount_examples.append(lresponse['input']['text'].strip())\n",
        "            list_mount_intents.append('irrelevant')\n",
        "            list_mount_confidence.append('0')\n",
        "            list_mount_intents2.append('N/A')\n",
        "            list_mount_confidence2.append('0')\n",
        "    \n",
        "\n",
        "    df_temp = pd.DataFrame({\n",
        "        'Example': list_mount_examples,\n",
        "        'Intent_1': list_mount_intents,\n",
        "        'Confidence_1': list_mount_confidence,\n",
        "        'Intent_2': list_mount_intents2,\n",
        "        'Confidence_2': list_mount_confidence2,\n",
        "        'Entities': list_mount_entities,\n",
        "        'Erro': list_mount_erro\n",
        "    }, columns=['Example','Intent_1','Confidence_1','Intent_2','Confidence_2', 'Entities', 'Erro'])\n",
        "    \n",
        "    return df_temp"
      ],
      "metadata": {
        "id": "GOuTftrMYpf4"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gather logs of all conversations made in the workspace\n",
        "list_logs = mount_logs_dump(logs)\n",
        "display(list_logs)\n",
        "\n",
        "# Check for all logs\n",
        "flag_logs = len(list_logs) > 0"
      ],
      "metadata": {
        "id": "70ls9uXrYsfq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount our data frame for later viewing\n",
        "if flag_logs:\n",
        "  dist_logs = pd.DataFrame(columns=['intent', 'sizes','confidencesum', 'entities', 'erro'])\n",
        "\n",
        "  # There is no need to analyze these\n",
        "  intent_blacklist = ['greetings','fim']\n",
        "\n",
        "  for idx,log in list_logs.iterrows():\n",
        "      if log['Intent_1'] not in intent_blacklist:\n",
        "          dist_logs.loc[idx] = pd.Series({'intent': ('Erro' if (log['Erro'] != \"\") else log['Intent_1']), 'entities': log['Entities'], 'erro': log['Erro'], 'sizes': 1, 'confidencesum': float(log['Confidence_1'])})\n",
        "          dist_logs"
      ],
      "metadata": {
        "id": "IFJgh6_RYuv1"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculates overall average confidence\n",
        "if flag_logs:\n",
        "    print(\"Average confidence (with irrelevant) = \" + str(round(dist_logs['confidencesum'].mean()*100,2)) + \" % \")\n",
        "    #print(\"Average confidence (without irrelevant) = \" + str(round(dist_logs[dist_logs['intent'] != 'irrelevant']['confidencesum'].mean()*100,2)) + \" % \")\n",
        "    print(\"Average confidence (with irrelevant, without entities) = \" + str(round(dist_logs[(dist_logs['entities'] == \"\")]['confidencesum'].mean()*100,2)) + \" % \")\n",
        "    print(\"Average confidence (without irrelevant) = \" + str(round(dist_logs[dist_logs['confidencesum'] > 0.10]['confidencesum'].mean()*100,2)) + \" % \")\n",
        "    print(\"Average confidence (without irrelevant, entities) = \" + str(round(dist_logs[(dist_logs['confidencesum'] > 0.10) & (dist_logs['entities'] == \"\")]['confidencesum'].mean()*100,2)) + \" % \")\n",
        "    print(\"Average confidence (without irrelevant, entities, erro) = \" + str(round(dist_logs[(dist_logs['confidencesum'] > 0.10) & (dist_logs['entities'] == \"\") & (dist_logs['erro'] == \"\")]['confidencesum'].mean()*100,2)) + \" % \")\n",
        "    \n",
        "    \n",
        "    print(\"Volume:\", len(dist_logs), \"Erro:\", len(dist_logs[dist_logs['erro'] != \"\"]), \"=\", str(round((len(dist_logs[dist_logs['erro'] != \"\"])*100) / len(dist_logs),2)),\"%\")"
      ],
      "metadata": {
        "id": "vkZB4aaSYxBx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Group intentions, sum events, and calculate the average confidence for each.\n",
        "if flag_logs:\n",
        "\n",
        "    dist_logs_filter = dist_logs[dist_logs['entities'] == \"\"]\n",
        "    \n",
        "    # Groups of intentions\n",
        "    grouped1 = dist_logs_filter.groupby('intent').mean()\n",
        "    grouped2 = dist_logs_filter.groupby('intent').count()\n",
        "    \n",
        "    cte_logs = 5\n",
        "\n",
        "    print(colored('More asked intentions with more than ' + str(cte_logs) + ' samples\\n', attrs=['bold']))\n",
        "    grouped4 = grouped2.where(lambda x : x['sizes'] > cte_logs).dropna().sort_values(by='sizes')\n",
        "    \n",
        "    grouped4.index.name='intent'\n",
        "    grouped4['intent']=grouped4.index\n",
        "    \n",
        "    # Displays the distribution log on the screen\n",
        "    grouped4.plot(kind='bar',x='intent', y='confidencesum',figsize=(30,7),title='Most-asked intentions with more than ' + str(cte_logs) + ' samples')"
      ],
      "metadata": {
        "id": "6CiiogWoYzJM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if flag_logs:\n",
        "    # Calculates the intersection between series\n",
        "    df_intersection = pd.merge(grouped4, grouped1, left_index=True, right_index=True)\n",
        "    df_intersection.drop('confidencesum_x', axis=1, inplace=True)\n",
        "    \n",
        "    print(colored('Average confidence by intention (with more than ' + str(cte_logs) + ' samples):\\n', attrs=['bold']))\n",
        "    display(df_intersection.sort_values(by='confidencesum_y'))"
      ],
      "metadata": {
        "id": "JJf5Ta9YY1F4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prints graph with log distribution vs. general confidence\n",
        "\n",
        "if flag_logs:\n",
        "    df_intersection.sort_values(by='confidencesum_y').plot(kind='bar',x='intent', y='confidencesum_y',figsize=(30,7),title='Average confidence by intention (with more than ' + str(cte_logs) + ' samples)')"
      ],
      "metadata": {
        "id": "Emzy2JF-Y3Ew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print mixed graphics\n",
        "if flag_logs:\n",
        "    # Removes unnecessary columns\n",
        "    grouped4.drop('confidencesum', axis=1, inplace=True)\n",
        "    df_intersection.drop('sizes', axis=1, inplace=True)\n",
        "\n",
        "    ax = grouped4.plot(kind='line', x='intent', y='sizes', figsize=(30,7), color='b', linewidth=3, secondary_y=True, legend=True)\n",
        "    df_intersection.plot(ax=ax, kind='bar', x='intent', y='confidencesum_y', figsize=(30,7), title='Average confidence by intention x size', legend=True)\n",
        "    \n",
        "    plt.gcf().autofmt_xdate()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "uJ6WQtXhY5cw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MIN_CONFIDENCE = 0.5\n",
        "\n",
        "if flag_logs:\n",
        "    counter = 0\n",
        "    arr_inputs = []\n",
        "    arr_intents = []\n",
        "    arr_confidence = []\n",
        "    arr_repeated = []\n",
        "\n",
        "    for idx,log in list_logs.iterrows():\n",
        "        current_intent = log['Intent_1']\n",
        "\n",
        "        if current_intent not in intent_blacklist and float(log['Confidence_1']) < MIN_CONFIDENCE:\n",
        "            arr_repeated.append(log['Example'])\n",
        "\n",
        "            if float(log['Confidence_1']) < MIN_CONFIDENCE and log['Example'] not in arr_inputs:\n",
        "                counter = counter + 1\n",
        "                arr_inputs.append(log['Example'])\n",
        "                arr_intents.append(log['Intent_1'])\n",
        "                arr_confidence.append(log['Confidence_1'])\n",
        "    \n",
        "    low_confidence = pd.DataFrame({\n",
        "        'Example': arr_inputs,\n",
        "        'Intent': arr_intents,\n",
        "        'Confidence': arr_confidence,\n",
        "    }, columns=['Example','Intent','Confidence'])\n",
        "\n",
        "    print(display(arr_inputs[0:20]))"
      ],
      "metadata": {
        "id": "KCjHahE2Y7gQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if flag_logs:\n",
        "    print(colored(\"\\nIt selects the examples with greater occurrence (more than 2 repetitions) and less confidence:\\n\", attrs=['bold']))\n",
        "\n",
        "    fdist = nltk.FreqDist(arr_repeated)\n",
        "    flag = False\n",
        "\n",
        "    for k,v in sorted(fdist.items(), key=lambda t:t[-1], reverse=True):\n",
        "        if v > 2:\n",
        "            flag = True\n",
        "            print(\"[\" + str(v) + \"] > \",k)\n",
        "    if flag is False:\n",
        "        print(\"No repeated logs were found.\")"
      ],
      "metadata": {
        "id": "1AKTIyfXY_PZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mounts the dataframe to be worked with\n",
        "le_examples = []\n",
        "le_intents = []\n",
        "\n",
        "for intent in list_original_intents['intents']:\n",
        "    examples = assistant.list_examples(\n",
        "    workspace_id = original_workspace_id,\n",
        "    intent = intent['intent']\n",
        "    ).get_result()\n",
        "        \n",
        "    for ex in examples['examples']:\n",
        "        le_examples.append(ex['text'])\n",
        "        le_intents.append(intent['intent'])\n",
        "        \n",
        "list_ml_examples = pd.DataFrame({\n",
        "    'Example': le_examples,\n",
        "    'Intent': le_intents,\n",
        "}, columns=['Example','Intent'])"
      ],
      "metadata": {
        "id": "xqZV0d58ZCjw"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Because we need to submit some examples to test the template, this task can generate costs if you have already exceeded 10,000 free monthly requests.\n",
        "length_test_set = len(list_ml_examples)*0.2*3\n",
        "print(\"Exemplos: \"+ str(len(list_ml_examples)*0.2*3))\n",
        "\n",
        "print(colored('ATENTION! When you run this test, you may be charged up to US$ ' + str(round(length_test_set*0.0025,2)) + '. Those costs are associated with ' \n",
        "              + str(round(length_test_set)) + ' calls to be made to your workspace! Enter OK to accept...','red', attrs=['bold']))\n",
        "\n",
        "acceptance = input()\n",
        "\n",
        "if acceptance == 'OK'or acceptance == 'Ok'or acceptance == 'ok':\n",
        "    print(\"OK! Let's continue...\")"
      ],
      "metadata": {
        "id": "1gf2VE_qZFp4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# An auxiliary function that groups intentions together and assembles an array that the WAS can understand. It also validates the number of examples in each intent.\n",
        "def group_and_mount_intents(train_set):\n",
        "    \n",
        "    intents = {}\n",
        "    \n",
        "    # Groups of intentions\n",
        "    for idx, example in train_set.iterrows():\n",
        "        current_intent = example['Intent']\n",
        "        \n",
        "        if current_intent not in intents: \n",
        "            intents[current_intent] = []\n",
        "\n",
        "        intents[current_intent].append(example['Example'])\n",
        "        \n",
        "\n",
        "    workspace_intens = []\n",
        "    \n",
        "    # Transforms the format of intent into what is accepted by the WAS\n",
        "    for intent, examples in intents.items():\n",
        "        entry = {'intent': intent, 'examples': []}\n",
        "        if len(examples) < 10:\n",
        "            print(\"[ATTENTION] Intent #\" + intent + \" has few examples [\" + str(len(examples)) + \"] than expected, and therefore is not a good example for this test..\")\n",
        "\n",
        "        for example in examples:\n",
        "            entry['examples'].append({ 'text': example })\n",
        "\n",
        "        workspace_intens.append(entry)\n",
        "\n",
        "    print('\\nIntentions mounted.')\n",
        "\n",
        "    return workspace_intens\n",
        "\n",
        "# Auxiliary function that creates a new workspace to run tests without damaging the current chatbot\n",
        "def create_test_workspace(train_set):\n",
        "    intents_json = group_and_mount_intents(train_set)\n",
        "    \n",
        "    entities_list = assistant.list_entities(\n",
        "        workspace_id = original_workspace_id\n",
        "    ).get_result()    \n",
        "    \n",
        "    response = assistant.create_workspace(name = 'Create_by_Watson_Studio', entities = entities_list['entities'], intents = intents_json, language = 'pt-br')\n",
        "\n",
        "    print('Skill replicated, wait to be ready...')\n",
        "        \n",
        "    #print(response.result[\"workspace_id\"])\n",
        "    #print(response)\n",
        "\n",
        "    check_wksp_status(response.result[\"workspace_id\"])\n",
        "\n",
        "    #check_wksp_status(response[\"workspace_id\"])\n",
        "    \n",
        "    return response.result[\"workspace_id\"]\n",
        "\n",
        "\n",
        "def mount_confusion_matrix(test_set,teste_wid):\n",
        "    cm_predicted = []\n",
        "    cm_predicted_2 = []\n",
        "\n",
        "    cm_conf_p1 = []\n",
        "    cm_conf_p2 = []\n",
        "    cm_delta = []\n",
        "\n",
        "    cm_true = []\n",
        "    cm_true_q = []\n",
        "    \n",
        "    for index, row in test_set.iterrows():\n",
        "        message = { 'text': row['Example'] }\n",
        "        \n",
        "        response = assistant.message(workspace_id=teste_wid,input=message,alternate_intents=True).get_result()\n",
        "\n",
        "        if response['intents'] != []:\n",
        "            cm_true_q.append(row['Example'])\n",
        "            cm_true.append(row['Intent'])\n",
        "            cm_predicted.append(response['intents'][0]['intent'])\n",
        "            cm_conf_p1.append(response['intents'][0]['confidence'])\n",
        "\n",
        "            if len(response['intents']) > 1:\n",
        "                cm_predicted_2.append(response['intents'][1]['intent'])\n",
        "                cm_conf_p2.append(response['intents'][1]['confidence'])\n",
        "                cm_delta.append(float(response['intents'][0]['confidence']) - float(response['intents'][1]['confidence']))\n",
        "            else:\n",
        "                cm_predicted_2.append('irrelevant')\n",
        "                cm_conf_p2.append(0)\n",
        "                cm_delta.append(1)\n",
        "    \n",
        "    resultados = pd.DataFrame({\n",
        "        'Question': cm_true_q,\n",
        "        'True': cm_true,\n",
        "        'Predicted_1': cm_predicted,\n",
        "        'Predicted_2': cm_predicted_2,\n",
        "        'Conf_1': cm_conf_p1,\n",
        "        'Conf_2': cm_conf_p2,\n",
        "        'Delta': cm_delta\n",
        "    }, columns=['Question','True','Predicted_1','Predicted_2','Conf_1','Conf_2','Delta','Missed'])\n",
        "\n",
        "    resultados['Missed'] = resultados.apply(lambda x : 'X' if x['True'] != x['Predicted_1'] else '', axis=1)\n",
        "    \n",
        "    return resultados, cm_true, cm_predicted, cm_predicted_2\n",
        "\n",
        "def plot_confusion_matrix(cm, classes=None, normalize=False, title='', cmap=plt.cm.Blues):\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Matrix of confusion, without normalization')\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j],\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')"
      ],
      "metadata": {
        "id": "oWYNWb9KZJcC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "N_ROUNDS = 1\n",
        "run_wids = []\n",
        "run_total_df_result = pd.DataFrame(columns=['Question','True','Predicted_1','Predicted_2','Conf_1','Conf_2','Delta','Missed'])\n",
        "run_total_true = []\n",
        "run_total_predicted = []\n",
        "run_total_predicted_2 = []\n",
        "\n",
        "for run_counter in range(0,N_ROUNDS):\n",
        "    \n",
        "    # Divide the training and test sets with a ratio of 80/20\n",
        "    X_train, X_test, y_train, y_test = train_test_split(list_ml_examples, list_ml_examples.Intent, test_size=0.2)#, stratify=list_ml_examples.Intent)\n",
        "    \n",
        "    # Create Skill for test\n",
        "    test_wid = create_test_workspace(X_train)\n",
        "    \n",
        "    run_temp_result, run_temp_true, run_temp_predicted, run_temp_predicted_2 = mount_confusion_matrix(X_test,test_wid)\n",
        "    \n",
        "    run_wids.append(test_wid)\n",
        "    run_total_df_result = pd.concat([run_total_df_result,run_temp_result])\n",
        "    run_total_true = run_total_true + run_temp_true\n",
        "    run_total_predicted = run_total_predicted + run_temp_predicted\n",
        "    run_total_predicted_2 = run_total_predicted + run_temp_predicted_2\n",
        "    \n",
        "    run_counter = run_counter + 1\n",
        "    \n",
        "    # Automatically deletes the Skill / Workspace created for testing\n",
        "    assistant.delete_workspace(workspace_id=test_wid)"
      ],
      "metadata": {
        "id": "bRZI2R_pZKW0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = run_total_df_result['True'].drop_duplicates().tolist()\n",
        "class_names.append('irrelevant')\n",
        "\n",
        "figure_size = (50,50)\n",
        "mpl.rcParams['figure.figsize'] = figure_size\n",
        "\n",
        "cnf_matrix = confusion_matrix(run_total_true, run_total_predicted, labels=class_names)\n",
        "plot_confusion_matrix(cnf_matrix, classes=class_names, title='Matrix of Confusion for Intentions')"
      ],
      "metadata": {
        "id": "LYaVQsixZOYJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Precision\")\n",
        "print(\"Weighted: \" + str(precision_score(run_total_true, run_total_predicted, average='weighted')))\n",
        "\n",
        "print(\"\\n\\nRecall\")\n",
        "print(\"Weighted: \" + str(recall_score(run_total_true, run_total_predicted, average='weighted')))\n",
        "\n",
        "print(\"\\n\\nF1\")\n",
        "print(\"Weighted: \" + str(f1_score(run_total_true, run_total_predicted, average='weighted')))\n",
        "\n",
        "# Metrics for each intent\n",
        "print(classification_report(run_total_true, run_total_predicted))"
      ],
      "metadata": {
        "id": "jx5S46Q7ZQiE"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Colab's New Code Editor",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}