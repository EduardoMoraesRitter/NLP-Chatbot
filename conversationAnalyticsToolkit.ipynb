{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EduardoMoraesRitter/NLP-Chatbot/blob/main/conversationAnalyticsToolkit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Note, on Watson Studio the pip magic command `%pip` is not supported from within the notebook.  Use !pip instead. \n",
        "#!pip install --user conversation_analytics_toolkit\n",
        "%pip install --user conversation_analytics_toolkit"
      ],
      "metadata": {
        "id": "kbHlagyPVkra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('words')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "u7gz2ClCaP5P",
        "outputId": "2d289bf2-0001-4123-8ad1-f10417da618e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import conversation_analytics_toolkit\n",
        "\n",
        "from conversation_analytics_toolkit import wa_assistant_skills, transformation, analysis, visualization, wa_adaptor, transcript, flows, keyword_analysis, sentiment_analysis\n",
        "from conversation_analytics_toolkit import selection as vis_selection\n",
        "from conversation_analytics_toolkit import filtering2 as filtering\n",
        "\n",
        "import json\n",
        "import pandas as pd\n",
        "from pandas.io.json import json_normalize\n",
        "from IPython.core.display import display, HTML\n",
        "\n",
        "import ibm_watson\n",
        "from ibm_watson import AssistantV1\n",
        "from ibm_cloud_sdk_core.authenticators import IAMAuthenticator\n",
        "\n",
        "# fill it out with your WAS credentials\n",
        "workspace_id = \"\"\n",
        "WAS_API_KEY = \"\"\n",
        "WAS_URL = \"https://api.us-east.assistant.watson.cloud.ibm.com\""
      ],
      "metadata": {
        "id": "eCa780-6aT8s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set pandas to show more rows and columns\n",
        "#pd.set_option('display.max_rows', 200)\n",
        "pd.set_option('display.max_colwidth', -1)"
      ],
      "metadata": {
        "id": "pScJvYntaklf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fetch and load a workspace"
      ],
      "metadata": {
        "id": "cRHpLQ6KamIZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "authenticator = IAMAuthenticator(WAS_API_KEY)\n",
        "service = AssistantV1(version='2019-02-20', authenticator = authenticator)\n",
        "\n",
        "service.set_service_url(WAS_URL)\n",
        "\n",
        "# or fetch one via the APIs\n",
        "workspaces = service.list_workspaces().get_result()\n",
        "#workspaces['workspaces']\n",
        "#workspace_id = service['workspaces'][0]['workspace_id']\n",
        "#workspace_id"
      ],
      "metadata": {
        "id": "NTE39Uikamsp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#fetch the workspace\n",
        "workspace=service.get_workspace(workspace_id=workspace_id, export=True).get_result()\n",
        "#workspace = assistant.get_workspace(workspace_id=check_workspace_id).get_result()\n",
        "workspace"
      ],
      "metadata": {
        "id": "WF0yrQf-apy3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set query parameters\n",
        "limit_number_of_records=2000\n",
        "\n",
        "# example of time range query\n",
        "query_filter = \"response_timestamp>=2022-10-13,response_timestamp<2022-10-14\" #query_filter = None\n",
        "\n",
        "# Fetch the logs for the workspace\n",
        "df_logs = wa_adaptor.read_logs(service, workspace_id, limit_number_of_records, query_filter)\n",
        "#response = assistant.list_logs( workspace_id = wid, page_limit = pl ).get_result()\n",
        "df_logs"
      ],
      "metadata": {
        "id": "YJX7Wjd7aqQm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extract and Transform"
      ],
      "metadata": {
        "id": "sKDlzzWmawSu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#if you have more than one skill, you can add multiple skill definitions\n",
        "skill_id = workspace_id\n",
        "assistant_skills = wa_assistant_skills.WA_Assistant_Skills()\n",
        "assistant_skills.add_skill(skill_id, workspace)\n",
        "\n",
        "#validate the number of workspace_ids\n",
        "print(\"workspace_ids in skills: \" + pd.DataFrame(assistant_skills.list_skills())[\"skill_id\"].unique())\n",
        "print(\"workspace_ids in logs: \"+ df_logs.workspace_id.unique())"
      ],
      "metadata": {
        "id": "Ur6kuZu_astw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_logs_canonical = transformation.to_canonical_WA_v2(df_logs, assistant_skills, skill_id_field=None, include_nodes_visited_str_types=True, include_context=True);\n",
        "#df_logs_canonical = transformation.to_canonical_WA_v2(df_logs, assistant_skills, skill_id_field=\"workspace_id\", include_nodes_visited_str_types=True, include_context=False)\n",
        "df_logs_canonical.head()"
      ],
      "metadata": {
        "id": "m5UQPwVga6q1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the rest of the notebook runs on the df_logs_to_analyze object.  \n",
        "df_logs_to_analyze = df_logs_canonical.copy(deep=False)\n",
        "df_logs_to_analyze.head(2)"
      ],
      "metadata": {
        "id": "NiPEjknIbDTd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualizing user journeys and abandonments"
      ],
      "metadata": {
        "id": "lhW485qCbFvm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "title = \"All Conversations\"\n",
        "turn_based_path_flows = analysis.aggregate_flows(df_logs_to_analyze, mode=\"turn-based\", on_column=\"turn_label\", max_depth=400, trim_reroutes=False)\n",
        "# increase the width of the Jupyter output cell   \n",
        "display(HTML(\"<style>.container { width:95% !important; }</style>\"))\n",
        "config = {\n",
        "    'commonRootPathName': title, # label for the first root node \n",
        "    'height': 800, # control the visualization height.  Default 600\n",
        "    'nodeWidth': 250, \n",
        "    'maxChildrenInNode': 6, # control the number of immediate children to show (and collapse rest into *others* node).  Default 5\n",
        "    'linkWidth' : 400,  # control the width between pathflow layers.  Default 360     'sortByAttribute': 'flowRatio'  # control the sorting of the chart. (Options: flowRatio, dropped_offRatio, flows, dropped_off, rerouted)\n",
        "    'sortByAttribute': 'flowRatio',\n",
        "    'title': title,\n",
        "    'mode': \"turn-based\"\n",
        "}\n",
        "jsondata = json.loads(turn_based_path_flows.to_json(orient='records'))\n",
        "visualization.draw_flowchart(config, jsondata, python_selection_var=\"selection\")"
      ],
      "metadata": {
        "id": "11cBD5cHbD2f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualize flows in all conversations"
      ],
      "metadata": {
        "id": "dJ90hEZ6bNzB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "title = \"All Conversations\"\n",
        "turn_based_path_flows = analysis.aggregate_flows(df_logs_to_analyze, mode=\"turn-based\", on_column=\"turn_label\", max_depth=400, trim_reroutes=False)\n",
        "# increase the width of the Jupyter output cell   \n",
        "display(HTML(\"<style>.container { width:95% !important; }</style>\"))\n",
        "config = {\n",
        "    'commonRootPathName': title, # label for the first root node \n",
        "    'height': 800, # control the visualization height.  Default 600\n",
        "    'nodeWidth': 250, \n",
        "    'maxChildrenInNode': 6, # control the number of immediate children to show (and collapse rest into *others* node).  Default 5\n",
        "    'linkWidth' : 400,  # control the width between pathflow layers.  Default 360     'sortByAttribute': 'flowRatio'  # control the sorting of the chart. (Options: flowRatio, dropped_offRatio, flows, dropped_off, rerouted)\n",
        "    'sortByAttribute': 'flowRatio',\n",
        "    'title': title,\n",
        "    'mode': \"turn-based\"\n",
        "}\n",
        "jsondata = json.loads(turn_based_path_flows.to_json(orient='records'))\n",
        "visualization.draw_flowchart(config, jsondata, python_selection_var=\"selection\")"
      ],
      "metadata": {
        "id": "TXqYzAE-bLaB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualize flows in subset of conversations"
      ],
      "metadata": {
        "id": "9X6MNuwWbR--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# filter the conversations that include escalation\n",
        "title2=\"Banking Card Escalated\"\n",
        "filters = filtering.ChainFilter(df_logs_to_analyze).setDescription(title2) \n",
        "# node with condition on the #Banking-Card_Selection (node_1_1510880732839) and visit the node \"Transfer To Live Agent\" (node_25_1516679473977)\n",
        "#filters.by_dialog_node_id('node_1_1510880732839').by_dialog_node_id('node_25_1516679473977')\n",
        "filters.printConversationFilters() \n",
        "# get a reference to the dataframe.  Note: you can get access to intermediate dataframes by calling getDataFrame(index)"
      ],
      "metadata": {
        "id": "sGIo_IDAbUfi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_df = filters.getDataFrame()\n",
        "turn_based_path_flows = analysis.aggregate_flows(filtered_df, mode=\"turn-based\", on_column=\"turn_label\", max_depth=400, trim_reroutes=False)  \n",
        "config = {\n",
        "    'commonRootPathName': title2,  'title': title2,\n",
        "    'height': 800,  'nodeWidth': 250, 'maxChildrenInNode': 6, 'linkWidth' : 400, 'sortByAttribute': 'flowRatio',\n",
        "    'mode': \"turn-based\"\n",
        "}\n",
        "jsondata = json.loads(turn_based_path_flows.to_json(orient='records'))\n",
        "visualization.draw_flowchart(config, jsondata, python_selection_var=\"selection\")"
      ],
      "metadata": {
        "id": "3-OD0n9fbfTK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analyzing reverse flows"
      ],
      "metadata": {
        "id": "nGBzCrA9bhrC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# filter only conversations that include Confirming Live Agent Transfer.  In addition, remove true nodes, which simplifies the journey\n",
        "#removeing the true node, helps to make the visualization simpler\n",
        "#cf = filtering.ChainFilter(df_logs_canonical).setDescription(\"Escalated conversations\").by_turn_label(\"Confirming Live Agent Transfer\").remove_turn_by_label(\"true\").printConversationFilters()\n",
        "cf = filtering.ChainFilter(df_logs_canonical).remove_turn_by_label(\"true\").printConversationFilters()\n",
        "\n",
        "escalated_df = cf.getDataFrame()\n",
        "#addiginal remove duplicates consecutive states will also make the visualization simpler\n",
        "escalated_simplified_df = analysis.simplify_consecutive_duplicates(escalated_df, on_column=\"turn_label\")"
      ],
      "metadata": {
        "id": "jZ7W6y3qbixu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#note reverse=True in the aggregation step\n",
        "escalated_similified_reversed_df = analysis.aggregate_flows(escalated_simplified_df, mode=\"turn-based\", on_column=\"turn_label\", max_depth=30, trim_reroutes=False, reverse=True)"
      ],
      "metadata": {
        "id": "xADf6j9Bblp9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "title3 =  'Escalation Flows (reversed)'\n",
        "config = {\n",
        "    'commonRootPathName': title3,  'title': title3,\n",
        "    'height': 800,  'nodeWidth': 250, 'maxChildrenInNode': 6, 'linkWidth' : 400, 'sortByAttribute': 'flowRatio',\n",
        "    'mode': \"turn-based\"\n",
        "}\n",
        "jsondata = json.loads(escalated_similified_reversed_df.to_json(orient='records'))\n",
        "visualization.draw_flowchart(config, jsondata, python_selection_var=\"selection\")"
      ],
      "metadata": {
        "id": "HgVHVtF-bm6-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analyzing trends in flows"
      ],
      "metadata": {
        "id": "7UGWzZ46bodS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we will simiulate this on the previous chart (reverse escalated flows) by comparing the first and second 1000 logs\n",
        "prev_period_df = escalated_simplified_df[0:1000]\n",
        "curr_period_df = escalated_simplified_df[1001:2000]\n",
        "\n",
        "prev_flows = analysis.aggregate_flows(prev_period_df, mode=\"turn-based\", on_column=\"turn_label\", max_depth=30, trim_reroutes=False, reverse=True)\n",
        "curr_flows = analysis.aggregate_flows(curr_period_df, mode=\"turn-based\", on_column=\"turn_label\", max_depth=30, trim_reroutes=False, reverse=True)\n",
        "\n",
        "df_logs_compare_periods = analysis.merge_compare_flows(curr_flows, prev_flows)"
      ],
      "metadata": {
        "id": "D4oU3KyNbo4f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "title3 =  'Trending flows (curr vs. prev)'\n",
        "config = {\n",
        "    'commonRootPathName': title3,  'title': title3,\n",
        "    'height': 800,  'nodeWidth': 250, 'maxChildrenInNode': 6, 'linkWidth' : 400, 'sortByAttribute': 'flowRatio',\n",
        "    'mode': \"turn-based\"\n",
        "}\n",
        "jsondata = json.loads(df_logs_compare_periods.to_json(orient='records'))\n",
        "visualization.draw_flowchart(config, jsondata, python_selection_var=\"selection\")"
      ],
      "metadata": {
        "id": "Gu8nHk_CbrZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualize dialog flow (milestone-based)"
      ],
      "metadata": {
        "id": "EumWqMY_b0dn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#define the milestones and corresponding node ids for the `Schedule Appointment` task\n",
        "milestone_analysis = analysis.MilestoneFlowGraph(assistant_skills.get_skill_by_id(skill_id))\n",
        "milestone_analysis\n",
        "# milestone_analysis.add_milestones([\"Appointment scheduling start\",  \"Schedule time\", \"Enter zip code\", \"Branch selection\", \n",
        "#                     \"Enter purpose of appointment\", \"Scheduling completion\"])\n",
        "\n",
        "# milestone_analysis.add_node_to_milestone(\"node_21_1513047983871\", \"Appointment scheduling start\")   \n",
        "# milestone_analysis.add_node_to_milestone(\"handler_28_1513048122602\", \"Schedule time\")"
      ],
      "metadata": {
        "id": "HejsKEhQb01j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#enrich with milestone information - will add a column called 'milestone'\n",
        "milestone_analysis.enrich_milestones(df_logs_to_analyze)\n",
        "#remove all log records without a milestone\n",
        "#df_milestones = df_logs_to_analyze[pd.isna(df_logs_to_analyze[\"milestone\"]) == False]\n",
        "df_milestones = df_logs_to_analyze[pd.isna(df_logs_to_analyze) == False]\n",
        "#optionally, remove consecutive milestones for a more simplified flow visualization representation\n",
        "df_milestones = analysis.simplify_flow_consecutive_milestones(df_milestones)"
      ],
      "metadata": {
        "id": "07FpC5lob69j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compute the aggregate flows of milestones \n",
        "computed_flows= analysis.aggregate_flows(df_milestones, mode=\"milestone-based\", on_column=\"milestone\", max_depth=30, trim_reroutes=False)\n",
        "config = {\n",
        "    'commonRootPathName': 'All Conversations', # label for the first root node \n",
        "    'height': 800, # control the visualization height.  Default 600\n",
        "    'maxChildrenInNode': 6, # control the number of immediate children to show (and collapse the rest into *other* node).  Default 5\n",
        "#     'linkWidth' : 400,  # control the width between pathflow layers.  Default 360     '\n",
        "    'sortByAttribute': 'flowRatio', # control the sorting of the chart. (Options: flowRatio, dropped_offRatio, flows, dropped_off, rerouted)\n",
        "    'title': \"Abandoned Conversations in Appointment Schedule Flow\",\n",
        "    'showVisitRatio' : 'fromTotal', # default: 'fromTotal'.  'fromPrevious' will compute percentages from previous step,\n",
        "    'mode': 'milestone-based'\n",
        "}\n",
        "jsondata = json.loads(computed_flows.to_json(orient='records'))\n",
        "visualization.draw_flowchart(config, jsondata, python_selection_var=\"milestone_selection\")"
      ],
      "metadata": {
        "id": "TNJPTI3Cb90j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Select conversations at the point of abandonment"
      ],
      "metadata": {
        "id": "cHFsHzUXcAEg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#the selection variable contains details about the selected node, and conversations that were abandoned at that point\n",
        "print(\"Selected Path: \",milestone_selection[\"path\"])\n",
        "#fetch the dropped off conversations from the selection  \n",
        "dropped_off_conversations = vis_selection.to_dataframe(milestone_selection)[\"dropped_off\"]\n",
        "print(\"The selection contains {} records, with a reference back to the converstion logs\".format(str(len(dropped_off_conversations))))\n",
        "dropped_off_conversations.head()"
      ],
      "metadata": {
        "id": "B1BpTxfucAjp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analyzing abandoned conversations"
      ],
      "metadata": {
        "id": "2hwa2m6bcDGA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_logs_to_analyze = sentiment_analysis.add_sentiment_columns(df_logs_to_analyze) \n",
        "#create insights, and highlights annotation for the transcript visualization\n",
        "NEGATIVE_SENTIMENT_THRESHOLD=-0.15 \n",
        "df_logs_to_analyze[\"insights_tags\"] = df_logs_to_analyze.apply(lambda x: [\"Negative Sentiment\"] if x.sentiment < NEGATIVE_SENTIMENT_THRESHOLD else [], axis=1)\n",
        "df_logs_to_analyze[\"highlight\"] = df_logs_to_analyze.apply(lambda x: True if x.sentiment < NEGATIVE_SENTIMENT_THRESHOLD else False, axis=1)"
      ],
      "metadata": {
        "id": "Rf3x9BskcDpj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fetch the conversation records\n",
        "dropped_off_conversations = vis_selection.fetch_logs_by_selection(df_logs_to_analyze, dropped_off_conversations)\n",
        "# visualize using the transcript visualization \n",
        "dfc = transcript.to_transcript(dropped_off_conversations)\n",
        "config = {'debugger': True} \n",
        "visualization.draw_transcript(config, dfc)"
      ],
      "metadata": {
        "id": "NMQnJrYzcFQi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Identify key words and phrases at point of abandonment"
      ],
      "metadata": {
        "id": "XMqJ-FiNcHb_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# gather user utterances from the dropped off conversations - last utterances and all utterances\n",
        "last_utterances_abandoned=vis_selection.get_last_utterances_from_selection(milestone_selection, df_logs_to_analyze)\n",
        "all_utterances_abandoned=vis_selection.get_all_utterances_from_selection(milestone_selection, df_logs_to_analyze)"
      ],
      "metadata": {
        "id": "VbwV_-iEcH5I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summarize frequent keywords and phrases"
      ],
      "metadata": {
        "id": "9vD1aMYBcJ6g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# analyze the last user input before abandonment\n",
        "num_unigrams=10\n",
        "num_bigrams=15\n",
        "custom_stop_words=[\"would\",\"pm\",\"ok\",\"yes\",\"no\",\"thank\",\"thanks\",\"hi\",\"i\",\"you\"]\n",
        "data = keyword_analysis.get_frequent_words_bigrams(last_utterances_abandoned, num_unigrams,num_bigrams,custom_stop_words)    "
      ],
      "metadata": {
        "id": "GQ1xgyJvcKXX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = {'flattened': True, 'width' : 800, 'height' : 500}\n",
        "visualization.draw_wordpackchart(config, data)"
      ],
      "metadata": {
        "id": "Sl7GBu9vcNV3"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Overview of Colaboratory Features",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}