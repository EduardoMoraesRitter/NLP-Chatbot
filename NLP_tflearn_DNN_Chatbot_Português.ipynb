{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP - tflearn.DNN - Chatbot - PortuguÃªs ",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EduardoMoraesRitter/NLP-Chatbot/blob/main/NLP_tflearn_DNN_Chatbot_Portugu%C3%AAs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LhRl9g-mn4g7",
        "outputId": "61a10466-f666-4cd2-d373-eeadfd045ed7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%tensorflow_version 1.x"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow 1.x selected.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3qQKsnrDpnnc",
        "outputId": "174b332a-9854-40d0-9964-e59e14cdc33a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install tflearn"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tflearn in /tensorflow-1.15.2/python3.7 (0.3.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from tflearn) (1.15.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tflearn) (1.19.5)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from tflearn) (7.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OrkgdgsvVi1K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d00bc29c-ba6a-4793-a7d0-ce126bf82e14"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('rslp')\n",
        "from nltk.stem.lancaster import LancasterStemmer\n",
        "#stemmer = LancasterStemmer()\n",
        "stemmer = nltk.stem.RSLPStemmer()\n",
        "\n",
        "import numpy\n",
        "import tflearn\n",
        "import tensorflow\n",
        "import random\n",
        "import json\n",
        "import pickle"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]   Package rslp is already up-to-date!\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tflearn/helpers/summarizer.py:9: The name tf.summary.merge is deprecated. Please use tf.compat.v1.summary.merge instead.\n",
            "\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tflearn/helpers/trainer.py:25: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
            "\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tflearn/collections.py:13: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
            "\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tflearn/config.py:123: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
            "\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tflearn/config.py:129: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.\n",
            "\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tflearn/config.py:131: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tigyVYyemoci",
        "outputId": "df589b68-bb91-4a16-dd7e-eb1ef0c059ae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import json, requests\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/EduardoMoraesRitter/NLP-Chatbot/main/intents.json'\n",
        "resp = requests.get(url)\n",
        "data = json.loads(resp.text)\n",
        "print(data)\n",
        "\n",
        "##OU\n",
        "#with open(\"intents.json\") as file:\n",
        "#    data = json.load(file)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'intents': [{'tag': 'saudacao', 'patterns': ['ola', 'oi', 'como vai', 'eai'], 'responses': ['Ola seja bem vindo', 'como vai?'], 'context_set': ''}, {'tag': 'despedida', 'patterns': ['xau', 'tchau', 'ate mais', 'falando'], 'responses': ['ate logo'], 'context_set': ''}]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cC8cJT-_VcSn"
      },
      "source": [
        "try:\n",
        "    with open(\"data.pickle\", \"rb\") as f:\n",
        "        words, labels, training, output = pickle.load(f)\n",
        "\n",
        "except:\n",
        "    words = []\n",
        "    labels = []\n",
        "    docs_x = []\n",
        "    docs_y = []\n",
        "\n",
        "    for intent in data[\"intents\"]:\n",
        "        for pattern in intent[\"patterns\"]:\n",
        "            wrds = nltk.word_tokenize(pattern)#cortar a frase me palavras\n",
        "            words.extend(wrds)\n",
        "            docs_x.append(wrds)\n",
        "            docs_y.append(intent[\"tag\"])\n",
        "\n",
        "        if intent[\"tag\"] not in labels:\n",
        "            labels.append(intent[\"tag\"])\n",
        "    #print(data)\n",
        "\n",
        "    #criar noss sproprio dicionario de palavras unicas\n",
        "    words = [stemmer.stem(w.lower()) for w in words if w != \"?\"]\n",
        "    words = sorted(list(set(words)))\n",
        "\n",
        "    labels = sorted(labels)\n",
        "\n",
        "    training = []\n",
        "    output = []\n",
        "\n",
        "    out_empty = [0 for _ in range(len(labels))]\n",
        "\n",
        "    for x, doc in enumerate(docs_x):\n",
        "        bag = [] \n",
        "\n",
        "        wrds = [stemmer.stem(w.lower()) for w in doc]\n",
        "\n",
        "        for w in words:\n",
        "            if w in wrds:\n",
        "                bag.append(1)\n",
        "            else:\n",
        "                bag.append(0)\n",
        "\n",
        "        output_row = out_empty[:]\n",
        "        output_row[labels.index(docs_y[x])] = 1\n",
        "\n",
        "        training.append(bag)\n",
        "        output.append(output_row)\n",
        "\n",
        "    training = numpy.array(training)\n",
        "    output = numpy.array(output)\n",
        "\n",
        "    print(training)\n",
        "    print(output)\n",
        "\n",
        "    with open(\"data.pickle\", \"wb\") as f:\n",
        "        pickle.dump((words, labels, training, output), f)\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Da0ynUZNXAOR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01a22e79-cfb9-4ab4-dd8a-14cc0e6680ec"
      },
      "source": [
        "tensorflow.reset_default_graph()\n",
        "\n",
        "net = tflearn.input_data(shape=[None, len(training[0])])\n",
        "net = tflearn.fully_connected(net, 8)\n",
        "net = tflearn.fully_connected(net, 8)\n",
        "net = tflearn.fully_connected(net, len(output[0]), activation=\"softmax\")\n",
        "net = tflearn.regression(net)\n",
        "\n",
        "model = tflearn.DNN(net)\n",
        "\n",
        "model.fit(training, output, n_epoch=1000, batch_size=8, show_metric=True)\n",
        "model.save(\"model.tflearn\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tflearn/layers/core.py:81: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tflearn/layers/core.py:145: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tflearn/initializations.py:174: calling TruncatedNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tflearn/optimizers.py:238: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tflearn/objectives.py:66: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "keep_dims is deprecated, use keepdims instead\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tflearn/objectives.py:70: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tflearn/layers/estimator.py:189: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tflearn/helpers/trainer.py:571: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tflearn/helpers/trainer.py:115: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tflearn/summaries.py:46: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
            "\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tflearn/helpers/trainer.py:134: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tflearn/helpers/trainer.py:164: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tflearn/helpers/trainer.py:165: The name tf.local_variables_initializer is deprecated. Please use tf.compat.v1.local_variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tflearn/helpers/trainer.py:166: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tflearn/helpers/trainer.py:167: The name tf.get_collection_ref is deprecated. Please use tf.compat.v1.get_collection_ref instead.\n",
            "\n",
            "---------------------------------\n",
            "Run id: LKRBP2\n",
            "Log directory: /tmp/tflearn_logs/\n",
            "INFO:tensorflow:Summary name Accuracy/ (raw) is illegal; using Accuracy/__raw_ instead.\n",
            "---------------------------------\n",
            "Training samples: 8\n",
            "Validation samples: 0\n",
            "--\n",
            "Training Step: 1  | time: 0.127s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - acc: 0.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 2  | total loss: \u001b[1m\u001b[32m0.62383\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 002 | loss: 0.62383 - acc: 0.5114 -- iter: 8/8\n",
            "--\n",
            "Training Step: 3  | total loss: \u001b[1m\u001b[32m0.68998\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 003 | loss: 0.68998 - acc: 0.5028 -- iter: 8/8\n",
            "--\n",
            "Training Step: 4  | total loss: \u001b[1m\u001b[32m0.69215\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 004 | loss: 0.69215 - acc: 0.5009 -- iter: 8/8\n",
            "--\n",
            "Training Step: 5  | total loss: \u001b[1m\u001b[32m0.69215\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 005 | loss: 0.69215 - acc: 0.5009 -- iter: 8/8\n",
            "--\n",
            "Training Step: 6  | total loss: \u001b[1m\u001b[32m0.69277\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 006 | loss: 0.69277 - acc: 0.5003 -- iter: 8/8\n",
            "--\n",
            "Training Step: 7  | total loss: \u001b[1m\u001b[32m0.69296\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 007 | loss: 0.69296 - acc: 0.5001 -- iter: 8/8\n",
            "--\n",
            "Training Step: 8  | total loss: \u001b[1m\u001b[32m0.69304\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 008 | loss: 0.69304 - acc: 0.5662 -- iter: 8/8\n",
            "--\n",
            "Training Step: 9  | total loss: \u001b[1m\u001b[32m0.69304\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 009 | loss: 0.69304 - acc: 0.5662 -- iter: 8/8\n",
            "--\n",
            "Training Step: 10  | total loss: \u001b[1m\u001b[32m0.69302\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 010 | loss: 0.69302 - acc: 0.5174 -- iter: 8/8\n",
            "--\n",
            "Training Step: 11  | total loss: \u001b[1m\u001b[32m0.69302\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 011 | loss: 0.69302 - acc: 0.5174 -- iter: 8/8\n",
            "--\n",
            "Training Step: 12  | total loss: \u001b[1m\u001b[32m0.69297\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 012 | loss: 0.69297 - acc: 0.5096 -- iter: 8/8\n",
            "--\n",
            "Training Step: 13  | total loss: \u001b[1m\u001b[32m0.69293\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 013 | loss: 0.69293 - acc: 0.7321 -- iter: 8/8\n",
            "--\n",
            "Training Step: 14  | total loss: \u001b[1m\u001b[32m0.69289\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 014 | loss: 0.69289 - acc: 0.6902 -- iter: 8/8\n",
            "--\n",
            "Training Step: 15  | total loss: \u001b[1m\u001b[32m0.69284\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 015 | loss: 0.69284 - acc: 0.7126 -- iter: 8/8\n",
            "--\n",
            "Training Step: 16  | total loss: \u001b[1m\u001b[32m0.69284\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 016 | loss: 0.69284 - acc: 0.7126 -- iter: 8/8\n",
            "--\n",
            "Training Step: 17  | total loss: \u001b[1m\u001b[32m0.69279\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 017 | loss: 0.69279 - acc: 0.8161 -- iter: 8/8\n",
            "--\n",
            "Training Step: 18  | total loss: \u001b[1m\u001b[32m0.69273\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 018 | loss: 0.69273 - acc: 0.8797 -- iter: 8/8\n",
            "--\n",
            "Training Step: 19  | total loss: \u001b[1m\u001b[32m0.69266\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 019 | loss: 0.69266 - acc: 0.9198 -- iter: 8/8\n",
            "--\n",
            "Training Step: 20  | total loss: \u001b[1m\u001b[32m0.69249\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 020 | loss: 0.69249 - acc: 0.9625 -- iter: 8/8\n",
            "--\n",
            "Training Step: 21  | total loss: \u001b[1m\u001b[32m0.69240\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 021 | loss: 0.69240 - acc: 0.9737 -- iter: 8/8\n",
            "--\n",
            "Training Step: 22  | total loss: \u001b[1m\u001b[32m0.69229\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 022 | loss: 0.69229 - acc: 0.9814 -- iter: 8/8\n",
            "--\n",
            "Training Step: 23  | total loss: \u001b[1m\u001b[32m0.69217\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 023 | loss: 0.69217 - acc: 0.9866 -- iter: 8/8\n",
            "--\n",
            "Training Step: 24  | total loss: \u001b[1m\u001b[32m0.69203\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 024 | loss: 0.69203 - acc: 0.9903 -- iter: 8/8\n",
            "--\n",
            "Training Step: 25  | total loss: \u001b[1m\u001b[32m0.69188\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 025 | loss: 0.69188 - acc: 0.9928 -- iter: 8/8\n",
            "--\n",
            "Training Step: 26  | total loss: \u001b[1m\u001b[32m0.69188\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 026 | loss: 0.69188 - acc: 0.9928 -- iter: 8/8\n",
            "--\n",
            "Training Step: 27  | total loss: \u001b[1m\u001b[32m0.69172\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 027 | loss: 0.69172 - acc: 0.9947 -- iter: 8/8\n",
            "--\n",
            "Training Step: 28  | total loss: \u001b[1m\u001b[32m0.69134\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 028 | loss: 0.69134 - acc: 0.9970 -- iter: 8/8\n",
            "--\n",
            "Training Step: 29  | total loss: \u001b[1m\u001b[32m0.69134\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 029 | loss: 0.69134 - acc: 0.9970 -- iter: 8/8\n",
            "--\n",
            "Training Step: 30  | total loss: \u001b[1m\u001b[32m0.69089\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 030 | loss: 0.69089 - acc: 0.9982 -- iter: 8/8\n",
            "--\n",
            "Training Step: 31  | total loss: \u001b[1m\u001b[32m0.69063\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 031 | loss: 0.69063 - acc: 0.9986 -- iter: 8/8\n",
            "--\n",
            "Training Step: 32  | total loss: \u001b[1m\u001b[32m0.69035\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 032 | loss: 0.69035 - acc: 0.9989 -- iter: 8/8\n",
            "--\n",
            "Training Step: 33  | total loss: \u001b[1m\u001b[32m0.69004\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 033 | loss: 0.69004 - acc: 0.9992 -- iter: 8/8\n",
            "--\n",
            "Training Step: 34  | total loss: \u001b[1m\u001b[32m0.69004\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 034 | loss: 0.69004 - acc: 0.9992 -- iter: 8/8\n",
            "--\n",
            "Training Step: 35  | total loss: \u001b[1m\u001b[32m0.68935\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 035 | loss: 0.68935 - acc: 0.9995 -- iter: 8/8\n",
            "--\n",
            "Training Step: 36  | total loss: \u001b[1m\u001b[32m0.68897\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 036 | loss: 0.68897 - acc: 0.9996 -- iter: 8/8\n",
            "--\n",
            "Training Step: 37  | total loss: \u001b[1m\u001b[32m0.68897\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 037 | loss: 0.68897 - acc: 0.9996 -- iter: 8/8\n",
            "--\n",
            "Training Step: 38  | total loss: \u001b[1m\u001b[32m0.68810\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 038 | loss: 0.68810 - acc: 0.9997 -- iter: 8/8\n",
            "--\n",
            "Training Step: 39  | total loss: \u001b[1m\u001b[32m0.68810\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 039 | loss: 0.68810 - acc: 0.9997 -- iter: 8/8\n",
            "--\n",
            "Training Step: 40  | total loss: \u001b[1m\u001b[32m0.68762\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 040 | loss: 0.68762 - acc: 0.9998 -- iter: 8/8\n",
            "--\n",
            "Training Step: 41  | total loss: \u001b[1m\u001b[32m0.68710\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 041 | loss: 0.68710 - acc: 0.9998 -- iter: 8/8\n",
            "--\n",
            "Training Step: 42  | total loss: \u001b[1m\u001b[32m0.68655\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 042 | loss: 0.68655 - acc: 0.9998 -- iter: 8/8\n",
            "--\n",
            "Training Step: 43  | total loss: \u001b[1m\u001b[32m0.68596\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 043 | loss: 0.68596 - acc: 0.9999 -- iter: 8/8\n",
            "--\n",
            "Training Step: 44  | total loss: \u001b[1m\u001b[32m0.68533\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 044 | loss: 0.68533 - acc: 0.9999 -- iter: 8/8\n",
            "--\n",
            "Training Step: 45  | total loss: \u001b[1m\u001b[32m0.68465\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 045 | loss: 0.68465 - acc: 0.9999 -- iter: 8/8\n",
            "--\n",
            "Training Step: 46  | total loss: \u001b[1m\u001b[32m0.68394\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 046 | loss: 0.68394 - acc: 0.9999 -- iter: 8/8\n",
            "--\n",
            "Training Step: 47  | total loss: \u001b[1m\u001b[32m0.68317\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 047 | loss: 0.68317 - acc: 0.9999 -- iter: 8/8\n",
            "--\n",
            "Training Step: 48  | total loss: \u001b[1m\u001b[32m0.68236\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 048 | loss: 0.68236 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 49  | total loss: \u001b[1m\u001b[32m0.68150\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 049 | loss: 0.68150 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 50  | total loss: \u001b[1m\u001b[32m0.68059\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 050 | loss: 0.68059 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 51  | total loss: \u001b[1m\u001b[32m0.67962\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 051 | loss: 0.67962 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 52  | total loss: \u001b[1m\u001b[32m0.67860\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 052 | loss: 0.67860 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 53  | total loss: \u001b[1m\u001b[32m0.67752\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 053 | loss: 0.67752 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 54  | total loss: \u001b[1m\u001b[32m0.67639\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 054 | loss: 0.67639 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 55  | total loss: \u001b[1m\u001b[32m0.67393\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 055 | loss: 0.67393 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 56  | total loss: \u001b[1m\u001b[32m0.67260\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 056 | loss: 0.67260 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 57  | total loss: \u001b[1m\u001b[32m0.67120\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 057 | loss: 0.67120 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 58  | total loss: \u001b[1m\u001b[32m0.66974\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 058 | loss: 0.66974 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 59  | total loss: \u001b[1m\u001b[32m0.67284\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 059 | loss: 0.67284 - acc: 0.9338 -- iter: 8/8\n",
            "--\n",
            "Training Step: 60  | total loss: \u001b[1m\u001b[32m0.67284\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 060 | loss: 0.67284 - acc: 0.9338 -- iter: 8/8\n",
            "--\n",
            "Training Step: 61  | total loss: \u001b[1m\u001b[32m0.66852\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 061 | loss: 0.66852 - acc: 0.9498 -- iter: 8/8\n",
            "--\n",
            "Training Step: 62  | total loss: \u001b[1m\u001b[32m0.66640\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 062 | loss: 0.66640 - acc: 0.9562 -- iter: 8/8\n",
            "--\n",
            "Training Step: 63  | total loss: \u001b[1m\u001b[32m0.66640\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 063 | loss: 0.66640 - acc: 0.9562 -- iter: 8/8\n",
            "--\n",
            "Training Step: 64  | total loss: \u001b[1m\u001b[32m0.66427\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 064 | loss: 0.66427 - acc: 0.9617 -- iter: 8/8\n",
            "--\n",
            "Training Step: 65  | total loss: \u001b[1m\u001b[32m0.65996\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 065 | loss: 0.65996 - acc: 0.9705 -- iter: 8/8\n",
            "--\n",
            "Training Step: 66  | total loss: \u001b[1m\u001b[32m0.65775\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 066 | loss: 0.65775 - acc: 0.9740 -- iter: 8/8\n",
            "--\n",
            "Training Step: 67  | total loss: \u001b[1m\u001b[32m0.65549\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 067 | loss: 0.65549 - acc: 0.9771 -- iter: 8/8\n",
            "--\n",
            "Training Step: 68  | total loss: \u001b[1m\u001b[32m0.65317\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 068 | loss: 0.65317 - acc: 0.9798 -- iter: 8/8\n",
            "--\n",
            "Training Step: 69  | total loss: \u001b[1m\u001b[32m0.65079\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 069 | loss: 0.65079 - acc: 0.9821 -- iter: 8/8\n",
            "--\n",
            "Training Step: 70  | total loss: \u001b[1m\u001b[32m0.64834\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 070 | loss: 0.64834 - acc: 0.9842 -- iter: 8/8\n",
            "--\n",
            "Training Step: 71  | total loss: \u001b[1m\u001b[32m0.64582\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 071 | loss: 0.64582 - acc: 0.9859 -- iter: 8/8\n",
            "--\n",
            "Training Step: 72  | total loss: \u001b[1m\u001b[32m0.64322\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 072 | loss: 0.64322 - acc: 0.9875 -- iter: 8/8\n",
            "--\n",
            "Training Step: 73  | total loss: \u001b[1m\u001b[32m0.64053\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 073 | loss: 0.64053 - acc: 0.9889 -- iter: 8/8\n",
            "--\n",
            "Training Step: 74  | total loss: \u001b[1m\u001b[32m0.64053\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 074 | loss: 0.64053 - acc: 0.9889 -- iter: 8/8\n",
            "--\n",
            "Training Step: 75  | total loss: \u001b[1m\u001b[32m0.64424\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 075 | loss: 0.64424 - acc: 0.9376 -- iter: 8/8\n",
            "--\n",
            "Training Step: 76  | total loss: \u001b[1m\u001b[32m0.64424\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 076 | loss: 0.64424 - acc: 0.9376 -- iter: 8/8\n",
            "--\n",
            "Training Step: 77  | total loss: \u001b[1m\u001b[32m0.64035\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 077 | loss: 0.64035 - acc: 0.9442 -- iter: 8/8\n",
            "--\n",
            "Training Step: 78  | total loss: \u001b[1m\u001b[32m0.63275\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 078 | loss: 0.63275 - acc: 0.9552 -- iter: 8/8\n",
            "--\n",
            "Training Step: 79  | total loss: \u001b[1m\u001b[32m0.63275\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 079 | loss: 0.63275 - acc: 0.9552 -- iter: 8/8\n",
            "--\n",
            "Training Step: 80  | total loss: \u001b[1m\u001b[32m0.62913\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 080 | loss: 0.62913 - acc: 0.9409 -- iter: 8/8\n",
            "--\n",
            "Training Step: 81  | total loss: \u001b[1m\u001b[32m0.62499\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 081 | loss: 0.62499 - acc: 0.9468 -- iter: 8/8\n",
            "--\n",
            "Training Step: 82  | total loss: \u001b[1m\u001b[32m0.62084\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 082 | loss: 0.62084 - acc: 0.9521 -- iter: 8/8\n",
            "--\n",
            "Training Step: 83  | total loss: \u001b[1m\u001b[32m0.61664\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 083 | loss: 0.61664 - acc: 0.9569 -- iter: 8/8\n",
            "--\n",
            "Training Step: 84  | total loss: \u001b[1m\u001b[32m0.61240\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 084 | loss: 0.61240 - acc: 0.9612 -- iter: 8/8\n",
            "--\n",
            "Training Step: 85  | total loss: \u001b[1m\u001b[32m0.60811\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 085 | loss: 0.60811 - acc: 0.9651 -- iter: 8/8\n",
            "--\n",
            "Training Step: 86  | total loss: \u001b[1m\u001b[32m0.60374\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 086 | loss: 0.60374 - acc: 0.9686 -- iter: 8/8\n",
            "--\n",
            "Training Step: 87  | total loss: \u001b[1m\u001b[32m0.59929\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 087 | loss: 0.59929 - acc: 0.9717 -- iter: 8/8\n",
            "--\n",
            "Training Step: 88  | total loss: \u001b[1m\u001b[32m0.59929\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 088 | loss: 0.59929 - acc: 0.9717 -- iter: 8/8\n",
            "--\n",
            "Training Step: 89  | total loss: \u001b[1m\u001b[32m0.59477\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 089 | loss: 0.59477 - acc: 0.9745 -- iter: 8/8\n",
            "--\n",
            "Training Step: 90  | total loss: \u001b[1m\u001b[32m0.58544\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 090 | loss: 0.58544 - acc: 0.9794 -- iter: 8/8\n",
            "--\n",
            "Training Step: 91  | total loss: \u001b[1m\u001b[32m0.58063\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 091 | loss: 0.58063 - acc: 0.9814 -- iter: 8/8\n",
            "--\n",
            "Training Step: 92  | total loss: \u001b[1m\u001b[32m0.58063\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 092 | loss: 0.58063 - acc: 0.9814 -- iter: 8/8\n",
            "--\n",
            "Training Step: 93  | total loss: \u001b[1m\u001b[32m0.57072\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 093 | loss: 0.57072 - acc: 0.9850 -- iter: 8/8\n",
            "--\n",
            "Training Step: 94  | total loss: \u001b[1m\u001b[32m0.57072\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 094 | loss: 0.57072 - acc: 0.9850 -- iter: 8/8\n",
            "--\n",
            "Training Step: 95  | total loss: \u001b[1m\u001b[32m0.56038\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 095 | loss: 0.56038 - acc: 0.9878 -- iter: 8/8\n",
            "--\n",
            "Training Step: 96  | total loss: \u001b[1m\u001b[32m0.55506\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 096 | loss: 0.55506 - acc: 0.9890 -- iter: 8/8\n",
            "--\n",
            "Training Step: 97  | total loss: \u001b[1m\u001b[32m0.54963\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 097 | loss: 0.54963 - acc: 0.9901 -- iter: 8/8\n",
            "--\n",
            "Training Step: 98  | total loss: \u001b[1m\u001b[32m0.54409\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 098 | loss: 0.54409 - acc: 0.9911 -- iter: 8/8\n",
            "--\n",
            "Training Step: 99  | total loss: \u001b[1m\u001b[32m0.53845\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 099 | loss: 0.53845 - acc: 0.9911 -- iter: 8/8\n",
            "--\n",
            "Training Step: 100  | total loss: \u001b[1m\u001b[32m0.53845\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 100 | loss: 0.53845 - acc: 0.9920 -- iter: 8/8\n",
            "--\n",
            "Training Step: 101  | total loss: \u001b[1m\u001b[32m0.52687\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 101 | loss: 0.52687 - acc: 0.9928 -- iter: 8/8\n",
            "--\n",
            "Training Step: 102  | total loss: \u001b[1m\u001b[32m0.52687\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 102 | loss: 0.52687 - acc: 0.9935 -- iter: 8/8\n",
            "--\n",
            "Training Step: 103  | total loss: \u001b[1m\u001b[32m0.52093\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 103 | loss: 0.52093 - acc: 0.9942 -- iter: 8/8\n",
            "--\n",
            "Training Step: 104  | total loss: \u001b[1m\u001b[32m0.51489\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 104 | loss: 0.51489 - acc: 0.9948 -- iter: 8/8\n",
            "--\n",
            "Training Step: 105  | total loss: \u001b[1m\u001b[32m0.50876\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 105 | loss: 0.50876 - acc: 0.9953 -- iter: 8/8\n",
            "--\n",
            "Training Step: 106  | total loss: \u001b[1m\u001b[32m0.50254\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 106 | loss: 0.50254 - acc: 0.9958 -- iter: 8/8\n",
            "--\n",
            "Training Step: 107  | total loss: \u001b[1m\u001b[32m0.48985\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 107 | loss: 0.48985 - acc: 0.9962 -- iter: 8/8\n",
            "--\n",
            "Training Step: 108  | total loss: \u001b[1m\u001b[32m0.48985\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 108 | loss: 0.48985 - acc: 0.9966 -- iter: 8/8\n",
            "--\n",
            "Training Step: 109  | total loss: \u001b[1m\u001b[32m0.48339\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 109 | loss: 0.48339 - acc: 0.9969 -- iter: 8/8\n",
            "--\n",
            "Training Step: 110  | total loss: \u001b[1m\u001b[32m0.47685\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 110 | loss: 0.47685 - acc: 0.9972 -- iter: 8/8\n",
            "--\n",
            "Training Step: 111  | total loss: \u001b[1m\u001b[32m0.47025\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 111 | loss: 0.47025 - acc: 0.9975 -- iter: 8/8\n",
            "--\n",
            "Training Step: 112  | total loss: \u001b[1m\u001b[32m0.46357\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 112 | loss: 0.46357 - acc: 0.9977 -- iter: 8/8\n",
            "--\n",
            "Training Step: 113  | total loss: \u001b[1m\u001b[32m0.45684\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 113 | loss: 0.45684 - acc: 0.9980 -- iter: 8/8\n",
            "--\n",
            "Training Step: 114  | total loss: \u001b[1m\u001b[32m0.45006\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 114 | loss: 0.45006 - acc: 0.9982 -- iter: 8/8\n",
            "--\n",
            "Training Step: 115  | total loss: \u001b[1m\u001b[32m0.44322\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 115 | loss: 0.44322 - acc: 0.9984 -- iter: 8/8\n",
            "--\n",
            "Training Step: 116  | total loss: \u001b[1m\u001b[32m0.43635\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 116 | loss: 0.43635 - acc: 0.9985 -- iter: 8/8\n",
            "--\n",
            "Training Step: 117  | total loss: \u001b[1m\u001b[32m0.42943\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 117 | loss: 0.42943 - acc: 0.9987 -- iter: 8/8\n",
            "--\n",
            "Training Step: 118  | total loss: \u001b[1m\u001b[32m0.42248\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 118 | loss: 0.42248 - acc: 0.9988 -- iter: 8/8\n",
            "--\n",
            "Training Step: 119  | total loss: \u001b[1m\u001b[32m0.41551\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 119 | loss: 0.41551 - acc: 0.9989 -- iter: 8/8\n",
            "--\n",
            "Training Step: 120  | total loss: \u001b[1m\u001b[32m0.40851\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 120 | loss: 0.40851 - acc: 0.9990 -- iter: 8/8\n",
            "--\n",
            "Training Step: 121  | total loss: \u001b[1m\u001b[32m0.40150\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 121 | loss: 0.40150 - acc: 0.9991 -- iter: 8/8\n",
            "--\n",
            "Training Step: 122  | total loss: \u001b[1m\u001b[32m0.39448\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 122 | loss: 0.39448 - acc: 0.9992 -- iter: 8/8\n",
            "--\n",
            "Training Step: 123  | total loss: \u001b[1m\u001b[32m0.38745\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 123 | loss: 0.38745 - acc: 0.9993 -- iter: 8/8\n",
            "--\n",
            "Training Step: 124  | total loss: \u001b[1m\u001b[32m0.38043\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 124 | loss: 0.38043 - acc: 0.9994 -- iter: 8/8\n",
            "--\n",
            "Training Step: 125  | total loss: \u001b[1m\u001b[32m0.37341\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 125 | loss: 0.37341 - acc: 0.9994 -- iter: 8/8\n",
            "--\n",
            "Training Step: 126  | total loss: \u001b[1m\u001b[32m0.36641\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 126 | loss: 0.36641 - acc: 0.9995 -- iter: 8/8\n",
            "--\n",
            "Training Step: 127  | total loss: \u001b[1m\u001b[32m0.35943\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 127 | loss: 0.35943 - acc: 0.9995 -- iter: 8/8\n",
            "--\n",
            "Training Step: 128  | total loss: \u001b[1m\u001b[32m0.35247\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 128 | loss: 0.35247 - acc: 0.9996 -- iter: 8/8\n",
            "--\n",
            "Training Step: 129  | total loss: \u001b[1m\u001b[32m0.34554\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 129 | loss: 0.34554 - acc: 0.9996 -- iter: 8/8\n",
            "--\n",
            "Training Step: 130  | total loss: \u001b[1m\u001b[32m0.33864\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 130 | loss: 0.33864 - acc: 0.9997 -- iter: 8/8\n",
            "--\n",
            "Training Step: 131  | total loss: \u001b[1m\u001b[32m0.33179\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 131 | loss: 0.33179 - acc: 0.9997 -- iter: 8/8\n",
            "--\n",
            "Training Step: 132  | total loss: \u001b[1m\u001b[32m0.32498\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 132 | loss: 0.32498 - acc: 0.9997 -- iter: 8/8\n",
            "--\n",
            "Training Step: 133  | total loss: \u001b[1m\u001b[32m0.31822\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 133 | loss: 0.31822 - acc: 0.9998 -- iter: 8/8\n",
            "--\n",
            "Training Step: 134  | total loss: \u001b[1m\u001b[32m0.31151\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 134 | loss: 0.31151 - acc: 0.9998 -- iter: 8/8\n",
            "--\n",
            "Training Step: 135  | total loss: \u001b[1m\u001b[32m0.30486\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 135 | loss: 0.30486 - acc: 0.9998 -- iter: 8/8\n",
            "--\n",
            "Training Step: 136  | total loss: \u001b[1m\u001b[32m0.29828\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 136 | loss: 0.29828 - acc: 0.9998 -- iter: 8/8\n",
            "--\n",
            "Training Step: 137  | total loss: \u001b[1m\u001b[32m0.29176\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 137 | loss: 0.29176 - acc: 0.9998 -- iter: 8/8\n",
            "--\n",
            "Training Step: 138  | total loss: \u001b[1m\u001b[32m0.28532\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 138 | loss: 0.28532 - acc: 0.9999 -- iter: 8/8\n",
            "--\n",
            "Training Step: 139  | total loss: \u001b[1m\u001b[32m0.27895\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 139 | loss: 0.27895 - acc: 0.9999 -- iter: 8/8\n",
            "--\n",
            "Training Step: 140  | total loss: \u001b[1m\u001b[32m0.27265\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 140 | loss: 0.27265 - acc: 0.9999 -- iter: 8/8\n",
            "--\n",
            "Training Step: 141  | total loss: \u001b[1m\u001b[32m0.26644\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 141 | loss: 0.26644 - acc: 0.9999 -- iter: 8/8\n",
            "--\n",
            "Training Step: 142  | total loss: \u001b[1m\u001b[32m0.26031\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 142 | loss: 0.26031 - acc: 0.9999 -- iter: 8/8\n",
            "--\n",
            "Training Step: 143  | total loss: \u001b[1m\u001b[32m0.25427\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 143 | loss: 0.25427 - acc: 0.9999 -- iter: 8/8\n",
            "--\n",
            "Training Step: 144  | total loss: \u001b[1m\u001b[32m0.24832\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 144 | loss: 0.24832 - acc: 0.9999 -- iter: 8/8\n",
            "--\n",
            "Training Step: 145  | total loss: \u001b[1m\u001b[32m0.24246\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 145 | loss: 0.24246 - acc: 0.9999 -- iter: 8/8\n",
            "--\n",
            "Training Step: 146  | total loss: \u001b[1m\u001b[32m0.23670\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 146 | loss: 0.23670 - acc: 0.9999 -- iter: 8/8\n",
            "--\n",
            "Training Step: 147  | total loss: \u001b[1m\u001b[32m0.23103\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 147 | loss: 0.23103 - acc: 0.9999 -- iter: 8/8\n",
            "--\n",
            "Training Step: 148  | total loss: \u001b[1m\u001b[32m0.22545\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 148 | loss: 0.22545 - acc: 0.9999 -- iter: 8/8\n",
            "--\n",
            "Training Step: 149  | total loss: \u001b[1m\u001b[32m0.21998\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 149 | loss: 0.21998 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 150  | total loss: \u001b[1m\u001b[32m0.21461\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 150 | loss: 0.21461 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 151  | total loss: \u001b[1m\u001b[32m0.20934\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 151 | loss: 0.20934 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 152  | total loss: \u001b[1m\u001b[32m0.29796\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 152 | loss: 0.29796 - acc: 0.9500 -- iter: 8/8\n",
            "--\n",
            "Training Step: 153  | total loss: \u001b[1m\u001b[32m0.28366\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 153 | loss: 0.28366 - acc: 0.8845 -- iter: 8/8\n",
            "--\n",
            "Training Step: 154  | total loss: \u001b[1m\u001b[32m0.42185\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 154 | loss: 0.42185 - acc: 0.8845 -- iter: 8/8\n",
            "--\n",
            "Training Step: 155  | total loss: \u001b[1m\u001b[32m0.39482\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 155 | loss: 0.39482 - acc: 0.8960 -- iter: 8/8\n",
            "--\n",
            "Training Step: 156  | total loss: \u001b[1m\u001b[32m0.52373\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 156 | loss: 0.52373 - acc: 0.8314 -- iter: 8/8\n",
            "--\n",
            "Training Step: 157  | total loss: \u001b[1m\u001b[32m0.48648\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 157 | loss: 0.48648 - acc: 0.8483 -- iter: 8/8\n",
            "--\n",
            "Training Step: 158  | total loss: \u001b[1m\u001b[32m0.45300\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 158 | loss: 0.45300 - acc: 0.8635 -- iter: 8/8\n",
            "--\n",
            "Training Step: 159  | total loss: \u001b[1m\u001b[32m0.42286\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 159 | loss: 0.42286 - acc: 0.8771 -- iter: 8/8\n",
            "--\n",
            "Training Step: 160  | total loss: \u001b[1m\u001b[32m0.39570\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 160 | loss: 0.39570 - acc: 0.8894 -- iter: 8/8\n",
            "--\n",
            "Training Step: 161  | total loss: \u001b[1m\u001b[32m0.37121\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 161 | loss: 0.37121 - acc: 0.9005 -- iter: 8/8\n",
            "--\n",
            "Training Step: 162  | total loss: \u001b[1m\u001b[32m0.45998\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 162 | loss: 0.45998 - acc: 0.8604 -- iter: 8/8\n",
            "--\n",
            "Training Step: 163  | total loss: \u001b[1m\u001b[32m0.42900\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 163 | loss: 0.42900 - acc: 0.8744 -- iter: 8/8\n",
            "--\n",
            "Training Step: 164  | total loss: \u001b[1m\u001b[32m0.40110\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 164 | loss: 0.40110 - acc: 0.8869 -- iter: 8/8\n",
            "--\n",
            "Training Step: 165  | total loss: \u001b[1m\u001b[32m0.37595\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 165 | loss: 0.37595 - acc: 0.8982 -- iter: 8/8\n",
            "--\n",
            "Training Step: 166  | total loss: \u001b[1m\u001b[32m0.35325\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 166 | loss: 0.35325 - acc: 0.9084 -- iter: 8/8\n",
            "--\n",
            "Training Step: 167  | total loss: \u001b[1m\u001b[32m0.33274\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 167 | loss: 0.33274 - acc: 0.9176 -- iter: 8/8\n",
            "--\n",
            "Training Step: 168  | total loss: \u001b[1m\u001b[32m0.31417\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 168 | loss: 0.31417 - acc: 0.9258 -- iter: 8/8\n",
            "--\n",
            "Training Step: 169  | total loss: \u001b[1m\u001b[32m0.29734\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 169 | loss: 0.29734 - acc: 0.9332 -- iter: 8/8\n",
            "--\n",
            "Training Step: 170  | total loss: \u001b[1m\u001b[32m0.28207\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 170 | loss: 0.28207 - acc: 0.9399 -- iter: 8/8\n",
            "--\n",
            "Training Step: 171  | total loss: \u001b[1m\u001b[32m0.26817\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 171 | loss: 0.26817 - acc: 0.9459 -- iter: 8/8\n",
            "--\n",
            "Training Step: 172  | total loss: \u001b[1m\u001b[32m0.25550\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 172 | loss: 0.25550 - acc: 0.9513 -- iter: 8/8\n",
            "--\n",
            "Training Step: 173  | total loss: \u001b[1m\u001b[32m0.24393\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 173 | loss: 0.24393 - acc: 0.9562 -- iter: 8/8\n",
            "--\n",
            "Training Step: 174  | total loss: \u001b[1m\u001b[32m0.23335\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 174 | loss: 0.23335 - acc: 0.9606 -- iter: 8/8\n",
            "--\n",
            "Training Step: 175  | total loss: \u001b[1m\u001b[32m0.22364\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 175 | loss: 0.22364 - acc: 0.9645 -- iter: 8/8\n",
            "--\n",
            "Training Step: 176  | total loss: \u001b[1m\u001b[32m0.21471\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 176 | loss: 0.21471 - acc: 0.9681 -- iter: 8/8\n",
            "--\n",
            "Training Step: 177  | total loss: \u001b[1m\u001b[32m0.20647\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 177 | loss: 0.20647 - acc: 0.9713 -- iter: 8/8\n",
            "--\n",
            "Training Step: 178  | total loss: \u001b[1m\u001b[32m0.36287\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 178 | loss: 0.36287 - acc: 0.8991 -- iter: 8/8\n",
            "--\n",
            "Training Step: 179  | total loss: \u001b[1m\u001b[32m0.33959\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 179 | loss: 0.33959 - acc: 0.9092 -- iter: 8/8\n",
            "--\n",
            "Training Step: 180  | total loss: \u001b[1m\u001b[32m0.36235\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 180 | loss: 0.36235 - acc: 0.8933 -- iter: 8/8\n",
            "--\n",
            "Training Step: 181  | total loss: \u001b[1m\u001b[32m0.33904\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 181 | loss: 0.33904 - acc: 0.9040 -- iter: 8/8\n",
            "--\n",
            "Training Step: 182  | total loss: \u001b[1m\u001b[32m0.31801\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 182 | loss: 0.31801 - acc: 0.9136 -- iter: 8/8\n",
            "--\n",
            "Training Step: 183  | total loss: \u001b[1m\u001b[32m0.29901\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 183 | loss: 0.29901 - acc: 0.9222 -- iter: 8/8\n",
            "--\n",
            "Training Step: 184  | total loss: \u001b[1m\u001b[32m0.28184\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 184 | loss: 0.28184 - acc: 0.9300 -- iter: 8/8\n",
            "--\n",
            "Training Step: 185  | total loss: \u001b[1m\u001b[32m0.26629\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 185 | loss: 0.26629 - acc: 0.9370 -- iter: 8/8\n",
            "--\n",
            "Training Step: 186  | total loss: \u001b[1m\u001b[32m0.25219\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 186 | loss: 0.25219 - acc: 0.9433 -- iter: 8/8\n",
            "--\n",
            "Training Step: 187  | total loss: \u001b[1m\u001b[32m0.23939\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 187 | loss: 0.23939 - acc: 0.9490 -- iter: 8/8\n",
            "--\n",
            "Training Step: 188  | total loss: \u001b[1m\u001b[32m0.22773\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 188 | loss: 0.22773 - acc: 0.9541 -- iter: 8/8\n",
            "--\n",
            "Training Step: 189  | total loss: \u001b[1m\u001b[32m0.21711\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 189 | loss: 0.21711 - acc: 0.9587 -- iter: 8/8\n",
            "--\n",
            "Training Step: 190  | total loss: \u001b[1m\u001b[32m0.20741\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 190 | loss: 0.20741 - acc: 0.9628 -- iter: 8/8\n",
            "--\n",
            "Training Step: 191  | total loss: \u001b[1m\u001b[32m0.19854\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 191 | loss: 0.19854 - acc: 0.9665 -- iter: 8/8\n",
            "--\n",
            "Training Step: 192  | total loss: \u001b[1m\u001b[32m0.19039\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 192 | loss: 0.19039 - acc: 0.9699 -- iter: 8/8\n",
            "--\n",
            "Training Step: 193  | total loss: \u001b[1m\u001b[32m0.18291\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 193 | loss: 0.18291 - acc: 0.9729 -- iter: 8/8\n",
            "--\n",
            "Training Step: 194  | total loss: \u001b[1m\u001b[32m0.23803\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 194 | loss: 0.23803 - acc: 0.9506 -- iter: 8/8\n",
            "--\n",
            "Training Step: 195  | total loss: \u001b[1m\u001b[32m0.22551\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 195 | loss: 0.22551 - acc: 0.9555 -- iter: 8/8\n",
            "--\n",
            "Training Step: 196  | total loss: \u001b[1m\u001b[32m0.26127\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 196 | loss: 0.26127 - acc: 0.9350 -- iter: 8/8\n",
            "--\n",
            "Training Step: 197  | total loss: \u001b[1m\u001b[32m0.24623\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 197 | loss: 0.24623 - acc: 0.9415 -- iter: 8/8\n",
            "--\n",
            "Training Step: 198  | total loss: \u001b[1m\u001b[32m0.34424\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 198 | loss: 0.34424 - acc: 0.8973 -- iter: 8/8\n",
            "--\n",
            "Training Step: 199  | total loss: \u001b[1m\u001b[32m0.32080\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 199 | loss: 0.32080 - acc: 0.9076 -- iter: 8/8\n",
            "--\n",
            "Training Step: 200  | total loss: \u001b[1m\u001b[32m0.37632\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 200 | loss: 0.37632 - acc: 0.8918 -- iter: 8/8\n",
            "--\n",
            "Training Step: 201  | total loss: \u001b[1m\u001b[32m0.34965\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 201 | loss: 0.34965 - acc: 0.9027 -- iter: 8/8\n",
            "--\n",
            "Training Step: 202  | total loss: \u001b[1m\u001b[32m0.32565\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 202 | loss: 0.32565 - acc: 0.9124 -- iter: 8/8\n",
            "--\n",
            "Training Step: 203  | total loss: \u001b[1m\u001b[32m0.30402\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 203 | loss: 0.30402 - acc: 0.9212 -- iter: 8/8\n",
            "--\n",
            "Training Step: 204  | total loss: \u001b[1m\u001b[32m0.45623\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 204 | loss: 0.45623 - acc: 0.8540 -- iter: 8/8\n",
            "--\n",
            "Training Step: 205  | total loss: \u001b[1m\u001b[32m0.42159\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 205 | loss: 0.42159 - acc: 0.8686 -- iter: 8/8\n",
            "--\n",
            "Training Step: 206  | total loss: \u001b[1m\u001b[32m0.39046\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 206 | loss: 0.39046 - acc: 0.8818 -- iter: 8/8\n",
            "--\n",
            "Training Step: 207  | total loss: \u001b[1m\u001b[32m0.36248\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 207 | loss: 0.36248 - acc: 0.8936 -- iter: 8/8\n",
            "--\n",
            "Training Step: 208  | total loss: \u001b[1m\u001b[32m0.43518\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 208 | loss: 0.43518 - acc: 0.8542 -- iter: 8/8\n",
            "--\n",
            "Training Step: 209  | total loss: \u001b[1m\u001b[32m0.40281\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 209 | loss: 0.40281 - acc: 0.8688 -- iter: 8/8\n",
            "--\n",
            "Training Step: 210  | total loss: \u001b[1m\u001b[32m0.37373\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 210 | loss: 0.37373 - acc: 0.8819 -- iter: 8/8\n",
            "--\n",
            "Training Step: 211  | total loss: \u001b[1m\u001b[32m0.34758\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 211 | loss: 0.34758 - acc: 0.8937 -- iter: 8/8\n",
            "--\n",
            "Training Step: 212  | total loss: \u001b[1m\u001b[32m0.49153\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 212 | loss: 0.49153 - acc: 0.8294 -- iter: 8/8\n",
            "--\n",
            "Training Step: 213  | total loss: \u001b[1m\u001b[32m0.45372\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 213 | loss: 0.45372 - acc: 0.8464 -- iter: 8/8\n",
            "--\n",
            "Training Step: 214  | total loss: \u001b[1m\u001b[32m0.41978\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 214 | loss: 0.41978 - acc: 0.8618 -- iter: 8/8\n",
            "--\n",
            "Training Step: 215  | total loss: \u001b[1m\u001b[32m0.38929\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 215 | loss: 0.38929 - acc: 0.8756 -- iter: 8/8\n",
            "--\n",
            "Training Step: 216  | total loss: \u001b[1m\u001b[32m0.45712\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 216 | loss: 0.45712 - acc: 0.8380 -- iter: 8/8\n",
            "--\n",
            "Training Step: 217  | total loss: \u001b[1m\u001b[32m0.42303\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 217 | loss: 0.42303 - acc: 0.8542 -- iter: 8/8\n",
            "--\n",
            "Training Step: 218  | total loss: \u001b[1m\u001b[32m0.54960\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 218 | loss: 0.54960 - acc: 0.7938 -- iter: 8/8\n",
            "--\n",
            "Training Step: 219  | total loss: \u001b[1m\u001b[32m0.50648\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 219 | loss: 0.50648 - acc: 0.8144 -- iter: 8/8\n",
            "--\n",
            "Training Step: 220  | total loss: \u001b[1m\u001b[32m0.56471\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 220 | loss: 0.56471 - acc: 0.7830 -- iter: 8/8\n",
            "--\n",
            "Training Step: 221  | total loss: \u001b[1m\u001b[32m0.52038\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 221 | loss: 0.52038 - acc: 0.8047 -- iter: 8/8\n",
            "--\n",
            "Training Step: 222  | total loss: \u001b[1m\u001b[32m0.58400\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 222 | loss: 0.58400 - acc: 0.7742 -- iter: 8/8\n",
            "--\n",
            "Training Step: 223  | total loss: \u001b[1m\u001b[32m0.53806\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 223 | loss: 0.53806 - acc: 0.7968 -- iter: 8/8\n",
            "--\n",
            "Training Step: 224  | total loss: \u001b[1m\u001b[32m0.55553\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 224 | loss: 0.55553 - acc: 0.7921 -- iter: 8/8\n",
            "--\n",
            "Training Step: 225  | total loss: \u001b[1m\u001b[32m0.51274\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 225 | loss: 0.51274 - acc: 0.8129 -- iter: 8/8\n",
            "--\n",
            "Training Step: 226  | total loss: \u001b[1m\u001b[32m0.61154\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 226 | loss: 0.61154 - acc: 0.7566 -- iter: 8/8\n",
            "--\n",
            "Training Step: 227  | total loss: \u001b[1m\u001b[32m0.56349\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 227 | loss: 0.56349 - acc: 0.7810 -- iter: 8/8\n",
            "--\n",
            "Training Step: 228  | total loss: \u001b[1m\u001b[32m0.52041\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 228 | loss: 0.52041 - acc: 0.8029 -- iter: 8/8\n",
            "--\n",
            "Training Step: 229  | total loss: \u001b[1m\u001b[32m0.48177\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 229 | loss: 0.48177 - acc: 0.8226 -- iter: 8/8\n",
            "--\n",
            "Training Step: 230  | total loss: \u001b[1m\u001b[32m0.44709\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 230 | loss: 0.44709 - acc: 0.8403 -- iter: 8/8\n",
            "--\n",
            "Training Step: 231  | total loss: \u001b[1m\u001b[32m0.41596\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 231 | loss: 0.41596 - acc: 0.8563 -- iter: 8/8\n",
            "--\n",
            "Training Step: 232  | total loss: \u001b[1m\u001b[32m0.38798\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 232 | loss: 0.38798 - acc: 0.8707 -- iter: 8/8\n",
            "--\n",
            "Training Step: 233  | total loss: \u001b[1m\u001b[32m0.36283\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 233 | loss: 0.36283 - acc: 0.8836 -- iter: 8/8\n",
            "--\n",
            "Training Step: 234  | total loss: \u001b[1m\u001b[32m0.34019\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 234 | loss: 0.34019 - acc: 0.8952 -- iter: 8/8\n",
            "--\n",
            "Training Step: 235  | total loss: \u001b[1m\u001b[32m0.31979\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 235 | loss: 0.31979 - acc: 0.9057 -- iter: 8/8\n",
            "--\n",
            "Training Step: 236  | total loss: \u001b[1m\u001b[32m0.30140\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 236 | loss: 0.30140 - acc: 0.9151 -- iter: 8/8\n",
            "--\n",
            "Training Step: 237  | total loss: \u001b[1m\u001b[32m0.28479\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 237 | loss: 0.28479 - acc: 0.9236 -- iter: 8/8\n",
            "--\n",
            "Training Step: 238  | total loss: \u001b[1m\u001b[32m0.26978\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 238 | loss: 0.26978 - acc: 0.9313 -- iter: 8/8\n",
            "--\n",
            "Training Step: 239  | total loss: \u001b[1m\u001b[32m0.25619\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 239 | loss: 0.25619 - acc: 0.9381 -- iter: 8/8\n",
            "--\n",
            "Training Step: 240  | total loss: \u001b[1m\u001b[32m0.24387\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 240 | loss: 0.24387 - acc: 0.9443 -- iter: 8/8\n",
            "--\n",
            "Training Step: 241  | total loss: \u001b[1m\u001b[32m0.23268\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 241 | loss: 0.23268 - acc: 0.9499 -- iter: 8/8\n",
            "--\n",
            "Training Step: 242  | total loss: \u001b[1m\u001b[32m0.22250\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 242 | loss: 0.22250 - acc: 0.9549 -- iter: 8/8\n",
            "--\n",
            "Training Step: 243  | total loss: \u001b[1m\u001b[32m0.21323\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 243 | loss: 0.21323 - acc: 0.9594 -- iter: 8/8\n",
            "--\n",
            "Training Step: 244  | total loss: \u001b[1m\u001b[32m0.20476\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 244 | loss: 0.20476 - acc: 0.9671 -- iter: 8/8\n",
            "--\n",
            "Training Step: 245  | total loss: \u001b[1m\u001b[32m0.19701\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 245 | loss: 0.19701 - acc: 0.9704 -- iter: 8/8\n",
            "--\n",
            "Training Step: 246  | total loss: \u001b[1m\u001b[32m0.18990\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 246 | loss: 0.18990 - acc: 0.9704 -- iter: 8/8\n",
            "--\n",
            "Training Step: 247  | total loss: \u001b[1m\u001b[32m0.18336\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 247 | loss: 0.18336 - acc: 0.9734 -- iter: 8/8\n",
            "--\n",
            "Training Step: 248  | total loss: \u001b[1m\u001b[32m0.17734\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 248 | loss: 0.17734 - acc: 0.9760 -- iter: 8/8\n",
            "--\n",
            "Training Step: 249  | total loss: \u001b[1m\u001b[32m0.17177\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 249 | loss: 0.17177 - acc: 0.9784 -- iter: 8/8\n",
            "--\n",
            "Training Step: 250  | total loss: \u001b[1m\u001b[32m0.27469\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 250 | loss: 0.27469 - acc: 0.9306 -- iter: 8/8\n",
            "--\n",
            "Training Step: 251  | total loss: \u001b[1m\u001b[32m0.25918\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 251 | loss: 0.25918 - acc: 0.9375 -- iter: 8/8\n",
            "--\n",
            "Training Step: 252  | total loss: \u001b[1m\u001b[32m0.24514\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 252 | loss: 0.24514 - acc: 0.9438 -- iter: 8/8\n",
            "--\n",
            "Training Step: 253  | total loss: \u001b[1m\u001b[32m0.23242\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 253 | loss: 0.23242 - acc: 0.9494 -- iter: 8/8\n",
            "--\n",
            "Training Step: 254  | total loss: \u001b[1m\u001b[32m0.22088\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 254 | loss: 0.22088 - acc: 0.9545 -- iter: 8/8\n",
            "--\n",
            "Training Step: 255  | total loss: \u001b[1m\u001b[32m0.21039\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 255 | loss: 0.21039 - acc: 0.9590 -- iter: 8/8\n",
            "--\n",
            "Training Step: 256  | total loss: \u001b[1m\u001b[32m0.20085\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 256 | loss: 0.20085 - acc: 0.9631 -- iter: 8/8\n",
            "--\n",
            "Training Step: 257  | total loss: \u001b[1m\u001b[32m0.19214\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 257 | loss: 0.19214 - acc: 0.9668 -- iter: 8/8\n",
            "--\n",
            "Training Step: 258  | total loss: \u001b[1m\u001b[32m0.18420\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 258 | loss: 0.18420 - acc: 0.9701 -- iter: 8/8\n",
            "--\n",
            "Training Step: 259  | total loss: \u001b[1m\u001b[32m0.17692\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 259 | loss: 0.17692 - acc: 0.9731 -- iter: 8/8\n",
            "--\n",
            "Training Step: 260  | total loss: \u001b[1m\u001b[32m0.17026\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 260 | loss: 0.17026 - acc: 0.9758 -- iter: 8/8\n",
            "--\n",
            "Training Step: 261  | total loss: \u001b[1m\u001b[32m0.16413\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 261 | loss: 0.16413 - acc: 0.9782 -- iter: 8/8\n",
            "--\n",
            "Training Step: 262  | total loss: \u001b[1m\u001b[32m0.21817\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 262 | loss: 0.21817 - acc: 0.9554 -- iter: 8/8\n",
            "--\n",
            "Training Step: 263  | total loss: \u001b[1m\u001b[32m0.20704\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 263 | loss: 0.20704 - acc: 0.9599 -- iter: 8/8\n",
            "--\n",
            "Training Step: 264  | total loss: \u001b[1m\u001b[32m0.19693\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 264 | loss: 0.19693 - acc: 0.9639 -- iter: 8/8\n",
            "--\n",
            "Training Step: 265  | total loss: \u001b[1m\u001b[32m0.18772\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 265 | loss: 0.18772 - acc: 0.9675 -- iter: 8/8\n",
            "--\n",
            "Training Step: 266  | total loss: \u001b[1m\u001b[32m0.17934\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 266 | loss: 0.17934 - acc: 0.9707 -- iter: 8/8\n",
            "--\n",
            "Training Step: 267  | total loss: \u001b[1m\u001b[32m0.17169\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 267 | loss: 0.17169 - acc: 0.9737 -- iter: 8/8\n",
            "--\n",
            "Training Step: 268  | total loss: \u001b[1m\u001b[32m0.16469\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 268 | loss: 0.16469 - acc: 0.9763 -- iter: 8/8\n",
            "--\n",
            "Training Step: 269  | total loss: \u001b[1m\u001b[32m0.15828\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 269 | loss: 0.15828 - acc: 0.9787 -- iter: 8/8\n",
            "--\n",
            "Training Step: 270  | total loss: \u001b[1m\u001b[32m0.15239\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 270 | loss: 0.15239 - acc: 0.9808 -- iter: 8/8\n",
            "--\n",
            "Training Step: 271  | total loss: \u001b[1m\u001b[32m0.14698\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 271 | loss: 0.14698 - acc: 0.9827 -- iter: 8/8\n",
            "--\n",
            "Training Step: 272  | total loss: \u001b[1m\u001b[32m0.14199\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 272 | loss: 0.14199 - acc: 0.9844 -- iter: 8/8\n",
            "--\n",
            "Training Step: 273  | total loss: \u001b[1m\u001b[32m0.13739\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 273 | loss: 0.13739 - acc: 0.9860 -- iter: 8/8\n",
            "--\n",
            "Training Step: 274  | total loss: \u001b[1m\u001b[32m0.13312\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 274 | loss: 0.13312 - acc: 0.9874 -- iter: 8/8\n",
            "--\n",
            "Training Step: 275  | total loss: \u001b[1m\u001b[32m0.12916\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 275 | loss: 0.12916 - acc: 0.9887 -- iter: 8/8\n",
            "--\n",
            "Training Step: 276  | total loss: \u001b[1m\u001b[32m0.12548\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 276 | loss: 0.12548 - acc: 0.9898 -- iter: 8/8\n",
            "--\n",
            "Training Step: 277  | total loss: \u001b[1m\u001b[32m0.12205\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 277 | loss: 0.12205 - acc: 0.9908 -- iter: 8/8\n",
            "--\n",
            "Training Step: 278  | total loss: \u001b[1m\u001b[32m0.11884\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 278 | loss: 0.11884 - acc: 0.9917 -- iter: 8/8\n",
            "--\n",
            "Training Step: 279  | total loss: \u001b[1m\u001b[32m0.11583\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 279 | loss: 0.11583 - acc: 0.9926 -- iter: 8/8\n",
            "--\n",
            "Training Step: 280  | total loss: \u001b[1m\u001b[32m0.11301\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 280 | loss: 0.11301 - acc: 0.9933 -- iter: 8/8\n",
            "--\n",
            "Training Step: 281  | total loss: \u001b[1m\u001b[32m0.11035\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 281 | loss: 0.11035 - acc: 0.9940 -- iter: 8/8\n",
            "--\n",
            "Training Step: 282  | total loss: \u001b[1m\u001b[32m0.10784\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 282 | loss: 0.10784 - acc: 0.9946 -- iter: 8/8\n",
            "--\n",
            "Training Step: 283  | total loss: \u001b[1m\u001b[32m0.10547\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 283 | loss: 0.10547 - acc: 0.9951 -- iter: 8/8\n",
            "--\n",
            "Training Step: 284  | total loss: \u001b[1m\u001b[32m0.10322\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 284 | loss: 0.10322 - acc: 0.9956 -- iter: 8/8\n",
            "--\n",
            "Training Step: 285  | total loss: \u001b[1m\u001b[32m0.10108\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 285 | loss: 0.10108 - acc: 0.9960 -- iter: 8/8\n",
            "--\n",
            "Training Step: 286  | total loss: \u001b[1m\u001b[32m0.09905\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 286 | loss: 0.09905 - acc: 0.9964 -- iter: 8/8\n",
            "--\n",
            "Training Step: 287  | total loss: \u001b[1m\u001b[32m0.09711\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 287 | loss: 0.09711 - acc: 0.9968 -- iter: 8/8\n",
            "--\n",
            "Training Step: 288  | total loss: \u001b[1m\u001b[32m0.09525\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 288 | loss: 0.09525 - acc: 0.9971 -- iter: 8/8\n",
            "--\n",
            "Training Step: 289  | total loss: \u001b[1m\u001b[32m0.09347\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 289 | loss: 0.09347 - acc: 0.9974 -- iter: 8/8\n",
            "--\n",
            "Training Step: 290  | total loss: \u001b[1m\u001b[32m0.09177\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 290 | loss: 0.09177 - acc: 0.9977 -- iter: 8/8\n",
            "--\n",
            "Training Step: 291  | total loss: \u001b[1m\u001b[32m0.09012\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 291 | loss: 0.09012 - acc: 0.9979 -- iter: 8/8\n",
            "--\n",
            "Training Step: 292  | total loss: \u001b[1m\u001b[32m0.08854\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 292 | loss: 0.08854 - acc: 0.9981 -- iter: 8/8\n",
            "--\n",
            "Training Step: 293  | total loss: \u001b[1m\u001b[32m0.08702\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 293 | loss: 0.08702 - acc: 0.9983 -- iter: 8/8\n",
            "--\n",
            "Training Step: 294  | total loss: \u001b[1m\u001b[32m0.08555\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 294 | loss: 0.08555 - acc: 0.9985 -- iter: 8/8\n",
            "--\n",
            "Training Step: 295  | total loss: \u001b[1m\u001b[32m0.08413\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 295 | loss: 0.08413 - acc: 0.9986 -- iter: 8/8\n",
            "--\n",
            "Training Step: 296  | total loss: \u001b[1m\u001b[32m0.08275\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 296 | loss: 0.08275 - acc: 0.9988 -- iter: 8/8\n",
            "--\n",
            "Training Step: 297  | total loss: \u001b[1m\u001b[32m0.08141\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 297 | loss: 0.08141 - acc: 0.9989 -- iter: 8/8\n",
            "--\n",
            "Training Step: 298  | total loss: \u001b[1m\u001b[32m0.08011\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 298 | loss: 0.08011 - acc: 0.9990 -- iter: 8/8\n",
            "--\n",
            "Training Step: 299  | total loss: \u001b[1m\u001b[32m0.07885\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 299 | loss: 0.07885 - acc: 0.9991 -- iter: 8/8\n",
            "--\n",
            "Training Step: 300  | total loss: \u001b[1m\u001b[32m0.07763\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 300 | loss: 0.07763 - acc: 0.9992 -- iter: 8/8\n",
            "--\n",
            "Training Step: 301  | total loss: \u001b[1m\u001b[32m0.07644\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 301 | loss: 0.07644 - acc: 0.9993 -- iter: 8/8\n",
            "--\n",
            "Training Step: 302  | total loss: \u001b[1m\u001b[32m0.07527\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 302 | loss: 0.07527 - acc: 0.9993 -- iter: 8/8\n",
            "--\n",
            "Training Step: 303  | total loss: \u001b[1m\u001b[32m0.07414\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 303 | loss: 0.07414 - acc: 0.9994 -- iter: 8/8\n",
            "--\n",
            "Training Step: 304  | total loss: \u001b[1m\u001b[32m0.14069\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 304 | loss: 0.14069 - acc: 0.9745 -- iter: 8/8\n",
            "--\n",
            "Training Step: 305  | total loss: \u001b[1m\u001b[32m0.13288\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 305 | loss: 0.13288 - acc: 0.9770 -- iter: 8/8\n",
            "--\n",
            "Training Step: 306  | total loss: \u001b[1m\u001b[32m0.12580\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 306 | loss: 0.12580 - acc: 0.9793 -- iter: 8/8\n",
            "--\n",
            "Training Step: 307  | total loss: \u001b[1m\u001b[32m0.11938\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 307 | loss: 0.11938 - acc: 0.9814 -- iter: 8/8\n",
            "--\n",
            "Training Step: 308  | total loss: \u001b[1m\u001b[32m0.11355\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 308 | loss: 0.11355 - acc: 0.9832 -- iter: 8/8\n",
            "--\n",
            "Training Step: 309  | total loss: \u001b[1m\u001b[32m0.10825\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 309 | loss: 0.10825 - acc: 0.9849 -- iter: 8/8\n",
            "--\n",
            "Training Step: 310  | total loss: \u001b[1m\u001b[32m0.10343\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 310 | loss: 0.10343 - acc: 0.9864 -- iter: 8/8\n",
            "--\n",
            "Training Step: 311  | total loss: \u001b[1m\u001b[32m0.09904\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 311 | loss: 0.09904 - acc: 0.9878 -- iter: 8/8\n",
            "--\n",
            "Training Step: 312  | total loss: \u001b[1m\u001b[32m0.09503\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 312 | loss: 0.09503 - acc: 0.9890 -- iter: 8/8\n",
            "--\n",
            "Training Step: 313  | total loss: \u001b[1m\u001b[32m0.09136\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 313 | loss: 0.09136 - acc: 0.9901 -- iter: 8/8\n",
            "--\n",
            "Training Step: 314  | total loss: \u001b[1m\u001b[32m0.08800\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 314 | loss: 0.08800 - acc: 0.9911 -- iter: 8/8\n",
            "--\n",
            "Training Step: 315  | total loss: \u001b[1m\u001b[32m0.08492\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 315 | loss: 0.08492 - acc: 0.9920 -- iter: 8/8\n",
            "--\n",
            "Training Step: 316  | total loss: \u001b[1m\u001b[32m0.08209\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 316 | loss: 0.08209 - acc: 0.9928 -- iter: 8/8\n",
            "--\n",
            "Training Step: 317  | total loss: \u001b[1m\u001b[32m0.07947\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 317 | loss: 0.07947 - acc: 0.9935 -- iter: 8/8\n",
            "--\n",
            "Training Step: 318  | total loss: \u001b[1m\u001b[32m0.07706\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 318 | loss: 0.07706 - acc: 0.9942 -- iter: 8/8\n",
            "--\n",
            "Training Step: 319  | total loss: \u001b[1m\u001b[32m0.07482\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 319 | loss: 0.07482 - acc: 0.9947 -- iter: 8/8\n",
            "--\n",
            "Training Step: 320  | total loss: \u001b[1m\u001b[32m0.07275\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 320 | loss: 0.07275 - acc: 0.9953 -- iter: 8/8\n",
            "--\n",
            "Training Step: 321  | total loss: \u001b[1m\u001b[32m0.07081\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 321 | loss: 0.07081 - acc: 0.9957 -- iter: 8/8\n",
            "--\n",
            "Training Step: 322  | total loss: \u001b[1m\u001b[32m0.06901\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 322 | loss: 0.06901 - acc: 0.9962 -- iter: 8/8\n",
            "--\n",
            "Training Step: 323  | total loss: \u001b[1m\u001b[32m0.06732\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 323 | loss: 0.06732 - acc: 0.9966 -- iter: 8/8\n",
            "--\n",
            "Training Step: 324  | total loss: \u001b[1m\u001b[32m0.06574\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 324 | loss: 0.06574 - acc: 0.9969 -- iter: 8/8\n",
            "--\n",
            "Training Step: 325  | total loss: \u001b[1m\u001b[32m0.06425\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 325 | loss: 0.06425 - acc: 0.9972 -- iter: 8/8\n",
            "--\n",
            "Training Step: 326  | total loss: \u001b[1m\u001b[32m0.06284\u001b[0m\u001b[0m | time: 0.021s\n",
            "| Adam | epoch: 326 | loss: 0.06284 - acc: 0.9975 -- iter: 8/8\n",
            "--\n",
            "Training Step: 327  | total loss: \u001b[1m\u001b[32m0.06152\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 327 | loss: 0.06152 - acc: 0.9977 -- iter: 8/8\n",
            "--\n",
            "Training Step: 328  | total loss: \u001b[1m\u001b[32m0.06026\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 328 | loss: 0.06026 - acc: 0.9980 -- iter: 8/8\n",
            "--\n",
            "Training Step: 329  | total loss: \u001b[1m\u001b[32m0.05907\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 329 | loss: 0.05907 - acc: 0.9982 -- iter: 8/8\n",
            "--\n",
            "Training Step: 330  | total loss: \u001b[1m\u001b[32m0.05794\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 330 | loss: 0.05794 - acc: 0.9984 -- iter: 8/8\n",
            "--\n",
            "Training Step: 331  | total loss: \u001b[1m\u001b[32m0.05686\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 331 | loss: 0.05686 - acc: 0.9985 -- iter: 8/8\n",
            "--\n",
            "Training Step: 332  | total loss: \u001b[1m\u001b[32m0.05584\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 332 | loss: 0.05584 - acc: 0.9987 -- iter: 8/8\n",
            "--\n",
            "Training Step: 333  | total loss: \u001b[1m\u001b[32m0.05485\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 333 | loss: 0.05485 - acc: 0.9988 -- iter: 8/8\n",
            "--\n",
            "Training Step: 334  | total loss: \u001b[1m\u001b[32m0.05391\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 334 | loss: 0.05391 - acc: 0.9989 -- iter: 8/8\n",
            "--\n",
            "Training Step: 335  | total loss: \u001b[1m\u001b[32m0.05301\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 335 | loss: 0.05301 - acc: 0.9990 -- iter: 8/8\n",
            "--\n",
            "Training Step: 336  | total loss: \u001b[1m\u001b[32m0.05214\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 336 | loss: 0.05214 - acc: 0.9991 -- iter: 8/8\n",
            "--\n",
            "Training Step: 337  | total loss: \u001b[1m\u001b[32m0.05131\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 337 | loss: 0.05131 - acc: 0.9992 -- iter: 8/8\n",
            "--\n",
            "Training Step: 338  | total loss: \u001b[1m\u001b[32m0.05051\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 338 | loss: 0.05051 - acc: 0.9993 -- iter: 8/8\n",
            "--\n",
            "Training Step: 339  | total loss: \u001b[1m\u001b[32m0.04973\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 339 | loss: 0.04973 - acc: 0.9994 -- iter: 8/8\n",
            "--\n",
            "Training Step: 340  | total loss: \u001b[1m\u001b[32m0.04899\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 340 | loss: 0.04899 - acc: 0.9994 -- iter: 8/8\n",
            "--\n",
            "Training Step: 341  | total loss: \u001b[1m\u001b[32m0.04826\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 341 | loss: 0.04826 - acc: 0.9995 -- iter: 8/8\n",
            "--\n",
            "Training Step: 342  | total loss: \u001b[1m\u001b[32m0.04757\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 342 | loss: 0.04757 - acc: 0.9995 -- iter: 8/8\n",
            "--\n",
            "Training Step: 343  | total loss: \u001b[1m\u001b[32m0.04689\u001b[0m\u001b[0m | time: 0.002s\n",
            "| Adam | epoch: 343 | loss: 0.04689 - acc: 0.9996 -- iter: 8/8\n",
            "--\n",
            "Training Step: 344  | total loss: \u001b[1m\u001b[32m0.04623\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 344 | loss: 0.04623 - acc: 0.9996 -- iter: 8/8\n",
            "--\n",
            "Training Step: 345  | total loss: \u001b[1m\u001b[32m0.04560\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 345 | loss: 0.04560 - acc: 0.9997 -- iter: 8/8\n",
            "--\n",
            "Training Step: 346  | total loss: \u001b[1m\u001b[32m0.04498\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 346 | loss: 0.04498 - acc: 0.9997 -- iter: 8/8\n",
            "--\n",
            "Training Step: 347  | total loss: \u001b[1m\u001b[32m0.04438\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 347 | loss: 0.04438 - acc: 0.9997 -- iter: 8/8\n",
            "--\n",
            "Training Step: 348  | total loss: \u001b[1m\u001b[32m0.04379\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 348 | loss: 0.04379 - acc: 0.9998 -- iter: 8/8\n",
            "--\n",
            "Training Step: 349  | total loss: \u001b[1m\u001b[32m0.04322\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 349 | loss: 0.04322 - acc: 0.9998 -- iter: 8/8\n",
            "--\n",
            "Training Step: 350  | total loss: \u001b[1m\u001b[32m0.04267\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 350 | loss: 0.04267 - acc: 0.9998 -- iter: 8/8\n",
            "--\n",
            "Training Step: 351  | total loss: \u001b[1m\u001b[32m0.04213\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 351 | loss: 0.04213 - acc: 0.9998 -- iter: 8/8\n",
            "--\n",
            "Training Step: 352  | total loss: \u001b[1m\u001b[32m0.04160\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 352 | loss: 0.04160 - acc: 0.9998 -- iter: 8/8\n",
            "--\n",
            "Training Step: 353  | total loss: \u001b[1m\u001b[32m0.04108\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 353 | loss: 0.04108 - acc: 0.9999 -- iter: 8/8\n",
            "--\n",
            "Training Step: 354  | total loss: \u001b[1m\u001b[32m0.04058\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 354 | loss: 0.04058 - acc: 0.9999 -- iter: 8/8\n",
            "--\n",
            "Training Step: 355  | total loss: \u001b[1m\u001b[32m0.04008\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 355 | loss: 0.04008 - acc: 0.9999 -- iter: 8/8\n",
            "--\n",
            "Training Step: 356  | total loss: \u001b[1m\u001b[32m0.03960\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 356 | loss: 0.03960 - acc: 0.9999 -- iter: 8/8\n",
            "--\n",
            "Training Step: 357  | total loss: \u001b[1m\u001b[32m0.03913\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 357 | loss: 0.03913 - acc: 0.9999 -- iter: 8/8\n",
            "--\n",
            "Training Step: 358  | total loss: \u001b[1m\u001b[32m0.03867\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 358 | loss: 0.03867 - acc: 0.9999 -- iter: 8/8\n",
            "--\n",
            "Training Step: 359  | total loss: \u001b[1m\u001b[32m0.03822\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 359 | loss: 0.03822 - acc: 0.9999 -- iter: 8/8\n",
            "--\n",
            "Training Step: 360  | total loss: \u001b[1m\u001b[32m0.03777\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 360 | loss: 0.03777 - acc: 0.9999 -- iter: 8/8\n",
            "--\n",
            "Training Step: 361  | total loss: \u001b[1m\u001b[32m0.03734\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 361 | loss: 0.03734 - acc: 0.9999 -- iter: 8/8\n",
            "--\n",
            "Training Step: 362  | total loss: \u001b[1m\u001b[32m0.03691\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 362 | loss: 0.03691 - acc: 0.9999 -- iter: 8/8\n",
            "--\n",
            "Training Step: 363  | total loss: \u001b[1m\u001b[32m0.03649\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 363 | loss: 0.03649 - acc: 0.9999 -- iter: 8/8\n",
            "--\n",
            "Training Step: 364  | total loss: \u001b[1m\u001b[32m0.03608\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 364 | loss: 0.03608 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 365  | total loss: \u001b[1m\u001b[32m0.03568\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 365 | loss: 0.03568 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 366  | total loss: \u001b[1m\u001b[32m0.03528\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 366 | loss: 0.03528 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 367  | total loss: \u001b[1m\u001b[32m0.03489\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 367 | loss: 0.03489 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 368  | total loss: \u001b[1m\u001b[32m0.03451\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 368 | loss: 0.03451 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 369  | total loss: \u001b[1m\u001b[32m0.03414\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 369 | loss: 0.03414 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 370  | total loss: \u001b[1m\u001b[32m0.03377\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 370 | loss: 0.03377 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 371  | total loss: \u001b[1m\u001b[32m0.03340\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 371 | loss: 0.03340 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 372  | total loss: \u001b[1m\u001b[32m0.03305\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 372 | loss: 0.03305 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 373  | total loss: \u001b[1m\u001b[32m0.03270\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 373 | loss: 0.03270 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 374  | total loss: \u001b[1m\u001b[32m0.03235\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 374 | loss: 0.03235 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 375  | total loss: \u001b[1m\u001b[32m0.03201\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 375 | loss: 0.03201 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 376  | total loss: \u001b[1m\u001b[32m0.03168\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 376 | loss: 0.03168 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 377  | total loss: \u001b[1m\u001b[32m0.03135\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 377 | loss: 0.03135 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 378  | total loss: \u001b[1m\u001b[32m0.22791\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 378 | loss: 0.22791 - acc: 0.9500 -- iter: 8/8\n",
            "--\n",
            "Training Step: 379  | total loss: \u001b[1m\u001b[32m0.20795\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 379 | loss: 0.20795 - acc: 0.9550 -- iter: 8/8\n",
            "--\n",
            "Training Step: 380  | total loss: \u001b[1m\u001b[32m0.19000\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 380 | loss: 0.19000 - acc: 0.9595 -- iter: 8/8\n",
            "--\n",
            "Training Step: 381  | total loss: \u001b[1m\u001b[32m0.17386\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 381 | loss: 0.17386 - acc: 0.9635 -- iter: 8/8\n",
            "--\n",
            "Training Step: 382  | total loss: \u001b[1m\u001b[32m0.15934\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 382 | loss: 0.15934 - acc: 0.9672 -- iter: 8/8\n",
            "--\n",
            "Training Step: 383  | total loss: \u001b[1m\u001b[32m0.14628\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 383 | loss: 0.14628 - acc: 0.9705 -- iter: 8/8\n",
            "--\n",
            "Training Step: 384  | total loss: \u001b[1m\u001b[32m0.13452\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 384 | loss: 0.13452 - acc: 0.9734 -- iter: 8/8\n",
            "--\n",
            "Training Step: 385  | total loss: \u001b[1m\u001b[32m0.12394\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 385 | loss: 0.12394 - acc: 0.9761 -- iter: 8/8\n",
            "--\n",
            "Training Step: 386  | total loss: \u001b[1m\u001b[32m0.11441\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 386 | loss: 0.11441 - acc: 0.9785 -- iter: 8/8\n",
            "--\n",
            "Training Step: 387  | total loss: \u001b[1m\u001b[32m0.10583\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 387 | loss: 0.10583 - acc: 0.9806 -- iter: 8/8\n",
            "--\n",
            "Training Step: 388  | total loss: \u001b[1m\u001b[32m0.09810\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 388 | loss: 0.09810 - acc: 0.9826 -- iter: 8/8\n",
            "--\n",
            "Training Step: 389  | total loss: \u001b[1m\u001b[32m0.09114\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 389 | loss: 0.09114 - acc: 0.9843 -- iter: 8/8\n",
            "--\n",
            "Training Step: 390  | total loss: \u001b[1m\u001b[32m0.08486\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 390 | loss: 0.08486 - acc: 0.9859 -- iter: 8/8\n",
            "--\n",
            "Training Step: 391  | total loss: \u001b[1m\u001b[32m0.07920\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 391 | loss: 0.07920 - acc: 0.9873 -- iter: 8/8\n",
            "--\n",
            "Training Step: 392  | total loss: \u001b[1m\u001b[32m0.07408\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 392 | loss: 0.07408 - acc: 0.9886 -- iter: 8/8\n",
            "--\n",
            "Training Step: 393  | total loss: \u001b[1m\u001b[32m0.06947\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 393 | loss: 0.06947 - acc: 0.9897 -- iter: 8/8\n",
            "--\n",
            "Training Step: 394  | total loss: \u001b[1m\u001b[32m0.06530\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 394 | loss: 0.06530 - acc: 0.9907 -- iter: 8/8\n",
            "--\n",
            "Training Step: 395  | total loss: \u001b[1m\u001b[32m0.06153\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 395 | loss: 0.06153 - acc: 0.9917 -- iter: 8/8\n",
            "--\n",
            "Training Step: 396  | total loss: \u001b[1m\u001b[32m0.05812\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 396 | loss: 0.05812 - acc: 0.9925 -- iter: 8/8\n",
            "--\n",
            "Training Step: 397  | total loss: \u001b[1m\u001b[32m0.05504\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 397 | loss: 0.05504 - acc: 0.9932 -- iter: 8/8\n",
            "--\n",
            "Training Step: 398  | total loss: \u001b[1m\u001b[32m0.05224\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 398 | loss: 0.05224 - acc: 0.9939 -- iter: 8/8\n",
            "--\n",
            "Training Step: 399  | total loss: \u001b[1m\u001b[32m0.04971\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 399 | loss: 0.04971 - acc: 0.9945 -- iter: 8/8\n",
            "--\n",
            "Training Step: 400  | total loss: \u001b[1m\u001b[32m0.04741\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 400 | loss: 0.04741 - acc: 0.9951 -- iter: 8/8\n",
            "--\n",
            "Training Step: 401  | total loss: \u001b[1m\u001b[32m0.04531\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 401 | loss: 0.04531 - acc: 0.9956 -- iter: 8/8\n",
            "--\n",
            "Training Step: 402  | total loss: \u001b[1m\u001b[32m0.04341\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 402 | loss: 0.04341 - acc: 0.9960 -- iter: 8/8\n",
            "--\n",
            "Training Step: 403  | total loss: \u001b[1m\u001b[32m0.04168\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 403 | loss: 0.04168 - acc: 0.9964 -- iter: 8/8\n",
            "--\n",
            "Training Step: 404  | total loss: \u001b[1m\u001b[32m0.04010\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 404 | loss: 0.04010 - acc: 0.9968 -- iter: 8/8\n",
            "--\n",
            "Training Step: 405  | total loss: \u001b[1m\u001b[32m0.03866\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 405 | loss: 0.03866 - acc: 0.9971 -- iter: 8/8\n",
            "--\n",
            "Training Step: 406  | total loss: \u001b[1m\u001b[32m0.03735\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 406 | loss: 0.03735 - acc: 0.9974 -- iter: 8/8\n",
            "--\n",
            "Training Step: 407  | total loss: \u001b[1m\u001b[32m0.03614\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 407 | loss: 0.03614 - acc: 0.9976 -- iter: 8/8\n",
            "--\n",
            "Training Step: 408  | total loss: \u001b[1m\u001b[32m0.21249\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 408 | loss: 0.21249 - acc: 0.9479 -- iter: 8/8\n",
            "--\n",
            "Training Step: 409  | total loss: \u001b[1m\u001b[32m0.19377\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 409 | loss: 0.19377 - acc: 0.9531 -- iter: 8/8\n",
            "--\n",
            "Training Step: 410  | total loss: \u001b[1m\u001b[32m0.46215\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 410 | loss: 0.46215 - acc: 0.8828 -- iter: 8/8\n",
            "--\n",
            "Training Step: 411  | total loss: \u001b[1m\u001b[32m0.41856\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 411 | loss: 0.41856 - acc: 0.8945 -- iter: 8/8\n",
            "--\n",
            "Training Step: 412  | total loss: \u001b[1m\u001b[32m0.37939\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 412 | loss: 0.37939 - acc: 0.9051 -- iter: 8/8\n",
            "--\n",
            "Training Step: 413  | total loss: \u001b[1m\u001b[32m0.34419\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 413 | loss: 0.34419 - acc: 0.9145 -- iter: 8/8\n",
            "--\n",
            "Training Step: 414  | total loss: \u001b[1m\u001b[32m0.31257\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 414 | loss: 0.31257 - acc: 0.9231 -- iter: 8/8\n",
            "--\n",
            "Training Step: 415  | total loss: \u001b[1m\u001b[32m0.28415\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 415 | loss: 0.28415 - acc: 0.9308 -- iter: 8/8\n",
            "--\n",
            "Training Step: 416  | total loss: \u001b[1m\u001b[32m0.25862\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 416 | loss: 0.25862 - acc: 0.9377 -- iter: 8/8\n",
            "--\n",
            "Training Step: 417  | total loss: \u001b[1m\u001b[32m0.23567\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 417 | loss: 0.23567 - acc: 0.9439 -- iter: 8/8\n",
            "--\n",
            "Training Step: 418  | total loss: \u001b[1m\u001b[32m0.21504\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 418 | loss: 0.21504 - acc: 0.9495 -- iter: 8/8\n",
            "--\n",
            "Training Step: 419  | total loss: \u001b[1m\u001b[32m0.19649\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 419 | loss: 0.19649 - acc: 0.9546 -- iter: 8/8\n",
            "--\n",
            "Training Step: 420  | total loss: \u001b[1m\u001b[32m0.17982\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 420 | loss: 0.17982 - acc: 0.9591 -- iter: 8/8\n",
            "--\n",
            "Training Step: 421  | total loss: \u001b[1m\u001b[32m0.16483\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 421 | loss: 0.16483 - acc: 0.9632 -- iter: 8/8\n",
            "--\n",
            "Training Step: 422  | total loss: \u001b[1m\u001b[32m0.15135\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 422 | loss: 0.15135 - acc: 0.9669 -- iter: 8/8\n",
            "--\n",
            "Training Step: 423  | total loss: \u001b[1m\u001b[32m0.13922\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 423 | loss: 0.13922 - acc: 0.9702 -- iter: 8/8\n",
            "--\n",
            "Training Step: 424  | total loss: \u001b[1m\u001b[32m0.12830\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 424 | loss: 0.12830 - acc: 0.9732 -- iter: 8/8\n",
            "--\n",
            "Training Step: 425  | total loss: \u001b[1m\u001b[32m0.11848\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 425 | loss: 0.11848 - acc: 0.9759 -- iter: 8/8\n",
            "--\n",
            "Training Step: 426  | total loss: \u001b[1m\u001b[32m0.10964\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 426 | loss: 0.10964 - acc: 0.9783 -- iter: 8/8\n",
            "--\n",
            "Training Step: 427  | total loss: \u001b[1m\u001b[32m0.10168\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 427 | loss: 0.10168 - acc: 0.9805 -- iter: 8/8\n",
            "--\n",
            "Training Step: 428  | total loss: \u001b[1m\u001b[32m0.09450\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 428 | loss: 0.09450 - acc: 0.9824 -- iter: 8/8\n",
            "--\n",
            "Training Step: 429  | total loss: \u001b[1m\u001b[32m0.08803\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 429 | loss: 0.08803 - acc: 0.9842 -- iter: 8/8\n",
            "--\n",
            "Training Step: 430  | total loss: \u001b[1m\u001b[32m0.08220\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 430 | loss: 0.08220 - acc: 0.9857 -- iter: 8/8\n",
            "--\n",
            "Training Step: 431  | total loss: \u001b[1m\u001b[32m0.07694\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 431 | loss: 0.07694 - acc: 0.9872 -- iter: 8/8\n",
            "--\n",
            "Training Step: 432  | total loss: \u001b[1m\u001b[32m0.07220\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 432 | loss: 0.07220 - acc: 0.9885 -- iter: 8/8\n",
            "--\n",
            "Training Step: 433  | total loss: \u001b[1m\u001b[32m0.06791\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 433 | loss: 0.06791 - acc: 0.9896 -- iter: 8/8\n",
            "--\n",
            "Training Step: 434  | total loss: \u001b[1m\u001b[32m0.06403\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 434 | loss: 0.06403 - acc: 0.9906 -- iter: 8/8\n",
            "--\n",
            "Training Step: 435  | total loss: \u001b[1m\u001b[32m0.06053\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 435 | loss: 0.06053 - acc: 0.9916 -- iter: 8/8\n",
            "--\n",
            "Training Step: 436  | total loss: \u001b[1m\u001b[32m0.05736\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 436 | loss: 0.05736 - acc: 0.9924 -- iter: 8/8\n",
            "--\n",
            "Training Step: 437  | total loss: \u001b[1m\u001b[32m0.05449\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 437 | loss: 0.05449 - acc: 0.9932 -- iter: 8/8\n",
            "--\n",
            "Training Step: 438  | total loss: \u001b[1m\u001b[32m0.05188\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 438 | loss: 0.05188 - acc: 0.9939 -- iter: 8/8\n",
            "--\n",
            "Training Step: 439  | total loss: \u001b[1m\u001b[32m0.04952\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 439 | loss: 0.04952 - acc: 0.9945 -- iter: 8/8\n",
            "--\n",
            "Training Step: 440  | total loss: \u001b[1m\u001b[32m0.04737\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 440 | loss: 0.04737 - acc: 0.9950 -- iter: 8/8\n",
            "--\n",
            "Training Step: 441  | total loss: \u001b[1m\u001b[32m0.04542\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 441 | loss: 0.04542 - acc: 0.9955 -- iter: 8/8\n",
            "--\n",
            "Training Step: 442  | total loss: \u001b[1m\u001b[32m0.04364\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 442 | loss: 0.04364 - acc: 0.9960 -- iter: 8/8\n",
            "--\n",
            "Training Step: 443  | total loss: \u001b[1m\u001b[32m0.04202\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 443 | loss: 0.04202 - acc: 0.9964 -- iter: 8/8\n",
            "--\n",
            "Training Step: 444  | total loss: \u001b[1m\u001b[32m0.04054\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 444 | loss: 0.04054 - acc: 0.9967 -- iter: 8/8\n",
            "--\n",
            "Training Step: 445  | total loss: \u001b[1m\u001b[32m0.03919\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 445 | loss: 0.03919 - acc: 0.9971 -- iter: 8/8\n",
            "--\n",
            "Training Step: 446  | total loss: \u001b[1m\u001b[32m0.03796\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 446 | loss: 0.03796 - acc: 0.9974 -- iter: 8/8\n",
            "--\n",
            "Training Step: 447  | total loss: \u001b[1m\u001b[32m0.03682\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 447 | loss: 0.03682 - acc: 0.9976 -- iter: 8/8\n",
            "--\n",
            "Training Step: 448  | total loss: \u001b[1m\u001b[32m0.03578\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 448 | loss: 0.03578 - acc: 0.9979 -- iter: 8/8\n",
            "--\n",
            "Training Step: 449  | total loss: \u001b[1m\u001b[32m0.03482\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 449 | loss: 0.03482 - acc: 0.9981 -- iter: 8/8\n",
            "--\n",
            "Training Step: 450  | total loss: \u001b[1m\u001b[32m0.03393\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 450 | loss: 0.03393 - acc: 0.9983 -- iter: 8/8\n",
            "--\n",
            "Training Step: 451  | total loss: \u001b[1m\u001b[32m0.03312\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 451 | loss: 0.03312 - acc: 0.9984 -- iter: 8/8\n",
            "--\n",
            "Training Step: 452  | total loss: \u001b[1m\u001b[32m0.03236\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 452 | loss: 0.03236 - acc: 0.9986 -- iter: 8/8\n",
            "--\n",
            "Training Step: 453  | total loss: \u001b[1m\u001b[32m0.03166\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 453 | loss: 0.03166 - acc: 0.9987 -- iter: 8/8\n",
            "--\n",
            "Training Step: 454  | total loss: \u001b[1m\u001b[32m0.03101\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 454 | loss: 0.03101 - acc: 0.9989 -- iter: 8/8\n",
            "--\n",
            "Training Step: 455  | total loss: \u001b[1m\u001b[32m0.03040\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 455 | loss: 0.03040 - acc: 0.9990 -- iter: 8/8\n",
            "--\n",
            "Training Step: 456  | total loss: \u001b[1m\u001b[32m0.02983\u001b[0m\u001b[0m | time: 0.024s\n",
            "| Adam | epoch: 456 | loss: 0.02983 - acc: 0.9991 -- iter: 8/8\n",
            "--\n",
            "Training Step: 457  | total loss: \u001b[1m\u001b[32m0.02930\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 457 | loss: 0.02930 - acc: 0.9992 -- iter: 8/8\n",
            "--\n",
            "Training Step: 458  | total loss: \u001b[1m\u001b[32m0.02880\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 458 | loss: 0.02880 - acc: 0.9993 -- iter: 8/8\n",
            "--\n",
            "Training Step: 459  | total loss: \u001b[1m\u001b[32m0.02834\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 459 | loss: 0.02834 - acc: 0.9993 -- iter: 8/8\n",
            "--\n",
            "Training Step: 460  | total loss: \u001b[1m\u001b[32m0.02789\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 460 | loss: 0.02789 - acc: 0.9994 -- iter: 8/8\n",
            "--\n",
            "Training Step: 461  | total loss: \u001b[1m\u001b[32m0.02748\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 461 | loss: 0.02748 - acc: 0.9995 -- iter: 8/8\n",
            "--\n",
            "Training Step: 462  | total loss: \u001b[1m\u001b[32m0.02708\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 462 | loss: 0.02708 - acc: 0.9995 -- iter: 8/8\n",
            "--\n",
            "Training Step: 463  | total loss: \u001b[1m\u001b[32m0.02671\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 463 | loss: 0.02671 - acc: 0.9996 -- iter: 8/8\n",
            "--\n",
            "Training Step: 464  | total loss: \u001b[1m\u001b[32m0.02635\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 464 | loss: 0.02635 - acc: 0.9996 -- iter: 8/8\n",
            "--\n",
            "Training Step: 465  | total loss: \u001b[1m\u001b[32m0.02601\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 465 | loss: 0.02601 - acc: 0.9996 -- iter: 8/8\n",
            "--\n",
            "Training Step: 466  | total loss: \u001b[1m\u001b[32m0.02569\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 466 | loss: 0.02569 - acc: 0.9997 -- iter: 8/8\n",
            "--\n",
            "Training Step: 467  | total loss: \u001b[1m\u001b[32m0.02538\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 467 | loss: 0.02538 - acc: 0.9997 -- iter: 8/8\n",
            "--\n",
            "Training Step: 468  | total loss: \u001b[1m\u001b[32m0.02508\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 468 | loss: 0.02508 - acc: 0.9997 -- iter: 8/8\n",
            "--\n",
            "Training Step: 469  | total loss: \u001b[1m\u001b[32m0.02479\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 469 | loss: 0.02479 - acc: 0.9998 -- iter: 8/8\n",
            "--\n",
            "Training Step: 470  | total loss: \u001b[1m\u001b[32m0.02452\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 470 | loss: 0.02452 - acc: 0.9998 -- iter: 8/8\n",
            "--\n",
            "Training Step: 471  | total loss: \u001b[1m\u001b[32m0.02425\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 471 | loss: 0.02425 - acc: 0.9998 -- iter: 8/8\n",
            "--\n",
            "Training Step: 472  | total loss: \u001b[1m\u001b[32m0.02400\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 472 | loss: 0.02400 - acc: 0.9998 -- iter: 8/8\n",
            "--\n",
            "Training Step: 473  | total loss: \u001b[1m\u001b[32m0.02375\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 473 | loss: 0.02375 - acc: 0.9998 -- iter: 8/8\n",
            "--\n",
            "Training Step: 474  | total loss: \u001b[1m\u001b[32m0.02351\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 474 | loss: 0.02351 - acc: 0.9999 -- iter: 8/8\n",
            "--\n",
            "Training Step: 475  | total loss: \u001b[1m\u001b[32m0.02327\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 475 | loss: 0.02327 - acc: 0.9999 -- iter: 8/8\n",
            "--\n",
            "Training Step: 476  | total loss: \u001b[1m\u001b[32m0.02304\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 476 | loss: 0.02304 - acc: 0.9999 -- iter: 8/8\n",
            "--\n",
            "Training Step: 477  | total loss: \u001b[1m\u001b[32m0.02282\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 477 | loss: 0.02282 - acc: 0.9999 -- iter: 8/8\n",
            "--\n",
            "Training Step: 478  | total loss: \u001b[1m\u001b[32m0.02261\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 478 | loss: 0.02261 - acc: 0.9999 -- iter: 8/8\n",
            "--\n",
            "Training Step: 479  | total loss: \u001b[1m\u001b[32m0.02240\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 479 | loss: 0.02240 - acc: 0.9999 -- iter: 8/8\n",
            "--\n",
            "Training Step: 480  | total loss: \u001b[1m\u001b[32m0.02219\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 480 | loss: 0.02219 - acc: 0.9999 -- iter: 8/8\n",
            "--\n",
            "Training Step: 481  | total loss: \u001b[1m\u001b[32m0.02199\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 481 | loss: 0.02199 - acc: 0.9999 -- iter: 8/8\n",
            "--\n",
            "Training Step: 482  | total loss: \u001b[1m\u001b[32m0.02179\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 482 | loss: 0.02179 - acc: 0.9999 -- iter: 8/8\n",
            "--\n",
            "Training Step: 483  | total loss: \u001b[1m\u001b[32m0.02160\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 483 | loss: 0.02160 - acc: 0.9999 -- iter: 8/8\n",
            "--\n",
            "Training Step: 484  | total loss: \u001b[1m\u001b[32m0.02141\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 484 | loss: 0.02141 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 485  | total loss: \u001b[1m\u001b[32m0.02123\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 485 | loss: 0.02123 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 486  | total loss: \u001b[1m\u001b[32m0.02105\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 486 | loss: 0.02105 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 487  | total loss: \u001b[1m\u001b[32m0.02087\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 487 | loss: 0.02087 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 488  | total loss: \u001b[1m\u001b[32m0.02069\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 488 | loss: 0.02069 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 489  | total loss: \u001b[1m\u001b[32m0.02052\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 489 | loss: 0.02052 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 490  | total loss: \u001b[1m\u001b[32m0.02035\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 490 | loss: 0.02035 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 491  | total loss: \u001b[1m\u001b[32m0.02018\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 491 | loss: 0.02018 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 492  | total loss: \u001b[1m\u001b[32m0.02002\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 492 | loss: 0.02002 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 493  | total loss: \u001b[1m\u001b[32m0.01986\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 493 | loss: 0.01986 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 494  | total loss: \u001b[1m\u001b[32m0.01970\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 494 | loss: 0.01970 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 495  | total loss: \u001b[1m\u001b[32m0.01954\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 495 | loss: 0.01954 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 496  | total loss: \u001b[1m\u001b[32m0.01939\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 496 | loss: 0.01939 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 497  | total loss: \u001b[1m\u001b[32m0.01924\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 497 | loss: 0.01924 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 498  | total loss: \u001b[1m\u001b[32m0.01908\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 498 | loss: 0.01908 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 499  | total loss: \u001b[1m\u001b[32m0.01894\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 499 | loss: 0.01894 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 500  | total loss: \u001b[1m\u001b[32m0.01879\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 500 | loss: 0.01879 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 501  | total loss: \u001b[1m\u001b[32m0.01865\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 501 | loss: 0.01865 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 502  | total loss: \u001b[1m\u001b[32m0.01850\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 502 | loss: 0.01850 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 503  | total loss: \u001b[1m\u001b[32m0.01836\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 503 | loss: 0.01836 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 504  | total loss: \u001b[1m\u001b[32m0.01822\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 504 | loss: 0.01822 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 505  | total loss: \u001b[1m\u001b[32m0.01809\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 505 | loss: 0.01809 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 506  | total loss: \u001b[1m\u001b[32m0.01795\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 506 | loss: 0.01795 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 507  | total loss: \u001b[1m\u001b[32m0.01782\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 507 | loss: 0.01782 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 508  | total loss: \u001b[1m\u001b[32m0.01768\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 508 | loss: 0.01768 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 509  | total loss: \u001b[1m\u001b[32m0.01755\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 509 | loss: 0.01755 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 510  | total loss: \u001b[1m\u001b[32m0.01742\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 510 | loss: 0.01742 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 511  | total loss: \u001b[1m\u001b[32m0.01729\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 511 | loss: 0.01729 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 512  | total loss: \u001b[1m\u001b[32m0.01717\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 512 | loss: 0.01717 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 513  | total loss: \u001b[1m\u001b[32m0.01704\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 513 | loss: 0.01704 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 514  | total loss: \u001b[1m\u001b[32m0.01692\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 514 | loss: 0.01692 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 515  | total loss: \u001b[1m\u001b[32m0.01680\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 515 | loss: 0.01680 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 516  | total loss: \u001b[1m\u001b[32m0.33011\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 516 | loss: 0.33011 - acc: 0.9250 -- iter: 8/8\n",
            "--\n",
            "Training Step: 517  | total loss: \u001b[1m\u001b[32m0.29869\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 517 | loss: 0.29869 - acc: 0.9325 -- iter: 8/8\n",
            "--\n",
            "Training Step: 518  | total loss: \u001b[1m\u001b[32m0.49431\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 518 | loss: 0.49431 - acc: 0.8892 -- iter: 8/8\n",
            "--\n",
            "Training Step: 519  | total loss: \u001b[1m\u001b[32m0.44657\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 519 | loss: 0.44657 - acc: 0.9003 -- iter: 8/8\n",
            "--\n",
            "Training Step: 520  | total loss: \u001b[1m\u001b[32m0.40365\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 520 | loss: 0.40365 - acc: 0.9103 -- iter: 8/8\n",
            "--\n",
            "Training Step: 521  | total loss: \u001b[1m\u001b[32m0.36507\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 521 | loss: 0.36507 - acc: 0.9193 -- iter: 8/8\n",
            "--\n",
            "Training Step: 522  | total loss: \u001b[1m\u001b[32m0.33040\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 522 | loss: 0.33040 - acc: 0.9273 -- iter: 8/8\n",
            "--\n",
            "Training Step: 523  | total loss: \u001b[1m\u001b[32m0.29923\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 523 | loss: 0.29923 - acc: 0.9346 -- iter: 8/8\n",
            "--\n",
            "Training Step: 524  | total loss: \u001b[1m\u001b[32m0.27122\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 524 | loss: 0.27122 - acc: 0.9411 -- iter: 8/8\n",
            "--\n",
            "Training Step: 525  | total loss: \u001b[1m\u001b[32m0.24603\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 525 | loss: 0.24603 - acc: 0.9470 -- iter: 8/8\n",
            "--\n",
            "Training Step: 526  | total loss: \u001b[1m\u001b[32m0.22339\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 526 | loss: 0.22339 - acc: 0.9523 -- iter: 8/8\n",
            "--\n",
            "Training Step: 527  | total loss: \u001b[1m\u001b[32m0.20304\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 527 | loss: 0.20304 - acc: 0.9571 -- iter: 8/8\n",
            "--\n",
            "Training Step: 528  | total loss: \u001b[1m\u001b[32m0.18475\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 528 | loss: 0.18475 - acc: 0.9614 -- iter: 8/8\n",
            "--\n",
            "Training Step: 529  | total loss: \u001b[1m\u001b[32m0.16829\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 529 | loss: 0.16829 - acc: 0.9652 -- iter: 8/8\n",
            "--\n",
            "Training Step: 530  | total loss: \u001b[1m\u001b[32m0.15350\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 530 | loss: 0.15350 - acc: 0.9687 -- iter: 8/8\n",
            "--\n",
            "Training Step: 531  | total loss: \u001b[1m\u001b[32m0.14020\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 531 | loss: 0.14020 - acc: 0.9718 -- iter: 8/8\n",
            "--\n",
            "Training Step: 532  | total loss: \u001b[1m\u001b[32m0.12823\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 532 | loss: 0.12823 - acc: 0.9747 -- iter: 8/8\n",
            "--\n",
            "Training Step: 533  | total loss: \u001b[1m\u001b[32m0.11747\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 533 | loss: 0.11747 - acc: 0.9772 -- iter: 8/8\n",
            "--\n",
            "Training Step: 534  | total loss: \u001b[1m\u001b[32m0.10779\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 534 | loss: 0.10779 - acc: 0.9795 -- iter: 8/8\n",
            "--\n",
            "Training Step: 535  | total loss: \u001b[1m\u001b[32m0.09908\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 535 | loss: 0.09908 - acc: 0.9815 -- iter: 8/8\n",
            "--\n",
            "Training Step: 536  | total loss: \u001b[1m\u001b[32m0.09124\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 536 | loss: 0.09124 - acc: 0.9834 -- iter: 8/8\n",
            "--\n",
            "Training Step: 537  | total loss: \u001b[1m\u001b[32m0.08418\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 537 | loss: 0.08418 - acc: 0.9850 -- iter: 8/8\n",
            "--\n",
            "Training Step: 538  | total loss: \u001b[1m\u001b[32m0.07783\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 538 | loss: 0.07783 - acc: 0.9865 -- iter: 8/8\n",
            "--\n",
            "Training Step: 539  | total loss: \u001b[1m\u001b[32m0.07210\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 539 | loss: 0.07210 - acc: 0.9879 -- iter: 8/8\n",
            "--\n",
            "Training Step: 540  | total loss: \u001b[1m\u001b[32m0.06695\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 540 | loss: 0.06695 - acc: 0.9891 -- iter: 8/8\n",
            "--\n",
            "Training Step: 541  | total loss: \u001b[1m\u001b[32m0.06230\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 541 | loss: 0.06230 - acc: 0.9902 -- iter: 8/8\n",
            "--\n",
            "Training Step: 542  | total loss: \u001b[1m\u001b[32m0.05811\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 542 | loss: 0.05811 - acc: 0.9912 -- iter: 8/8\n",
            "--\n",
            "Training Step: 543  | total loss: \u001b[1m\u001b[32m0.05434\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 543 | loss: 0.05434 - acc: 0.9920 -- iter: 8/8\n",
            "--\n",
            "Training Step: 544  | total loss: \u001b[1m\u001b[32m0.05093\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 544 | loss: 0.05093 - acc: 0.9928 -- iter: 8/8\n",
            "--\n",
            "Training Step: 545  | total loss: \u001b[1m\u001b[32m0.04786\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 545 | loss: 0.04786 - acc: 0.9936 -- iter: 8/8\n",
            "--\n",
            "Training Step: 546  | total loss: \u001b[1m\u001b[32m0.04508\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 546 | loss: 0.04508 - acc: 0.9942 -- iter: 8/8\n",
            "--\n",
            "Training Step: 547  | total loss: \u001b[1m\u001b[32m0.04257\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 547 | loss: 0.04257 - acc: 0.9948 -- iter: 8/8\n",
            "--\n",
            "Training Step: 548  | total loss: \u001b[1m\u001b[32m0.04030\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 548 | loss: 0.04030 - acc: 0.9953 -- iter: 8/8\n",
            "--\n",
            "Training Step: 549  | total loss: \u001b[1m\u001b[32m0.03825\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 549 | loss: 0.03825 - acc: 0.9958 -- iter: 8/8\n",
            "--\n",
            "Training Step: 550  | total loss: \u001b[1m\u001b[32m0.03639\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 550 | loss: 0.03639 - acc: 0.9962 -- iter: 8/8\n",
            "--\n",
            "Training Step: 551  | total loss: \u001b[1m\u001b[32m0.03471\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 551 | loss: 0.03471 - acc: 0.9966 -- iter: 8/8\n",
            "--\n",
            "Training Step: 552  | total loss: \u001b[1m\u001b[32m0.03318\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 552 | loss: 0.03318 - acc: 0.9969 -- iter: 8/8\n",
            "--\n",
            "Training Step: 553  | total loss: \u001b[1m\u001b[32m0.03180\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 553 | loss: 0.03180 - acc: 0.9972 -- iter: 8/8\n",
            "--\n",
            "Training Step: 554  | total loss: \u001b[1m\u001b[32m0.03054\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 554 | loss: 0.03054 - acc: 0.9975 -- iter: 8/8\n",
            "--\n",
            "Training Step: 555  | total loss: \u001b[1m\u001b[32m0.02939\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 555 | loss: 0.02939 - acc: 0.9978 -- iter: 8/8\n",
            "--\n",
            "Training Step: 556  | total loss: \u001b[1m\u001b[32m0.02835\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 556 | loss: 0.02835 - acc: 0.9980 -- iter: 8/8\n",
            "--\n",
            "Training Step: 557  | total loss: \u001b[1m\u001b[32m0.02740\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 557 | loss: 0.02740 - acc: 0.9982 -- iter: 8/8\n",
            "--\n",
            "Training Step: 558  | total loss: \u001b[1m\u001b[32m0.02653\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 558 | loss: 0.02653 - acc: 0.9984 -- iter: 8/8\n",
            "--\n",
            "Training Step: 559  | total loss: \u001b[1m\u001b[32m0.02574\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 559 | loss: 0.02574 - acc: 0.9985 -- iter: 8/8\n",
            "--\n",
            "Training Step: 560  | total loss: \u001b[1m\u001b[32m0.02501\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 560 | loss: 0.02501 - acc: 0.9987 -- iter: 8/8\n",
            "--\n",
            "Training Step: 561  | total loss: \u001b[1m\u001b[32m0.02435\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 561 | loss: 0.02435 - acc: 0.9988 -- iter: 8/8\n",
            "--\n",
            "Training Step: 562  | total loss: \u001b[1m\u001b[32m0.02374\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 562 | loss: 0.02374 - acc: 0.9989 -- iter: 8/8\n",
            "--\n",
            "Training Step: 563  | total loss: \u001b[1m\u001b[32m0.02318\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 563 | loss: 0.02318 - acc: 0.9990 -- iter: 8/8\n",
            "--\n",
            "Training Step: 564  | total loss: \u001b[1m\u001b[32m0.02266\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 564 | loss: 0.02266 - acc: 0.9991 -- iter: 8/8\n",
            "--\n",
            "Training Step: 565  | total loss: \u001b[1m\u001b[32m0.02218\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 565 | loss: 0.02218 - acc: 0.9992 -- iter: 8/8\n",
            "--\n",
            "Training Step: 566  | total loss: \u001b[1m\u001b[32m0.02174\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 566 | loss: 0.02174 - acc: 0.9993 -- iter: 8/8\n",
            "--\n",
            "Training Step: 567  | total loss: \u001b[1m\u001b[32m0.02133\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 567 | loss: 0.02133 - acc: 0.9994 -- iter: 8/8\n",
            "--\n",
            "Training Step: 568  | total loss: \u001b[1m\u001b[32m0.02095\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 568 | loss: 0.02095 - acc: 0.9994 -- iter: 8/8\n",
            "--\n",
            "Training Step: 569  | total loss: \u001b[1m\u001b[32m0.02059\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 569 | loss: 0.02059 - acc: 0.9995 -- iter: 8/8\n",
            "--\n",
            "Training Step: 570  | total loss: \u001b[1m\u001b[32m0.02026\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 570 | loss: 0.02026 - acc: 0.9995 -- iter: 8/8\n",
            "--\n",
            "Training Step: 571  | total loss: \u001b[1m\u001b[32m0.01995\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 571 | loss: 0.01995 - acc: 0.9996 -- iter: 8/8\n",
            "--\n",
            "Training Step: 572  | total loss: \u001b[1m\u001b[32m0.01966\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 572 | loss: 0.01966 - acc: 0.9996 -- iter: 8/8\n",
            "--\n",
            "Training Step: 573  | total loss: \u001b[1m\u001b[32m0.01939\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 573 | loss: 0.01939 - acc: 0.9997 -- iter: 8/8\n",
            "--\n",
            "Training Step: 574  | total loss: \u001b[1m\u001b[32m0.01913\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 574 | loss: 0.01913 - acc: 0.9997 -- iter: 8/8\n",
            "--\n",
            "Training Step: 575  | total loss: \u001b[1m\u001b[32m0.01889\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 575 | loss: 0.01889 - acc: 0.9997 -- iter: 8/8\n",
            "--\n",
            "Training Step: 576  | total loss: \u001b[1m\u001b[32m0.01866\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 576 | loss: 0.01866 - acc: 0.9998 -- iter: 8/8\n",
            "--\n",
            "Training Step: 577  | total loss: \u001b[1m\u001b[32m0.01844\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 577 | loss: 0.01844 - acc: 0.9998 -- iter: 8/8\n",
            "--\n",
            "Training Step: 578  | total loss: \u001b[1m\u001b[32m0.01823\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 578 | loss: 0.01823 - acc: 0.9998 -- iter: 8/8\n",
            "--\n",
            "Training Step: 579  | total loss: \u001b[1m\u001b[32m0.01803\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 579 | loss: 0.01803 - acc: 0.9998 -- iter: 8/8\n",
            "--\n",
            "Training Step: 580  | total loss: \u001b[1m\u001b[32m0.01785\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 580 | loss: 0.01785 - acc: 0.9998 -- iter: 8/8\n",
            "--\n",
            "Training Step: 581  | total loss: \u001b[1m\u001b[32m0.01766\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 581 | loss: 0.01766 - acc: 0.9999 -- iter: 8/8\n",
            "--\n",
            "Training Step: 582  | total loss: \u001b[1m\u001b[32m0.01749\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 582 | loss: 0.01749 - acc: 0.9999 -- iter: 8/8\n",
            "--\n",
            "Training Step: 583  | total loss: \u001b[1m\u001b[32m0.01732\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 583 | loss: 0.01732 - acc: 0.9999 -- iter: 8/8\n",
            "--\n",
            "Training Step: 584  | total loss: \u001b[1m\u001b[32m0.01716\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 584 | loss: 0.01716 - acc: 0.9999 -- iter: 8/8\n",
            "--\n",
            "Training Step: 585  | total loss: \u001b[1m\u001b[32m0.01701\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 585 | loss: 0.01701 - acc: 0.9999 -- iter: 8/8\n",
            "--\n",
            "Training Step: 586  | total loss: \u001b[1m\u001b[32m0.01686\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 586 | loss: 0.01686 - acc: 0.9999 -- iter: 8/8\n",
            "--\n",
            "Training Step: 587  | total loss: \u001b[1m\u001b[32m0.01671\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 587 | loss: 0.01671 - acc: 0.9999 -- iter: 8/8\n",
            "--\n",
            "Training Step: 588  | total loss: \u001b[1m\u001b[32m0.01657\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 588 | loss: 0.01657 - acc: 0.9999 -- iter: 8/8\n",
            "--\n",
            "Training Step: 589  | total loss: \u001b[1m\u001b[32m0.01644\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 589 | loss: 0.01644 - acc: 0.9999 -- iter: 8/8\n",
            "--\n",
            "Training Step: 590  | total loss: \u001b[1m\u001b[32m0.01630\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 590 | loss: 0.01630 - acc: 0.9999 -- iter: 8/8\n",
            "--\n",
            "Training Step: 591  | total loss: \u001b[1m\u001b[32m0.01617\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 591 | loss: 0.01617 - acc: 0.9999 -- iter: 8/8\n",
            "--\n",
            "Training Step: 592  | total loss: \u001b[1m\u001b[32m0.01605\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 592 | loss: 0.01605 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 593  | total loss: \u001b[1m\u001b[32m0.01592\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 593 | loss: 0.01592 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 594  | total loss: \u001b[1m\u001b[32m0.01580\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 594 | loss: 0.01580 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 595  | total loss: \u001b[1m\u001b[32m0.01569\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 595 | loss: 0.01569 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 596  | total loss: \u001b[1m\u001b[32m0.01557\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 596 | loss: 0.01557 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 597  | total loss: \u001b[1m\u001b[32m0.01546\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 597 | loss: 0.01546 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 598  | total loss: \u001b[1m\u001b[32m0.01535\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 598 | loss: 0.01535 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 599  | total loss: \u001b[1m\u001b[32m0.01524\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 599 | loss: 0.01524 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 600  | total loss: \u001b[1m\u001b[32m0.01513\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 600 | loss: 0.01513 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 601  | total loss: \u001b[1m\u001b[32m0.01502\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 601 | loss: 0.01502 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 602  | total loss: \u001b[1m\u001b[32m0.01492\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 602 | loss: 0.01492 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 603  | total loss: \u001b[1m\u001b[32m0.01482\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 603 | loss: 0.01482 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 604  | total loss: \u001b[1m\u001b[32m0.01472\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 604 | loss: 0.01472 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 605  | total loss: \u001b[1m\u001b[32m0.01462\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 605 | loss: 0.01462 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 606  | total loss: \u001b[1m\u001b[32m0.01452\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 606 | loss: 0.01452 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 607  | total loss: \u001b[1m\u001b[32m0.01442\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 607 | loss: 0.01442 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 608  | total loss: \u001b[1m\u001b[32m0.01433\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 608 | loss: 0.01433 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 609  | total loss: \u001b[1m\u001b[32m0.01423\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 609 | loss: 0.01423 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 610  | total loss: \u001b[1m\u001b[32m0.01414\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 610 | loss: 0.01414 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 611  | total loss: \u001b[1m\u001b[32m0.01405\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 611 | loss: 0.01405 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 612  | total loss: \u001b[1m\u001b[32m0.01396\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 612 | loss: 0.01396 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 613  | total loss: \u001b[1m\u001b[32m0.01387\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 613 | loss: 0.01387 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 614  | total loss: \u001b[1m\u001b[32m0.01378\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 614 | loss: 0.01378 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 615  | total loss: \u001b[1m\u001b[32m0.01369\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 615 | loss: 0.01369 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 616  | total loss: \u001b[1m\u001b[32m0.45410\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 616 | loss: 0.45410 - acc: 0.9000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 617  | total loss: \u001b[1m\u001b[32m0.41001\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 617 | loss: 0.41001 - acc: 0.9100 -- iter: 8/8\n",
            "--\n",
            "Training Step: 618  | total loss: \u001b[1m\u001b[32m0.37038\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 618 | loss: 0.37038 - acc: 0.9190 -- iter: 8/8\n",
            "--\n",
            "Training Step: 619  | total loss: \u001b[1m\u001b[32m0.33475\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 619 | loss: 0.33475 - acc: 0.9271 -- iter: 8/8\n",
            "--\n",
            "Training Step: 620  | total loss: \u001b[1m\u001b[32m0.30272\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 620 | loss: 0.30272 - acc: 0.9344 -- iter: 8/8\n",
            "--\n",
            "Training Step: 621  | total loss: \u001b[1m\u001b[32m0.27392\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 621 | loss: 0.27392 - acc: 0.9409 -- iter: 8/8\n",
            "--\n",
            "Training Step: 622  | total loss: \u001b[1m\u001b[32m0.35947\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 622 | loss: 0.35947 - acc: 0.9219 -- iter: 8/8\n",
            "--\n",
            "Training Step: 623  | total loss: \u001b[1m\u001b[32m0.32506\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 623 | loss: 0.32506 - acc: 0.9297 -- iter: 8/8\n",
            "--\n",
            "Training Step: 624  | total loss: \u001b[1m\u001b[32m0.29413\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 624 | loss: 0.29413 - acc: 0.9367 -- iter: 8/8\n",
            "--\n",
            "Training Step: 625  | total loss: \u001b[1m\u001b[32m0.26632\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 625 | loss: 0.26632 - acc: 0.9430 -- iter: 8/8\n",
            "--\n",
            "Training Step: 626  | total loss: \u001b[1m\u001b[32m0.24132\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 626 | loss: 0.24132 - acc: 0.9487 -- iter: 8/8\n",
            "--\n",
            "Training Step: 627  | total loss: \u001b[1m\u001b[32m0.21884\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 627 | loss: 0.21884 - acc: 0.9539 -- iter: 8/8\n",
            "--\n",
            "Training Step: 628  | total loss: \u001b[1m\u001b[32m0.19864\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 628 | loss: 0.19864 - acc: 0.9585 -- iter: 8/8\n",
            "--\n",
            "Training Step: 629  | total loss: \u001b[1m\u001b[32m0.18047\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 629 | loss: 0.18047 - acc: 0.9626 -- iter: 8/8\n",
            "--\n",
            "Training Step: 630  | total loss: \u001b[1m\u001b[32m0.16413\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 630 | loss: 0.16413 - acc: 0.9664 -- iter: 8/8\n",
            "--\n",
            "Training Step: 631  | total loss: \u001b[1m\u001b[32m0.14945\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 631 | loss: 0.14945 - acc: 0.9697 -- iter: 8/8\n",
            "--\n",
            "Training Step: 632  | total loss: \u001b[1m\u001b[32m0.13624\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 632 | loss: 0.13624 - acc: 0.9728 -- iter: 8/8\n",
            "--\n",
            "Training Step: 633  | total loss: \u001b[1m\u001b[32m0.12436\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 633 | loss: 0.12436 - acc: 0.9755 -- iter: 8/8\n",
            "--\n",
            "Training Step: 634  | total loss: \u001b[1m\u001b[32m0.11367\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 634 | loss: 0.11367 - acc: 0.9779 -- iter: 8/8\n",
            "--\n",
            "Training Step: 635  | total loss: \u001b[1m\u001b[32m0.10406\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 635 | loss: 0.10406 - acc: 0.9801 -- iter: 8/8\n",
            "--\n",
            "Training Step: 636  | total loss: \u001b[1m\u001b[32m0.09541\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 636 | loss: 0.09541 - acc: 0.9821 -- iter: 8/8\n",
            "--\n",
            "Training Step: 637  | total loss: \u001b[1m\u001b[32m0.08763\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 637 | loss: 0.08763 - acc: 0.9839 -- iter: 8/8\n",
            "--\n",
            "Training Step: 638  | total loss: \u001b[1m\u001b[32m0.08063\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 638 | loss: 0.08063 - acc: 0.9855 -- iter: 8/8\n",
            "--\n",
            "Training Step: 639  | total loss: \u001b[1m\u001b[32m0.07433\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 639 | loss: 0.07433 - acc: 0.9870 -- iter: 8/8\n",
            "--\n",
            "Training Step: 640  | total loss: \u001b[1m\u001b[32m0.06865\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 640 | loss: 0.06865 - acc: 0.9883 -- iter: 8/8\n",
            "--\n",
            "Training Step: 641  | total loss: \u001b[1m\u001b[32m0.06354\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 641 | loss: 0.06354 - acc: 0.9894 -- iter: 8/8\n",
            "--\n",
            "Training Step: 642  | total loss: \u001b[1m\u001b[32m0.05894\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 642 | loss: 0.05894 - acc: 0.9905 -- iter: 8/8\n",
            "--\n",
            "Training Step: 643  | total loss: \u001b[1m\u001b[32m0.05480\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 643 | loss: 0.05480 - acc: 0.9914 -- iter: 8/8\n",
            "--\n",
            "Training Step: 644  | total loss: \u001b[1m\u001b[32m0.05106\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 644 | loss: 0.05106 - acc: 0.9923 -- iter: 8/8\n",
            "--\n",
            "Training Step: 645  | total loss: \u001b[1m\u001b[32m0.04769\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 645 | loss: 0.04769 - acc: 0.9931 -- iter: 8/8\n",
            "--\n",
            "Training Step: 646  | total loss: \u001b[1m\u001b[32m0.04465\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 646 | loss: 0.04465 - acc: 0.9938 -- iter: 8/8\n",
            "--\n",
            "Training Step: 647  | total loss: \u001b[1m\u001b[32m0.04191\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 647 | loss: 0.04191 - acc: 0.9944 -- iter: 8/8\n",
            "--\n",
            "Training Step: 648  | total loss: \u001b[1m\u001b[32m0.03944\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 648 | loss: 0.03944 - acc: 0.9950 -- iter: 8/8\n",
            "--\n",
            "Training Step: 649  | total loss: \u001b[1m\u001b[32m0.03721\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 649 | loss: 0.03721 - acc: 0.9955 -- iter: 8/8\n",
            "--\n",
            "Training Step: 650  | total loss: \u001b[1m\u001b[32m0.03519\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 650 | loss: 0.03519 - acc: 0.9959 -- iter: 8/8\n",
            "--\n",
            "Training Step: 651  | total loss: \u001b[1m\u001b[32m0.03336\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 651 | loss: 0.03336 - acc: 0.9963 -- iter: 8/8\n",
            "--\n",
            "Training Step: 652  | total loss: \u001b[1m\u001b[32m0.03171\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 652 | loss: 0.03171 - acc: 0.9967 -- iter: 8/8\n",
            "--\n",
            "Training Step: 653  | total loss: \u001b[1m\u001b[32m0.03022\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 653 | loss: 0.03022 - acc: 0.9970 -- iter: 8/8\n",
            "--\n",
            "Training Step: 654  | total loss: \u001b[1m\u001b[32m0.02887\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 654 | loss: 0.02887 - acc: 0.9973 -- iter: 8/8\n",
            "--\n",
            "Training Step: 655  | total loss: \u001b[1m\u001b[32m0.02764\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 655 | loss: 0.02764 - acc: 0.9976 -- iter: 8/8\n",
            "--\n",
            "Training Step: 656  | total loss: \u001b[1m\u001b[32m0.02653\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 656 | loss: 0.02653 - acc: 0.9978 -- iter: 8/8\n",
            "--\n",
            "Training Step: 657  | total loss: \u001b[1m\u001b[32m0.02552\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 657 | loss: 0.02552 - acc: 0.9980 -- iter: 8/8\n",
            "--\n",
            "Training Step: 658  | total loss: \u001b[1m\u001b[32m0.02460\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 658 | loss: 0.02460 - acc: 0.9982 -- iter: 8/8\n",
            "--\n",
            "Training Step: 659  | total loss: \u001b[1m\u001b[32m0.02376\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 659 | loss: 0.02376 - acc: 0.9984 -- iter: 8/8\n",
            "--\n",
            "Training Step: 660  | total loss: \u001b[1m\u001b[32m0.02300\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 660 | loss: 0.02300 - acc: 0.9986 -- iter: 8/8\n",
            "--\n",
            "Training Step: 661  | total loss: \u001b[1m\u001b[32m0.02230\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 661 | loss: 0.02230 - acc: 0.9987 -- iter: 8/8\n",
            "--\n",
            "Training Step: 662  | total loss: \u001b[1m\u001b[32m0.02167\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 662 | loss: 0.02167 - acc: 0.9988 -- iter: 8/8\n",
            "--\n",
            "Training Step: 663  | total loss: \u001b[1m\u001b[32m0.02109\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 663 | loss: 0.02109 - acc: 0.9990 -- iter: 8/8\n",
            "--\n",
            "Training Step: 664  | total loss: \u001b[1m\u001b[32m0.02056\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 664 | loss: 0.02056 - acc: 0.9991 -- iter: 8/8\n",
            "--\n",
            "Training Step: 665  | total loss: \u001b[1m\u001b[32m0.02007\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 665 | loss: 0.02007 - acc: 0.9992 -- iter: 8/8\n",
            "--\n",
            "Training Step: 666  | total loss: \u001b[1m\u001b[32m0.01962\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 666 | loss: 0.01962 - acc: 0.9992 -- iter: 8/8\n",
            "--\n",
            "Training Step: 667  | total loss: \u001b[1m\u001b[32m0.01920\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 667 | loss: 0.01920 - acc: 0.9993 -- iter: 8/8\n",
            "--\n",
            "Training Step: 668  | total loss: \u001b[1m\u001b[32m0.01882\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 668 | loss: 0.01882 - acc: 0.9994 -- iter: 8/8\n",
            "--\n",
            "Training Step: 669  | total loss: \u001b[1m\u001b[32m0.01847\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 669 | loss: 0.01847 - acc: 0.9994 -- iter: 8/8\n",
            "--\n",
            "Training Step: 670  | total loss: \u001b[1m\u001b[32m0.01814\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 670 | loss: 0.01814 - acc: 0.9995 -- iter: 8/8\n",
            "--\n",
            "Training Step: 671  | total loss: \u001b[1m\u001b[32m0.01784\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 671 | loss: 0.01784 - acc: 0.9996 -- iter: 8/8\n",
            "--\n",
            "Training Step: 672  | total loss: \u001b[1m\u001b[32m0.22815\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 672 | loss: 0.22815 - acc: 0.9496 -- iter: 8/8\n",
            "--\n",
            "Training Step: 673  | total loss: \u001b[1m\u001b[32m0.20686\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 673 | loss: 0.20686 - acc: 0.9546 -- iter: 8/8\n",
            "--\n",
            "Training Step: 674  | total loss: \u001b[1m\u001b[32m0.18771\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 674 | loss: 0.18771 - acc: 0.9592 -- iter: 8/8\n",
            "--\n",
            "Training Step: 675  | total loss: \u001b[1m\u001b[32m0.17050\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 675 | loss: 0.17050 - acc: 0.9633 -- iter: 8/8\n",
            "--\n",
            "Training Step: 676  | total loss: \u001b[1m\u001b[32m0.15502\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 676 | loss: 0.15502 - acc: 0.9669 -- iter: 8/8\n",
            "--\n",
            "Training Step: 677  | total loss: \u001b[1m\u001b[32m0.14110\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 677 | loss: 0.14110 - acc: 0.9702 -- iter: 8/8\n",
            "--\n",
            "Training Step: 678  | total loss: \u001b[1m\u001b[32m0.12858\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 678 | loss: 0.12858 - acc: 0.9732 -- iter: 8/8\n",
            "--\n",
            "Training Step: 679  | total loss: \u001b[1m\u001b[32m0.11732\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 679 | loss: 0.11732 - acc: 0.9759 -- iter: 8/8\n",
            "--\n",
            "Training Step: 680  | total loss: \u001b[1m\u001b[32m0.10719\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 680 | loss: 0.10719 - acc: 0.9783 -- iter: 8/8\n",
            "--\n",
            "Training Step: 681  | total loss: \u001b[1m\u001b[32m0.09808\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 681 | loss: 0.09808 - acc: 0.9805 -- iter: 8/8\n",
            "--\n",
            "Training Step: 682  | total loss: \u001b[1m\u001b[32m0.08988\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 682 | loss: 0.08988 - acc: 0.9824 -- iter: 8/8\n",
            "--\n",
            "Training Step: 683  | total loss: \u001b[1m\u001b[32m0.08250\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 683 | loss: 0.08250 - acc: 0.9842 -- iter: 8/8\n",
            "--\n",
            "Training Step: 684  | total loss: \u001b[1m\u001b[32m0.07586\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 684 | loss: 0.07586 - acc: 0.9858 -- iter: 8/8\n",
            "--\n",
            "Training Step: 685  | total loss: \u001b[1m\u001b[32m0.06989\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 685 | loss: 0.06989 - acc: 0.9872 -- iter: 8/8\n",
            "--\n",
            "Training Step: 686  | total loss: \u001b[1m\u001b[32m0.06451\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 686 | loss: 0.06451 - acc: 0.9885 -- iter: 8/8\n",
            "--\n",
            "Training Step: 687  | total loss: \u001b[1m\u001b[32m0.05966\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 687 | loss: 0.05966 - acc: 0.9896 -- iter: 8/8\n",
            "--\n",
            "Training Step: 688  | total loss: \u001b[1m\u001b[32m0.05530\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 688 | loss: 0.05530 - acc: 0.9907 -- iter: 8/8\n",
            "--\n",
            "Training Step: 689  | total loss: \u001b[1m\u001b[32m0.05137\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 689 | loss: 0.05137 - acc: 0.9916 -- iter: 8/8\n",
            "--\n",
            "Training Step: 690  | total loss: \u001b[1m\u001b[32m0.04783\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 690 | loss: 0.04783 - acc: 0.9924 -- iter: 8/8\n",
            "--\n",
            "Training Step: 691  | total loss: \u001b[1m\u001b[32m0.04464\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 691 | loss: 0.04464 - acc: 0.9932 -- iter: 8/8\n",
            "--\n",
            "Training Step: 692  | total loss: \u001b[1m\u001b[32m0.04176\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 692 | loss: 0.04176 - acc: 0.9939 -- iter: 8/8\n",
            "--\n",
            "Training Step: 693  | total loss: \u001b[1m\u001b[32m0.03916\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 693 | loss: 0.03916 - acc: 0.9945 -- iter: 8/8\n",
            "--\n",
            "Training Step: 694  | total loss: \u001b[1m\u001b[32m0.03682\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 694 | loss: 0.03682 - acc: 0.9950 -- iter: 8/8\n",
            "--\n",
            "Training Step: 695  | total loss: \u001b[1m\u001b[32m0.03470\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 695 | loss: 0.03470 - acc: 0.9955 -- iter: 8/8\n",
            "--\n",
            "Training Step: 696  | total loss: \u001b[1m\u001b[32m0.03279\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 696 | loss: 0.03279 - acc: 0.9960 -- iter: 8/8\n",
            "--\n",
            "Training Step: 697  | total loss: \u001b[1m\u001b[32m0.03106\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 697 | loss: 0.03106 - acc: 0.9964 -- iter: 8/8\n",
            "--\n",
            "Training Step: 698  | total loss: \u001b[1m\u001b[32m0.02950\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 698 | loss: 0.02950 - acc: 0.9967 -- iter: 8/8\n",
            "--\n",
            "Training Step: 699  | total loss: \u001b[1m\u001b[32m0.02809\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 699 | loss: 0.02809 - acc: 0.9971 -- iter: 8/8\n",
            "--\n",
            "Training Step: 700  | total loss: \u001b[1m\u001b[32m0.02681\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 700 | loss: 0.02681 - acc: 0.9974 -- iter: 8/8\n",
            "--\n",
            "Training Step: 701  | total loss: \u001b[1m\u001b[32m0.02565\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 701 | loss: 0.02565 - acc: 0.9976 -- iter: 8/8\n",
            "--\n",
            "Training Step: 702  | total loss: \u001b[1m\u001b[32m0.02460\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 702 | loss: 0.02460 - acc: 0.9979 -- iter: 8/8\n",
            "--\n",
            "Training Step: 703  | total loss: \u001b[1m\u001b[32m0.02365\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 703 | loss: 0.02365 - acc: 0.9981 -- iter: 8/8\n",
            "--\n",
            "Training Step: 704  | total loss: \u001b[1m\u001b[32m0.02278\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 704 | loss: 0.02278 - acc: 0.9983 -- iter: 8/8\n",
            "--\n",
            "Training Step: 705  | total loss: \u001b[1m\u001b[32m0.02199\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 705 | loss: 0.02199 - acc: 0.9984 -- iter: 8/8\n",
            "--\n",
            "Training Step: 706  | total loss: \u001b[1m\u001b[32m0.02127\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 706 | loss: 0.02127 - acc: 0.9986 -- iter: 8/8\n",
            "--\n",
            "Training Step: 707  | total loss: \u001b[1m\u001b[32m0.02062\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 707 | loss: 0.02062 - acc: 0.9987 -- iter: 8/8\n",
            "--\n",
            "Training Step: 708  | total loss: \u001b[1m\u001b[32m0.02002\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 708 | loss: 0.02002 - acc: 0.9989 -- iter: 8/8\n",
            "--\n",
            "Training Step: 709  | total loss: \u001b[1m\u001b[32m0.01948\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 709 | loss: 0.01948 - acc: 0.9990 -- iter: 8/8\n",
            "--\n",
            "Training Step: 710  | total loss: \u001b[1m\u001b[32m0.01898\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 710 | loss: 0.01898 - acc: 0.9991 -- iter: 8/8\n",
            "--\n",
            "Training Step: 711  | total loss: \u001b[1m\u001b[32m0.01852\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 711 | loss: 0.01852 - acc: 0.9992 -- iter: 8/8\n",
            "--\n",
            "Training Step: 712  | total loss: \u001b[1m\u001b[32m0.01810\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 712 | loss: 0.01810 - acc: 0.9993 -- iter: 8/8\n",
            "--\n",
            "Training Step: 713  | total loss: \u001b[1m\u001b[32m0.01771\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 713 | loss: 0.01771 - acc: 0.9993 -- iter: 8/8\n",
            "--\n",
            "Training Step: 714  | total loss: \u001b[1m\u001b[32m0.01736\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 714 | loss: 0.01736 - acc: 0.9994 -- iter: 8/8\n",
            "--\n",
            "Training Step: 715  | total loss: \u001b[1m\u001b[32m0.01703\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 715 | loss: 0.01703 - acc: 0.9995 -- iter: 8/8\n",
            "--\n",
            "Training Step: 716  | total loss: \u001b[1m\u001b[32m0.01672\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 716 | loss: 0.01672 - acc: 0.9995 -- iter: 8/8\n",
            "--\n",
            "Training Step: 717  | total loss: \u001b[1m\u001b[32m0.01644\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 717 | loss: 0.01644 - acc: 0.9996 -- iter: 8/8\n",
            "--\n",
            "Training Step: 718  | total loss: \u001b[1m\u001b[32m0.01618\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 718 | loss: 0.01618 - acc: 0.9996 -- iter: 8/8\n",
            "--\n",
            "Training Step: 719  | total loss: \u001b[1m\u001b[32m0.01594\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 719 | loss: 0.01594 - acc: 0.9996 -- iter: 8/8\n",
            "--\n",
            "Training Step: 720  | total loss: \u001b[1m\u001b[32m0.01571\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 720 | loss: 0.01571 - acc: 0.9997 -- iter: 8/8\n",
            "--\n",
            "Training Step: 721  | total loss: \u001b[1m\u001b[32m0.01550\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 721 | loss: 0.01550 - acc: 0.9997 -- iter: 8/8\n",
            "--\n",
            "Training Step: 722  | total loss: \u001b[1m\u001b[32m0.01530\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 722 | loss: 0.01530 - acc: 0.9997 -- iter: 8/8\n",
            "--\n",
            "Training Step: 723  | total loss: \u001b[1m\u001b[32m0.01511\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 723 | loss: 0.01511 - acc: 0.9998 -- iter: 8/8\n",
            "--\n",
            "Training Step: 724  | total loss: \u001b[1m\u001b[32m0.01493\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 724 | loss: 0.01493 - acc: 0.9998 -- iter: 8/8\n",
            "--\n",
            "Training Step: 725  | total loss: \u001b[1m\u001b[32m0.01477\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 725 | loss: 0.01477 - acc: 0.9998 -- iter: 8/8\n",
            "--\n",
            "Training Step: 726  | total loss: \u001b[1m\u001b[32m0.01461\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 726 | loss: 0.01461 - acc: 0.9998 -- iter: 8/8\n",
            "--\n",
            "Training Step: 727  | total loss: \u001b[1m\u001b[32m0.01446\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 727 | loss: 0.01446 - acc: 0.9998 -- iter: 8/8\n",
            "--\n",
            "Training Step: 728  | total loss: \u001b[1m\u001b[32m0.01432\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 728 | loss: 0.01432 - acc: 0.9999 -- iter: 8/8\n",
            "--\n",
            "Training Step: 729  | total loss: \u001b[1m\u001b[32m0.01418\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 729 | loss: 0.01418 - acc: 0.9999 -- iter: 8/8\n",
            "--\n",
            "Training Step: 730  | total loss: \u001b[1m\u001b[32m0.01405\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 730 | loss: 0.01405 - acc: 0.9999 -- iter: 8/8\n",
            "--\n",
            "Training Step: 731  | total loss: \u001b[1m\u001b[32m0.01393\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 731 | loss: 0.01393 - acc: 0.9999 -- iter: 8/8\n",
            "--\n",
            "Training Step: 732  | total loss: \u001b[1m\u001b[32m0.01381\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 732 | loss: 0.01381 - acc: 0.9999 -- iter: 8/8\n",
            "--\n",
            "Training Step: 733  | total loss: \u001b[1m\u001b[32m0.01370\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 733 | loss: 0.01370 - acc: 0.9999 -- iter: 8/8\n",
            "--\n",
            "Training Step: 734  | total loss: \u001b[1m\u001b[32m0.01359\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 734 | loss: 0.01359 - acc: 0.9999 -- iter: 8/8\n",
            "--\n",
            "Training Step: 735  | total loss: \u001b[1m\u001b[32m0.01348\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 735 | loss: 0.01348 - acc: 0.9999 -- iter: 8/8\n",
            "--\n",
            "Training Step: 736  | total loss: \u001b[1m\u001b[32m0.01338\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 736 | loss: 0.01338 - acc: 0.9999 -- iter: 8/8\n",
            "--\n",
            "Training Step: 737  | total loss: \u001b[1m\u001b[32m0.01328\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 737 | loss: 0.01328 - acc: 0.9999 -- iter: 8/8\n",
            "--\n",
            "Training Step: 738  | total loss: \u001b[1m\u001b[32m0.01318\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 738 | loss: 0.01318 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 739  | total loss: \u001b[1m\u001b[32m0.01309\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 739 | loss: 0.01309 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 740  | total loss: \u001b[1m\u001b[32m0.01300\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 740 | loss: 0.01300 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 741  | total loss: \u001b[1m\u001b[32m0.01291\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 741 | loss: 0.01291 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 742  | total loss: \u001b[1m\u001b[32m0.01282\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 742 | loss: 0.01282 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 743  | total loss: \u001b[1m\u001b[32m0.01273\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 743 | loss: 0.01273 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 744  | total loss: \u001b[1m\u001b[32m0.01265\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 744 | loss: 0.01265 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 745  | total loss: \u001b[1m\u001b[32m0.01257\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 745 | loss: 0.01257 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 746  | total loss: \u001b[1m\u001b[32m0.01249\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 746 | loss: 0.01249 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 747  | total loss: \u001b[1m\u001b[32m0.01241\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 747 | loss: 0.01241 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 748  | total loss: \u001b[1m\u001b[32m0.01233\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 748 | loss: 0.01233 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 749  | total loss: \u001b[1m\u001b[32m0.01226\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 749 | loss: 0.01226 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 750  | total loss: \u001b[1m\u001b[32m0.01218\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 750 | loss: 0.01218 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 751  | total loss: \u001b[1m\u001b[32m0.01211\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 751 | loss: 0.01211 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 752  | total loss: \u001b[1m\u001b[32m0.01204\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 752 | loss: 0.01204 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 753  | total loss: \u001b[1m\u001b[32m0.01197\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 753 | loss: 0.01197 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 754  | total loss: \u001b[1m\u001b[32m0.01190\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 754 | loss: 0.01190 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 755  | total loss: \u001b[1m\u001b[32m0.01183\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 755 | loss: 0.01183 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 756  | total loss: \u001b[1m\u001b[32m0.01176\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 756 | loss: 0.01176 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 757  | total loss: \u001b[1m\u001b[32m0.01169\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 757 | loss: 0.01169 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 758  | total loss: \u001b[1m\u001b[32m0.01162\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 758 | loss: 0.01162 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 759  | total loss: \u001b[1m\u001b[32m0.01156\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 759 | loss: 0.01156 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 760  | total loss: \u001b[1m\u001b[32m0.01149\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 760 | loss: 0.01149 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 761  | total loss: \u001b[1m\u001b[32m0.01143\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 761 | loss: 0.01143 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 762  | total loss: \u001b[1m\u001b[32m0.01136\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 762 | loss: 0.01136 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 763  | total loss: \u001b[1m\u001b[32m0.01130\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 763 | loss: 0.01130 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 764  | total loss: \u001b[1m\u001b[32m0.01124\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 764 | loss: 0.01124 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 765  | total loss: \u001b[1m\u001b[32m0.01118\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 765 | loss: 0.01118 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 766  | total loss: \u001b[1m\u001b[32m0.01112\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 766 | loss: 0.01112 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 767  | total loss: \u001b[1m\u001b[32m0.01105\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 767 | loss: 0.01105 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 768  | total loss: \u001b[1m\u001b[32m0.01099\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 768 | loss: 0.01099 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 769  | total loss: \u001b[1m\u001b[32m0.01093\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 769 | loss: 0.01093 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 770  | total loss: \u001b[1m\u001b[32m0.01088\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 770 | loss: 0.01088 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 771  | total loss: \u001b[1m\u001b[32m0.01082\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 771 | loss: 0.01082 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 772  | total loss: \u001b[1m\u001b[32m0.01076\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 772 | loss: 0.01076 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 773  | total loss: \u001b[1m\u001b[32m0.01070\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 773 | loss: 0.01070 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 774  | total loss: \u001b[1m\u001b[32m0.01065\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 774 | loss: 0.01065 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 775  | total loss: \u001b[1m\u001b[32m0.01059\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 775 | loss: 0.01059 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 776  | total loss: \u001b[1m\u001b[32m0.01053\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 776 | loss: 0.01053 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 777  | total loss: \u001b[1m\u001b[32m0.01048\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 777 | loss: 0.01048 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 778  | total loss: \u001b[1m\u001b[32m0.01042\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 778 | loss: 0.01042 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 779  | total loss: \u001b[1m\u001b[32m0.01037\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 779 | loss: 0.01037 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 780  | total loss: \u001b[1m\u001b[32m0.01031\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 780 | loss: 0.01031 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 781  | total loss: \u001b[1m\u001b[32m0.01026\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 781 | loss: 0.01026 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 782  | total loss: \u001b[1m\u001b[32m0.01021\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 782 | loss: 0.01021 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 783  | total loss: \u001b[1m\u001b[32m0.01016\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 783 | loss: 0.01016 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 784  | total loss: \u001b[1m\u001b[32m0.01010\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 784 | loss: 0.01010 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 785  | total loss: \u001b[1m\u001b[32m0.01005\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 785 | loss: 0.01005 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 786  | total loss: \u001b[1m\u001b[32m0.01000\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 786 | loss: 0.01000 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 787  | total loss: \u001b[1m\u001b[32m0.00995\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 787 | loss: 0.00995 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 788  | total loss: \u001b[1m\u001b[32m0.00990\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 788 | loss: 0.00990 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 789  | total loss: \u001b[1m\u001b[32m0.00985\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 789 | loss: 0.00985 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 790  | total loss: \u001b[1m\u001b[32m0.00980\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 790 | loss: 0.00980 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 791  | total loss: \u001b[1m\u001b[32m0.00975\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 791 | loss: 0.00975 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 792  | total loss: \u001b[1m\u001b[32m0.00970\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 792 | loss: 0.00970 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 793  | total loss: \u001b[1m\u001b[32m0.00965\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 793 | loss: 0.00965 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 794  | total loss: \u001b[1m\u001b[32m0.00960\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 794 | loss: 0.00960 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 795  | total loss: \u001b[1m\u001b[32m0.00956\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 795 | loss: 0.00956 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 796  | total loss: \u001b[1m\u001b[32m0.00951\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 796 | loss: 0.00951 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 797  | total loss: \u001b[1m\u001b[32m0.00946\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 797 | loss: 0.00946 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 798  | total loss: \u001b[1m\u001b[32m0.00941\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 798 | loss: 0.00941 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 799  | total loss: \u001b[1m\u001b[32m0.00937\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 799 | loss: 0.00937 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 800  | total loss: \u001b[1m\u001b[32m0.00932\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 800 | loss: 0.00932 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 801  | total loss: \u001b[1m\u001b[32m0.00928\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 801 | loss: 0.00928 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 802  | total loss: \u001b[1m\u001b[32m0.00923\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 802 | loss: 0.00923 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 803  | total loss: \u001b[1m\u001b[32m0.00919\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 803 | loss: 0.00919 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 804  | total loss: \u001b[1m\u001b[32m0.00914\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 804 | loss: 0.00914 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 805  | total loss: \u001b[1m\u001b[32m0.00910\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 805 | loss: 0.00910 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 806  | total loss: \u001b[1m\u001b[32m0.00905\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 806 | loss: 0.00905 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 807  | total loss: \u001b[1m\u001b[32m0.00901\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 807 | loss: 0.00901 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 808  | total loss: \u001b[1m\u001b[32m0.00897\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 808 | loss: 0.00897 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 809  | total loss: \u001b[1m\u001b[32m0.00892\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 809 | loss: 0.00892 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 810  | total loss: \u001b[1m\u001b[32m0.00888\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 810 | loss: 0.00888 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 811  | total loss: \u001b[1m\u001b[32m0.00884\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 811 | loss: 0.00884 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 812  | total loss: \u001b[1m\u001b[32m0.12568\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 812 | loss: 0.12568 - acc: 0.9750 -- iter: 8/8\n",
            "--\n",
            "Training Step: 813  | total loss: \u001b[1m\u001b[32m0.11396\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 813 | loss: 0.11396 - acc: 0.9775 -- iter: 8/8\n",
            "--\n",
            "Training Step: 814  | total loss: \u001b[1m\u001b[32m0.10342\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 814 | loss: 0.10342 - acc: 0.9797 -- iter: 8/8\n",
            "--\n",
            "Training Step: 815  | total loss: \u001b[1m\u001b[32m0.09393\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 815 | loss: 0.09393 - acc: 0.9818 -- iter: 8/8\n",
            "--\n",
            "Training Step: 816  | total loss: \u001b[1m\u001b[32m0.08540\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 816 | loss: 0.08540 - acc: 0.9836 -- iter: 8/8\n",
            "--\n",
            "Training Step: 817  | total loss: \u001b[1m\u001b[32m0.07773\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 817 | loss: 0.07773 - acc: 0.9852 -- iter: 8/8\n",
            "--\n",
            "Training Step: 818  | total loss: \u001b[1m\u001b[32m0.07083\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 818 | loss: 0.07083 - acc: 0.9867 -- iter: 8/8\n",
            "--\n",
            "Training Step: 819  | total loss: \u001b[1m\u001b[32m0.06462\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 819 | loss: 0.06462 - acc: 0.9880 -- iter: 8/8\n",
            "--\n",
            "Training Step: 820  | total loss: \u001b[1m\u001b[32m0.30154\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 820 | loss: 0.30154 - acc: 0.9392 -- iter: 8/8\n",
            "--\n",
            "Training Step: 821  | total loss: \u001b[1m\u001b[32m0.27228\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 821 | loss: 0.27228 - acc: 0.9453 -- iter: 8/8\n",
            "--\n",
            "Training Step: 822  | total loss: \u001b[1m\u001b[32m0.24597\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 822 | loss: 0.24597 - acc: 0.9508 -- iter: 8/8\n",
            "--\n",
            "Training Step: 823  | total loss: \u001b[1m\u001b[32m0.22231\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 823 | loss: 0.22231 - acc: 0.9557 -- iter: 8/8\n",
            "--\n",
            "Training Step: 824  | total loss: \u001b[1m\u001b[32m0.20103\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 824 | loss: 0.20103 - acc: 0.9601 -- iter: 8/8\n",
            "--\n",
            "Training Step: 825  | total loss: \u001b[1m\u001b[32m0.18189\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 825 | loss: 0.18189 - acc: 0.9641 -- iter: 8/8\n",
            "--\n",
            "Training Step: 826  | total loss: \u001b[1m\u001b[32m0.16468\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 826 | loss: 0.16468 - acc: 0.9677 -- iter: 8/8\n",
            "--\n",
            "Training Step: 827  | total loss: \u001b[1m\u001b[32m0.14920\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 827 | loss: 0.14920 - acc: 0.9709 -- iter: 8/8\n",
            "--\n",
            "Training Step: 828  | total loss: \u001b[1m\u001b[32m0.13529\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 828 | loss: 0.13529 - acc: 0.9738 -- iter: 8/8\n",
            "--\n",
            "Training Step: 829  | total loss: \u001b[1m\u001b[32m0.12277\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 829 | loss: 0.12277 - acc: 0.9765 -- iter: 8/8\n",
            "--\n",
            "Training Step: 830  | total loss: \u001b[1m\u001b[32m0.11151\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 830 | loss: 0.11151 - acc: 0.9788 -- iter: 8/8\n",
            "--\n",
            "Training Step: 831  | total loss: \u001b[1m\u001b[32m0.10138\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 831 | loss: 0.10138 - acc: 0.9809 -- iter: 8/8\n",
            "--\n",
            "Training Step: 832  | total loss: \u001b[1m\u001b[32m0.09227\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 832 | loss: 0.09227 - acc: 0.9828 -- iter: 8/8\n",
            "--\n",
            "Training Step: 833  | total loss: \u001b[1m\u001b[32m0.08408\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 833 | loss: 0.08408 - acc: 0.9846 -- iter: 8/8\n",
            "--\n",
            "Training Step: 834  | total loss: \u001b[1m\u001b[32m0.07670\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 834 | loss: 0.07670 - acc: 0.9861 -- iter: 8/8\n",
            "--\n",
            "Training Step: 835  | total loss: \u001b[1m\u001b[32m0.07007\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 835 | loss: 0.07007 - acc: 0.9875 -- iter: 8/8\n",
            "--\n",
            "Training Step: 836  | total loss: \u001b[1m\u001b[32m0.06410\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 836 | loss: 0.06410 - acc: 0.9887 -- iter: 8/8\n",
            "--\n",
            "Training Step: 837  | total loss: \u001b[1m\u001b[32m0.05873\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 837 | loss: 0.05873 - acc: 0.9899 -- iter: 8/8\n",
            "--\n",
            "Training Step: 838  | total loss: \u001b[1m\u001b[32m0.05390\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 838 | loss: 0.05390 - acc: 0.9909 -- iter: 8/8\n",
            "--\n",
            "Training Step: 839  | total loss: \u001b[1m\u001b[32m0.04955\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 839 | loss: 0.04955 - acc: 0.9918 -- iter: 8/8\n",
            "--\n",
            "Training Step: 840  | total loss: \u001b[1m\u001b[32m0.04564\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 840 | loss: 0.04564 - acc: 0.9926 -- iter: 8/8\n",
            "--\n",
            "Training Step: 841  | total loss: \u001b[1m\u001b[32m0.04211\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 841 | loss: 0.04211 - acc: 0.9934 -- iter: 8/8\n",
            "--\n",
            "Training Step: 842  | total loss: \u001b[1m\u001b[32m0.03894\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 842 | loss: 0.03894 - acc: 0.9940 -- iter: 8/8\n",
            "--\n",
            "Training Step: 843  | total loss: \u001b[1m\u001b[32m0.03608\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 843 | loss: 0.03608 - acc: 0.9946 -- iter: 8/8\n",
            "--\n",
            "Training Step: 844  | total loss: \u001b[1m\u001b[32m0.03350\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 844 | loss: 0.03350 - acc: 0.9952 -- iter: 8/8\n",
            "--\n",
            "Training Step: 845  | total loss: \u001b[1m\u001b[32m0.03118\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 845 | loss: 0.03118 - acc: 0.9956 -- iter: 8/8\n",
            "--\n",
            "Training Step: 846  | total loss: \u001b[1m\u001b[32m0.02909\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 846 | loss: 0.02909 - acc: 0.9961 -- iter: 8/8\n",
            "--\n",
            "Training Step: 847  | total loss: \u001b[1m\u001b[32m0.02721\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 847 | loss: 0.02721 - acc: 0.9965 -- iter: 8/8\n",
            "--\n",
            "Training Step: 848  | total loss: \u001b[1m\u001b[32m0.02551\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 848 | loss: 0.02551 - acc: 0.9968 -- iter: 8/8\n",
            "--\n",
            "Training Step: 849  | total loss: \u001b[1m\u001b[32m0.02397\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 849 | loss: 0.02397 - acc: 0.9971 -- iter: 8/8\n",
            "--\n",
            "Training Step: 850  | total loss: \u001b[1m\u001b[32m0.02259\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 850 | loss: 0.02259 - acc: 0.9974 -- iter: 8/8\n",
            "--\n",
            "Training Step: 851  | total loss: \u001b[1m\u001b[32m0.02134\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 851 | loss: 0.02134 - acc: 0.9977 -- iter: 8/8\n",
            "--\n",
            "Training Step: 852  | total loss: \u001b[1m\u001b[32m0.02021\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 852 | loss: 0.02021 - acc: 0.9979 -- iter: 8/8\n",
            "--\n",
            "Training Step: 853  | total loss: \u001b[1m\u001b[32m0.01920\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 853 | loss: 0.01920 - acc: 0.9981 -- iter: 8/8\n",
            "--\n",
            "Training Step: 854  | total loss: \u001b[1m\u001b[32m0.01827\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 854 | loss: 0.01827 - acc: 0.9983 -- iter: 8/8\n",
            "--\n",
            "Training Step: 855  | total loss: \u001b[1m\u001b[32m0.01744\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 855 | loss: 0.01744 - acc: 0.9985 -- iter: 8/8\n",
            "--\n",
            "Training Step: 856  | total loss: \u001b[1m\u001b[32m0.01669\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 856 | loss: 0.01669 - acc: 0.9986 -- iter: 8/8\n",
            "--\n",
            "Training Step: 857  | total loss: \u001b[1m\u001b[32m0.01600\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 857 | loss: 0.01600 - acc: 0.9988 -- iter: 8/8\n",
            "--\n",
            "Training Step: 858  | total loss: \u001b[1m\u001b[32m0.01539\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 858 | loss: 0.01539 - acc: 0.9989 -- iter: 8/8\n",
            "--\n",
            "Training Step: 859  | total loss: \u001b[1m\u001b[32m0.01482\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 859 | loss: 0.01482 - acc: 0.9990 -- iter: 8/8\n",
            "--\n",
            "Training Step: 860  | total loss: \u001b[1m\u001b[32m0.01431\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 860 | loss: 0.01431 - acc: 0.9991 -- iter: 8/8\n",
            "--\n",
            "Training Step: 861  | total loss: \u001b[1m\u001b[32m0.01385\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 861 | loss: 0.01385 - acc: 0.9992 -- iter: 8/8\n",
            "--\n",
            "Training Step: 862  | total loss: \u001b[1m\u001b[32m0.01343\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 862 | loss: 0.01343 - acc: 0.9993 -- iter: 8/8\n",
            "--\n",
            "Training Step: 863  | total loss: \u001b[1m\u001b[32m0.01305\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 863 | loss: 0.01305 - acc: 0.9993 -- iter: 8/8\n",
            "--\n",
            "Training Step: 864  | total loss: \u001b[1m\u001b[32m0.01270\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 864 | loss: 0.01270 - acc: 0.9994 -- iter: 8/8\n",
            "--\n",
            "Training Step: 865  | total loss: \u001b[1m\u001b[32m0.01238\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 865 | loss: 0.01238 - acc: 0.9995 -- iter: 8/8\n",
            "--\n",
            "Training Step: 866  | total loss: \u001b[1m\u001b[32m0.01209\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 866 | loss: 0.01209 - acc: 0.9995 -- iter: 8/8\n",
            "--\n",
            "Training Step: 867  | total loss: \u001b[1m\u001b[32m0.01182\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 867 | loss: 0.01182 - acc: 0.9996 -- iter: 8/8\n",
            "--\n",
            "Training Step: 868  | total loss: \u001b[1m\u001b[32m0.01157\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 868 | loss: 0.01157 - acc: 0.9996 -- iter: 8/8\n",
            "--\n",
            "Training Step: 869  | total loss: \u001b[1m\u001b[32m0.01135\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 869 | loss: 0.01135 - acc: 0.9997 -- iter: 8/8\n",
            "--\n",
            "Training Step: 870  | total loss: \u001b[1m\u001b[32m0.01114\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 870 | loss: 0.01114 - acc: 0.9997 -- iter: 8/8\n",
            "--\n",
            "Training Step: 871  | total loss: \u001b[1m\u001b[32m0.01095\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 871 | loss: 0.01095 - acc: 0.9997 -- iter: 8/8\n",
            "--\n",
            "Training Step: 872  | total loss: \u001b[1m\u001b[32m0.01078\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 872 | loss: 0.01078 - acc: 0.9997 -- iter: 8/8\n",
            "--\n",
            "Training Step: 873  | total loss: \u001b[1m\u001b[32m0.01061\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 873 | loss: 0.01061 - acc: 0.9998 -- iter: 8/8\n",
            "--\n",
            "Training Step: 874  | total loss: \u001b[1m\u001b[32m0.01046\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 874 | loss: 0.01046 - acc: 0.9998 -- iter: 8/8\n",
            "--\n",
            "Training Step: 875  | total loss: \u001b[1m\u001b[32m0.01032\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 875 | loss: 0.01032 - acc: 0.9998 -- iter: 8/8\n",
            "--\n",
            "Training Step: 876  | total loss: \u001b[1m\u001b[32m0.01019\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 876 | loss: 0.01019 - acc: 0.9998 -- iter: 8/8\n",
            "--\n",
            "Training Step: 877  | total loss: \u001b[1m\u001b[32m0.01007\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 877 | loss: 0.01007 - acc: 0.9999 -- iter: 8/8\n",
            "--\n",
            "Training Step: 878  | total loss: \u001b[1m\u001b[32m0.00996\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 878 | loss: 0.00996 - acc: 0.9999 -- iter: 8/8\n",
            "--\n",
            "Training Step: 879  | total loss: \u001b[1m\u001b[32m0.00985\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 879 | loss: 0.00985 - acc: 0.9999 -- iter: 8/8\n",
            "--\n",
            "Training Step: 880  | total loss: \u001b[1m\u001b[32m0.00975\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 880 | loss: 0.00975 - acc: 0.9999 -- iter: 8/8\n",
            "--\n",
            "Training Step: 881  | total loss: \u001b[1m\u001b[32m0.00966\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 881 | loss: 0.00966 - acc: 0.9999 -- iter: 8/8\n",
            "--\n",
            "Training Step: 882  | total loss: \u001b[1m\u001b[32m0.00957\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 882 | loss: 0.00957 - acc: 0.9999 -- iter: 8/8\n",
            "--\n",
            "Training Step: 883  | total loss: \u001b[1m\u001b[32m0.00948\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 883 | loss: 0.00948 - acc: 0.9999 -- iter: 8/8\n",
            "--\n",
            "Training Step: 884  | total loss: \u001b[1m\u001b[32m0.00940\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 884 | loss: 0.00940 - acc: 0.9999 -- iter: 8/8\n",
            "--\n",
            "Training Step: 885  | total loss: \u001b[1m\u001b[32m0.00933\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 885 | loss: 0.00933 - acc: 0.9999 -- iter: 8/8\n",
            "--\n",
            "Training Step: 886  | total loss: \u001b[1m\u001b[32m0.00925\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 886 | loss: 0.00925 - acc: 0.9999 -- iter: 8/8\n",
            "--\n",
            "Training Step: 887  | total loss: \u001b[1m\u001b[32m0.00919\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 887 | loss: 0.00919 - acc: 0.9999 -- iter: 8/8\n",
            "--\n",
            "Training Step: 888  | total loss: \u001b[1m\u001b[32m0.00912\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 888 | loss: 0.00912 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 889  | total loss: \u001b[1m\u001b[32m0.00905\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 889 | loss: 0.00905 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 890  | total loss: \u001b[1m\u001b[32m0.00899\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 890 | loss: 0.00899 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 891  | total loss: \u001b[1m\u001b[32m0.00893\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 891 | loss: 0.00893 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 892  | total loss: \u001b[1m\u001b[32m0.00888\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 892 | loss: 0.00888 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 893  | total loss: \u001b[1m\u001b[32m0.00882\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 893 | loss: 0.00882 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 894  | total loss: \u001b[1m\u001b[32m0.00877\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 894 | loss: 0.00877 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 895  | total loss: \u001b[1m\u001b[32m0.00872\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 895 | loss: 0.00872 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 896  | total loss: \u001b[1m\u001b[32m0.00866\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 896 | loss: 0.00866 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 897  | total loss: \u001b[1m\u001b[32m0.00861\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 897 | loss: 0.00861 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 898  | total loss: \u001b[1m\u001b[32m0.00857\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 898 | loss: 0.00857 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 899  | total loss: \u001b[1m\u001b[32m0.00852\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 899 | loss: 0.00852 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 900  | total loss: \u001b[1m\u001b[32m0.00847\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 900 | loss: 0.00847 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 901  | total loss: \u001b[1m\u001b[32m0.00843\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 901 | loss: 0.00843 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 902  | total loss: \u001b[1m\u001b[32m0.00838\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 902 | loss: 0.00838 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 903  | total loss: \u001b[1m\u001b[32m0.00834\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 903 | loss: 0.00834 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 904  | total loss: \u001b[1m\u001b[32m0.00830\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 904 | loss: 0.00830 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 905  | total loss: \u001b[1m\u001b[32m0.00825\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 905 | loss: 0.00825 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 906  | total loss: \u001b[1m\u001b[32m0.00821\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 906 | loss: 0.00821 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 907  | total loss: \u001b[1m\u001b[32m0.00817\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 907 | loss: 0.00817 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 908  | total loss: \u001b[1m\u001b[32m0.00813\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 908 | loss: 0.00813 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 909  | total loss: \u001b[1m\u001b[32m0.00809\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 909 | loss: 0.00809 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 910  | total loss: \u001b[1m\u001b[32m0.00805\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 910 | loss: 0.00805 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 911  | total loss: \u001b[1m\u001b[32m0.00801\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 911 | loss: 0.00801 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 912  | total loss: \u001b[1m\u001b[32m0.00798\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 912 | loss: 0.00798 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 913  | total loss: \u001b[1m\u001b[32m0.00794\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 913 | loss: 0.00794 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 914  | total loss: \u001b[1m\u001b[32m0.00790\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 914 | loss: 0.00790 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 915  | total loss: \u001b[1m\u001b[32m0.00786\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 915 | loss: 0.00786 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 916  | total loss: \u001b[1m\u001b[32m0.00783\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 916 | loss: 0.00783 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 917  | total loss: \u001b[1m\u001b[32m0.00779\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 917 | loss: 0.00779 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 918  | total loss: \u001b[1m\u001b[32m0.00775\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 918 | loss: 0.00775 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 919  | total loss: \u001b[1m\u001b[32m0.00772\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 919 | loss: 0.00772 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 920  | total loss: \u001b[1m\u001b[32m0.00768\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 920 | loss: 0.00768 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 921  | total loss: \u001b[1m\u001b[32m0.00765\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 921 | loss: 0.00765 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 922  | total loss: \u001b[1m\u001b[32m0.00761\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 922 | loss: 0.00761 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 923  | total loss: \u001b[1m\u001b[32m0.00758\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 923 | loss: 0.00758 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 924  | total loss: \u001b[1m\u001b[32m0.00754\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 924 | loss: 0.00754 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 925  | total loss: \u001b[1m\u001b[32m0.00751\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 925 | loss: 0.00751 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 926  | total loss: \u001b[1m\u001b[32m0.00748\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 926 | loss: 0.00748 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 927  | total loss: \u001b[1m\u001b[32m0.00744\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 927 | loss: 0.00744 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 928  | total loss: \u001b[1m\u001b[32m0.00741\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 928 | loss: 0.00741 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 929  | total loss: \u001b[1m\u001b[32m0.00738\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 929 | loss: 0.00738 - acc: 1.0000 -- iter: 8/8\n",
            "--\n",
            "Training Step: 930  | total loss: \u001b[1m\u001b[32m0.25683\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 930 | loss: 0.25683 - acc: 0.9500 -- iter: 8/8\n",
            "--\n",
            "Training Step: 931  | total loss: \u001b[1m\u001b[32m0.23187\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 931 | loss: 0.23187 - acc: 0.9550 -- iter: 8/8\n",
            "--\n",
            "Training Step: 932  | total loss: \u001b[1m\u001b[32m0.70341\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 932 | loss: 0.70341 - acc: 0.8595 -- iter: 8/8\n",
            "--\n",
            "Training Step: 933  | total loss: \u001b[1m\u001b[32m0.63386\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 933 | loss: 0.63386 - acc: 0.8735 -- iter: 8/8\n",
            "--\n",
            "Training Step: 934  | total loss: \u001b[1m\u001b[32m0.57131\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 934 | loss: 0.57131 - acc: 0.8862 -- iter: 8/8\n",
            "--\n",
            "Training Step: 935  | total loss: \u001b[1m\u001b[32m0.51506\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 935 | loss: 0.51506 - acc: 0.8976 -- iter: 8/8\n",
            "--\n",
            "Training Step: 936  | total loss: \u001b[1m\u001b[32m0.69238\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 936 | loss: 0.69238 - acc: 0.8578 -- iter: 8/8\n",
            "--\n",
            "Training Step: 937  | total loss: \u001b[1m\u001b[32m0.62413\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 937 | loss: 0.62413 - acc: 0.8720 -- iter: 8/8\n",
            "--\n",
            "Training Step: 938  | total loss: \u001b[1m\u001b[32m0.56276\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 938 | loss: 0.56276 - acc: 0.8848 -- iter: 8/8\n",
            "--\n",
            "Training Step: 939  | total loss: \u001b[1m\u001b[32m0.50757\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 939 | loss: 0.50757 - acc: 0.8963 -- iter: 8/8\n",
            "--\n",
            "Training Step: 940  | total loss: \u001b[1m\u001b[32m0.45796\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 940 | loss: 0.45796 - acc: 0.9067 -- iter: 8/8\n",
            "--\n",
            "Training Step: 941  | total loss: \u001b[1m\u001b[32m0.41335\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 941 | loss: 0.41335 - acc: 0.9160 -- iter: 8/8\n",
            "--\n",
            "Training Step: 942  | total loss: \u001b[1m\u001b[32m0.37324\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 942 | loss: 0.37324 - acc: 0.9244 -- iter: 8/8\n",
            "--\n",
            "Training Step: 943  | total loss: \u001b[1m\u001b[32m0.33718\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 943 | loss: 0.33718 - acc: 0.9320 -- iter: 8/8\n",
            "--\n",
            "Training Step: 944  | total loss: \u001b[1m\u001b[32m0.30476\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 944 | loss: 0.30476 - acc: 0.9388 -- iter: 8/8\n",
            "--\n",
            "Training Step: 945  | total loss: \u001b[1m\u001b[32m0.27561\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 945 | loss: 0.27561 - acc: 0.9449 -- iter: 8/8\n",
            "--\n",
            "Training Step: 946  | total loss: \u001b[1m\u001b[32m0.24940\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 946 | loss: 0.24940 - acc: 0.9504 -- iter: 8/8\n",
            "--\n",
            "Training Step: 947  | total loss: \u001b[1m\u001b[32m0.22584\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 947 | loss: 0.22584 - acc: 0.9554 -- iter: 8/8\n",
            "--\n",
            "Training Step: 948  | total loss: \u001b[1m\u001b[32m0.20466\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 948 | loss: 0.20466 - acc: 0.9598 -- iter: 8/8\n",
            "--\n",
            "Training Step: 949  | total loss: \u001b[1m\u001b[32m0.18561\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 949 | loss: 0.18561 - acc: 0.9639 -- iter: 8/8\n",
            "--\n",
            "Training Step: 950  | total loss: \u001b[1m\u001b[32m0.16848\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 950 | loss: 0.16848 - acc: 0.9675 -- iter: 8/8\n",
            "--\n",
            "Training Step: 951  | total loss: \u001b[1m\u001b[32m0.15308\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 951 | loss: 0.15308 - acc: 0.9707 -- iter: 8/8\n",
            "--\n",
            "Training Step: 952  | total loss: \u001b[1m\u001b[32m0.13923\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 952 | loss: 0.13923 - acc: 0.9737 -- iter: 8/8\n",
            "--\n",
            "Training Step: 953  | total loss: \u001b[1m\u001b[32m0.12678\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 953 | loss: 0.12678 - acc: 0.9763 -- iter: 8/8\n",
            "--\n",
            "Training Step: 954  | total loss: \u001b[1m\u001b[32m0.11558\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 954 | loss: 0.11558 - acc: 0.9787 -- iter: 8/8\n",
            "--\n",
            "Training Step: 955  | total loss: \u001b[1m\u001b[32m0.10550\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 955 | loss: 0.10550 - acc: 0.9808 -- iter: 8/8\n",
            "--\n",
            "Training Step: 956  | total loss: \u001b[1m\u001b[32m0.30961\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 956 | loss: 0.30961 - acc: 0.9327 -- iter: 8/8\n",
            "--\n",
            "Training Step: 957  | total loss: \u001b[1m\u001b[32m0.28017\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 957 | loss: 0.28017 - acc: 0.9394 -- iter: 8/8\n",
            "--\n",
            "Training Step: 958  | total loss: \u001b[1m\u001b[32m0.25370\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 958 | loss: 0.25370 - acc: 0.9455 -- iter: 8/8\n",
            "--\n",
            "Training Step: 959  | total loss: \u001b[1m\u001b[32m0.22991\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 959 | loss: 0.22991 - acc: 0.9509 -- iter: 8/8\n",
            "--\n",
            "Training Step: 960  | total loss: \u001b[1m\u001b[32m0.31781\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 960 | loss: 0.31781 - acc: 0.9309 -- iter: 8/8\n",
            "--\n",
            "Training Step: 961  | total loss: \u001b[1m\u001b[32m0.28766\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 961 | loss: 0.28766 - acc: 0.9378 -- iter: 8/8\n",
            "--\n",
            "Training Step: 962  | total loss: \u001b[1m\u001b[32m0.26056\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 962 | loss: 0.26056 - acc: 0.9440 -- iter: 8/8\n",
            "--\n",
            "Training Step: 963  | total loss: \u001b[1m\u001b[32m0.23620\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 963 | loss: 0.23620 - acc: 0.9496 -- iter: 8/8\n",
            "--\n",
            "Training Step: 964  | total loss: \u001b[1m\u001b[32m0.21430\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 964 | loss: 0.21430 - acc: 0.9546 -- iter: 8/8\n",
            "--\n",
            "Training Step: 965  | total loss: \u001b[1m\u001b[32m0.19461\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 965 | loss: 0.19461 - acc: 0.9592 -- iter: 8/8\n",
            "--\n",
            "Training Step: 966  | total loss: \u001b[1m\u001b[32m0.37910\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 966 | loss: 0.37910 - acc: 0.9133 -- iter: 8/8\n",
            "--\n",
            "Training Step: 967  | total loss: \u001b[1m\u001b[32m0.34300\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 967 | loss: 0.34300 - acc: 0.9219 -- iter: 8/8\n",
            "--\n",
            "Training Step: 968  | total loss: \u001b[1m\u001b[32m0.41220\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 968 | loss: 0.41220 - acc: 0.9047 -- iter: 8/8\n",
            "--\n",
            "Training Step: 969  | total loss: \u001b[1m\u001b[32m0.37289\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 969 | loss: 0.37289 - acc: 0.9143 -- iter: 8/8\n",
            "--\n",
            "Training Step: 970  | total loss: \u001b[1m\u001b[32m0.33755\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 970 | loss: 0.33755 - acc: 0.9228 -- iter: 8/8\n",
            "--\n",
            "Training Step: 971  | total loss: \u001b[1m\u001b[32m0.30580\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 971 | loss: 0.30580 - acc: 0.9306 -- iter: 8/8\n",
            "--\n",
            "Training Step: 972  | total loss: \u001b[1m\u001b[32m0.27726\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 972 | loss: 0.27726 - acc: 0.9375 -- iter: 8/8\n",
            "--\n",
            "Training Step: 973  | total loss: \u001b[1m\u001b[32m0.25161\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 973 | loss: 0.25161 - acc: 0.9437 -- iter: 8/8\n",
            "--\n",
            "Training Step: 974  | total loss: \u001b[1m\u001b[32m0.22856\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 974 | loss: 0.22856 - acc: 0.9494 -- iter: 8/8\n",
            "--\n",
            "Training Step: 975  | total loss: \u001b[1m\u001b[32m0.20783\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 975 | loss: 0.20783 - acc: 0.9544 -- iter: 8/8\n",
            "--\n",
            "Training Step: 976  | total loss: \u001b[1m\u001b[32m0.18921\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 976 | loss: 0.18921 - acc: 0.9590 -- iter: 8/8\n",
            "--\n",
            "Training Step: 977  | total loss: \u001b[1m\u001b[32m0.17246\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 977 | loss: 0.17246 - acc: 0.9631 -- iter: 8/8\n",
            "--\n",
            "Training Step: 978  | total loss: \u001b[1m\u001b[32m0.15741\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 978 | loss: 0.15741 - acc: 0.9668 -- iter: 8/8\n",
            "--\n",
            "Training Step: 979  | total loss: \u001b[1m\u001b[32m0.14387\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 979 | loss: 0.14387 - acc: 0.9701 -- iter: 8/8\n",
            "--\n",
            "Training Step: 980  | total loss: \u001b[1m\u001b[32m0.13170\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 980 | loss: 0.13170 - acc: 0.9731 -- iter: 8/8\n",
            "--\n",
            "Training Step: 981  | total loss: \u001b[1m\u001b[32m0.12075\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 981 | loss: 0.12075 - acc: 0.9758 -- iter: 8/8\n",
            "--\n",
            "Training Step: 982  | total loss: \u001b[1m\u001b[32m0.11090\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 982 | loss: 0.11090 - acc: 0.9782 -- iter: 8/8\n",
            "--\n",
            "Training Step: 983  | total loss: \u001b[1m\u001b[32m0.10204\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 983 | loss: 0.10204 - acc: 0.9804 -- iter: 8/8\n",
            "--\n",
            "Training Step: 984  | total loss: \u001b[1m\u001b[32m0.09406\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 984 | loss: 0.09406 - acc: 0.9823 -- iter: 8/8\n",
            "--\n",
            "Training Step: 985  | total loss: \u001b[1m\u001b[32m0.08689\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 985 | loss: 0.08689 - acc: 0.9841 -- iter: 8/8\n",
            "--\n",
            "Training Step: 986  | total loss: \u001b[1m\u001b[32m0.08042\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 986 | loss: 0.08042 - acc: 0.9857 -- iter: 8/8\n",
            "--\n",
            "Training Step: 987  | total loss: \u001b[1m\u001b[32m0.07460\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 987 | loss: 0.07460 - acc: 0.9871 -- iter: 8/8\n",
            "--\n",
            "Training Step: 988  | total loss: \u001b[1m\u001b[32m0.06936\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 988 | loss: 0.06936 - acc: 0.9884 -- iter: 8/8\n",
            "--\n",
            "Training Step: 989  | total loss: \u001b[1m\u001b[32m0.06463\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 989 | loss: 0.06463 - acc: 0.9896 -- iter: 8/8\n",
            "--\n",
            "Training Step: 990  | total loss: \u001b[1m\u001b[32m0.06037\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 990 | loss: 0.06037 - acc: 0.9906 -- iter: 8/8\n",
            "--\n",
            "Training Step: 991  | total loss: \u001b[1m\u001b[32m0.05653\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 991 | loss: 0.05653 - acc: 0.9916 -- iter: 8/8\n",
            "--\n",
            "Training Step: 992  | total loss: \u001b[1m\u001b[32m0.05306\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 992 | loss: 0.05306 - acc: 0.9924 -- iter: 8/8\n",
            "--\n",
            "Training Step: 993  | total loss: \u001b[1m\u001b[32m0.04993\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 993 | loss: 0.04993 - acc: 0.9932 -- iter: 8/8\n",
            "--\n",
            "Training Step: 994  | total loss: \u001b[1m\u001b[32m0.04711\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 994 | loss: 0.04711 - acc: 0.9938 -- iter: 8/8\n",
            "--\n",
            "Training Step: 995  | total loss: \u001b[1m\u001b[32m0.04455\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 995 | loss: 0.04455 - acc: 0.9945 -- iter: 8/8\n",
            "--\n",
            "Training Step: 996  | total loss: \u001b[1m\u001b[32m0.04224\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 996 | loss: 0.04224 - acc: 0.9950 -- iter: 8/8\n",
            "--\n",
            "Training Step: 997  | total loss: \u001b[1m\u001b[32m0.04015\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 997 | loss: 0.04015 - acc: 0.9955 -- iter: 8/8\n",
            "--\n",
            "Training Step: 998  | total loss: \u001b[1m\u001b[32m0.03825\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 998 | loss: 0.03825 - acc: 0.9960 -- iter: 8/8\n",
            "--\n",
            "Training Step: 999  | total loss: \u001b[1m\u001b[32m0.03653\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 999 | loss: 0.03653 - acc: 0.9964 -- iter: 8/8\n",
            "--\n",
            "Training Step: 1000  | total loss: \u001b[1m\u001b[32m0.03498\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 1000 | loss: 0.03498 - acc: 0.9967 -- iter: 8/8\n",
            "--\n",
            "INFO:tensorflow:/content/model.tflearn is not in all_model_checkpoint_paths. Manually adding it.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_IqfsOhpbXY3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "a7f72cfb-845f-4482-fd88-4e8c8452f7fe"
      },
      "source": [
        "'''\n",
        "try:\n",
        "  model.load(\"model.tflearn\")\n",
        "except:\n",
        "  model.fit(training, output, n_epoch=1000, batch_size=8, show_metric=True)\n",
        "  model.save(\"model.tflearn\")\n",
        "'''"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\ntry:\\n  model.load(\"model.tflearn\")\\nexcept:\\n  model.fit(training, output, n_epoch=1000, batch_size=8, show_metric=True)\\n  model.save(\"model.tflearn\")\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D2HR8SzGspoB",
        "outputId": "1c7ab2ff-f42d-4728-a2a7-0b45c220f85f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model.load(\"model.tflearn\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Restoring parameters from /content/model.tflearn\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "za2e-nXYtfly"
      },
      "source": [
        "def bag_words(s, words):\n",
        "    bag = [0 for _ in range(len(words))]\n",
        "\n",
        "    s_words = nltk.word_tokenize(s)\n",
        "    s_words = [stemmer.stem(word.lower()) for word in s_words]\n",
        "\n",
        "    for se in s_words:\n",
        "        for i, w in enumerate(words):\n",
        "            if w == se:\n",
        "                bag[i] = 1\n",
        "    \n",
        "    return numpy.array(bag)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41crosdRrvUu",
        "outputId": "ef546597-8b27-4c2e-dd55-edf0b5cff998",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "matrix_palavra = bag_words('oi', words)\n",
        "print(matrix_palavra)\n",
        "\n",
        "results = model.predict([matrix_palavra])\n",
        "print(results)\n",
        "\n",
        "results_index = numpy.argmax(results)\n",
        "print(results_index)\n",
        "\n",
        "tag = labels[results_index]\n",
        "print(tag)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 0 0 0 0 1 0 0 0 0]\n",
            "[[0.01946444 0.9805355 ]]\n",
            "1\n",
            "saudacao\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRt608aYN-5r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3316dccc-f5b7-4318-ccc6-7cf2d846fbe9"
      },
      "source": [
        "def chat():\n",
        "    print(\"Falar com o bot (sair)\")\n",
        "    while True:\n",
        "            inp = input(\"Voce: \")\n",
        "            if inp.lower() == \"sair\":\n",
        "                break\n",
        "            \n",
        "            results = model.predict([bag_words(inp, words)])\n",
        "            results_index = numpy.argmax(results)\n",
        "            tag = labels[results_index]\n",
        "            #print(tag)\n",
        "\n",
        "            for tg in data['intents']:\n",
        "                if tg['tag'] == tag:\n",
        "                    responses = tg['responses']\n",
        "                \n",
        "            print(random.choice(responses))\n",
        "\n",
        "chat()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Falar com o bot (sair)\n",
            "Voce: oi\n",
            "como vai?\n",
            "Voce: xau\n",
            "ate logo\n"
          ]
        }
      ]
    }
  ]
}